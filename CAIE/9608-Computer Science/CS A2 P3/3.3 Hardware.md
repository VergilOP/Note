# Table of contents

- [3.3 Hardware](#33-hardware)
  - [***3.3.1 Logic gates and circuit design***](#331-logic-gates-and-circuit-design)
  - [***3.3.2 Boolean algebra***](#332-boolean-algebra)
  - [***3.3.3 Karnaugh Maps***](#333-karnaugh-maps)
  - [***3.3.4 Flip-flops***](#334-flip-flops)
  - [***3.3.5 RISC processors***](#335-risc-processors)
    - [What is `RISC(Reduced instruction set computer)` \[1\]](#what-is-riscreduced-instruction-set-computer-1)
    - [What is `CISC(Complex instruction set computer)` \[1\]](#what-is-cisccomplex-instruction-set-computer-1)
    - [What is `Pipelining` \[1\]](#what-is-pipelining-1)
  - [***3.3.6 Parallel processing***](#336-parallel-processing)
    - [What is meant by `MISD` \[2\]](#what-is-meant-by-misd-2)
    - [What is meant by `SIMD` \[2\]](#what-is-meant-by-simd-2)
    - [What is meant by `SISD` \[2\]](#what-is-meant-by-sisd-2)
    - [What is meant by `MIMD` \[3\]](#what-is-meant-by-mimd-3)
    - [Explain why code optimisation is necessary \[2\]](#explain-why-code-optimisation-is-necessary-2)
    - [State three characteristics of massively `parallel computers` \[3\]](#state-three-characteristics-of-massively-parallel-computers-3)

## 3.3 Hardware
-------------

### ***3.3.1 Logic gates and circuit design***

> produce truth tables for common logic circuits including half adders and full adders
---

> derive a truth table for a given logic circuit
---

### ***3.3.2 Boolean algebra***

> show understanding of Boolean algebra
---

> show understanding of De Morgan’s Laws
---

> perform Boolean algebra using De Morgan’s Laws
---

> simplify a logic circuit/expression using Boolean algebra
---

### ***3.3.3 Karnaugh Maps***

> show understanding of Karnaugh Maps
---

> show understanding of the benefits of using Karnaugh Maps
---

> solve logic problems using Karnaugh Maps
---

### ***3.3.4 Flip-flops***

> show understanding of how to construct a flip-flop (SR and JK)
---

> describe the role of flip-flops as data storage elements
---

### ***3.3.5 RISC processors***

> show understanding of the differences between RISC and CISC processors
---

#### What is `RISC(Reduced instruction set computer)` \[1\]
> s20_33_Q5

- A processor with a few simple fixed-length instructions that have a few instruction formats is called a **RISC** processor

#### What is `CISC(Complex instruction set computer)` \[1\]
> s20_33_Q5

- A processor with many complex variable-length instructions that has many instruction formats is called a **CISC** processor

> show understanding of the importance/use of pipelining and registers in RISC processors
---

#### What is `Pipelining` \[1\]
> s20_33_Q5

- Instruction-level parallelism, applied to the execution of instructions during the fetch-execute cycle, is called **Pipelining**

> show understanding of interrupt handling on CISC and RISC processors
---

### ***3.3.6 Parallel processing***

> show awareness of the four basic computer architectures: SISD, SIMD, MISD, MIMD
---

#### What is meant by `MISD` \[2\]
> w19_33_Q9

- There are several processors
- Each processor executes different sets of instructions on one set of data at the same time

#### What is meant by `SIMD` \[2\]
> w19_33_Q9

- The processor has several ALUs
- Each ALU executes the same set of instructions on different sets of data at the same time

#### What is meant by `SISD` \[2\]
> w19_33_Q9

- There is only one processor
- The processor executes one set of instructions on one set of data

#### What is meant by `MIMD` \[3\]
> w19_33_Q9

- There are several processors
- Each processor executes a different set of instructions
- Each processor operates on different sets of data

> show awareness of the characteristics of massively parallel computers
---

#### Explain why code optimisation is necessary \[2\]
> s19_33_Q4

- Optimisation means that the code will have fewer instructions
- Optimised code occupies less space in memory
- Fewer instructions reduces the execution time of the program

#### State three characteristics of massively `parallel computers` \[3\]
> w19_33_Q9

- A large number of processors
- Collaborative processing
- Network infrastructure
- Communication using a message interface

