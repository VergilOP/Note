{"cells":[{"cell_type":"markdown","source":["Here in this notebook, we will have a look at the Perceptron model, as it is a useful model to understand many machine learning techniques. The textbook we use has a presentation as well. This notebook is just a brief introduction, which provides a different view of this model."],"metadata":{"id":"NxAoDZ2NJtzS"}},{"cell_type":"markdown","source":["# The Perceptron\n","\n","\n","<center>\n","<img src=\"https://github.com/jiankliu/jiankliu.github.io/blob/main/notebook_images/Fig3.3.jpg?raw=true\" width=450 height=450 />\n","\n","**Figure 3.3:** A dataset, consisting of measurement values which have been classified as belonging two one of two classes.\n","</center>\n","\n","A key idea in the development of machine learning is Rosenblatt's\n","perceptron. It is no longer used as a serous machine learning technique,\n","but it is an excellent device to demonstrate some of the key principles\n","that underpin neural networks. Consider Fig. 3.3. Here,\n","two quantities $x$ and $y$ have been measured, and the measurement\n","values have been *classified*. For example, $x$ and $y$ are two chemical\n","concentrations, and the classification can be hazardous or\n","non-hazardous. Based on the data sample that has been classified thus\n","far, it seems that the two classes are well separated in terms of\n","concentration space $(x,y)$. If future measurements behave similar, it\n","will not be difficult to design a *classifier*. One way of doing that is\n","to draw a straight line between the two sets of points, and if a new\n","data point arrives it can be classified according to which side of the\n","line it falls. This is the idea behind a *linear classifier* (or\n","discriminant).\n","\n","Mathematically, we can implement this by representing the line between\n","the two classes, the so-called *decision boundary* in terms of its\n","parameters. The most general representation of a straight line in two\n","dimensions is:\n","\n","$$ax + by = c$$\n","\n","and we can implement a decision by using\n","a so-called *squashing* function, which here we take to be the Heaviside\n","or step function:\n","\n","$$o = \\mathcal{H}(ax + by -c),\\tag{3.1}$$\n","\n","  with\n","\n","$$\\mathcal{H}(x) = \\left\\{ \\begin{array}{cc} 0 :  x &  < 0 \\\\ 1: x & \\ge 0 \\end{array} \\right.$$\n","\n","Note that we need all three parameters: without $c$, we would only be\n","able to represent lines through the origin, which would not do for the\n","dataset of Fig. 3.3.\n","\n","Inspired by neuroscience, we can introduce an artificial neuron with two\n","inputs, and a weight associated with each input. In general, we write:\n","\n","$$o = f(w_1 x_1 + w_2 x_2 -\\theta)\\tag{3.2}$$\n","\n","This is nothing but a rewrite of Eq. 3.1 of course. Our parameters $w_1, w_2$ are now called weights, and $\\theta$ is called a *threshold* or *bias* (we will\n","treat these words as synonyms). $x_1$ and $x_2$ are just our former\n","variable $x$ and $y$.\n","\n","The device represented by Eq.3.2 is called an *artificial neuron*. Calculating the *output state*, the value of variable $o$ is called *updating* the neuron (we often drop the\n","adjective artificial is this is clear from the context). Often, the\n","output state is retained until a new update is made, for example in\n","response to changed inputs.\n","\n","Note that conceptually there are similarities to the real neuron: one\n","could consider the quantity $w_1 x_1 + w_2x_2$, the so-called *local\n","field* as a representation of the fact that the weighted input\n","contributions exceed the threshold $\\theta$, or not. If the threshold is\n","not crossed, the response will be '0'. Like the real neuron, the\n","artificial one is not affected by sub threshold activities, but if the\n","threshold is exceeded, a non linear response follows: a sudden jump to\n","'1'. Later, it will turn out that these non linearities are very\n","important.\n","\n","To illustrate both the process and also provide an illustration of\n","McCullogh and Pitts point that networks of artificial neurons can\n","implement complex Boolean functions, let us look at the logical **AND**\n","gate as a classification problem.\n","\n","| $x1$ | $x2$ | $O$ |\n","|----|----|---|\n","| 0  | 0  | 0 |\n","| 0  | 1  | 0 |\n","| 1  | 0  | 0 |\n","| 1  | 1  | 1 |\n","\n","\n","**Table 3.1**: The AND gate as a classification problem. It can be considered\n","  such because $x_1$, $x_2$ can be considered its inputs and the\n","  required logical value is the desired output classification.\n","\n","The 4 input values can be represented as points in $x_1, x_2$ space and\n","can be plotted with a marker to indicate whether the desired\n","classification is '0' or '1'. The resulting figure, Fig. 3.4, is similar to Fig. 3.3 in that it is clear that a straight line can be found that separates the class '1' point from the class '0' points.\n","\n","The equation of the line in the figure can easily be seen to be\n","$$y = -x + 1.5$$\n","\n","We only have to do a simple rearrangement to bring this\n","into perceptron form:\n","$$x + y - 1.5 = 0$$\n","\n","We now have to decide which points should have outcome '0', and we\n","chose:\n","$$o = \\mathcal{H}(x_1 + x_2 - 1.5) \\tag{3.3}$$\n","\n","From this, we can read off weights and\n","threshold: $w_1 = w_2 =1$, $\\theta = 1.5$. Note that the value of the\n","threshold itself is a positive value, the minus sign in Eq.\n","3.2 ensures it works as a threshold.\n","\n","So far, we have only determined the position of the decision line, but\n","the decision could go the wrong way and we need to check that we have\n","not accidentally chosen the inverse classification! We try the point\n","$(0,0)$.\n","\n","This gives:\n","\n","$$1 \\cdot 0 + 1 \\cdot 0 = 0 < 1.5$$\n","\n","The weighted sum is less than the threshold, so the output classification is '0', as\n","it should be. The fact that the other points to be classified as '0' are\n","on the same side the line and the one that is to be classified as '1'\n","ensures that our classifier is correct. Repeating the calculation for\n","the other values in the table we see that the only input able to\n","overcome the threshold is: $x_1 = x_2 =1$. This shows why the classifier\n","works.\n","\n","Now consider the following questions:\n","\n","1.  Does multiplying all weights and threshold by the same constant\n","    change anything in the classifiers output?\n","\n","2.  Does it matter whether this constant is positive or negative?\n","\n","<center>\n","<img src=\"https://github.com/jiankliu/jiankliu.github.io/blob/main/notebook_images/Fig3.4.jpg?raw=true\" width=350 height=350 />\n","\n","**Figure 3.4**: The AND gate as a classification\n","problem is linearly separable.\n","</center>\n","\n","From the figure it is clear that our perceptron has a nice property: it\n","is fairly *robust*. If the input values are slightly distorted due to\n","noise, the classification works correct, at least for this line.\n","\n","It is also clear that the threshold plays an essential role: without it,\n","we would only be able to form decision lines through the origin, and the\n","**AND** problem can not be solved that way. Later, when we start to\n","develop algorithms to adapt weights to the classification problem at\n","hand, it will become inconvenient to distinguish between weights and\n","threshold. Like the weights, the threshold may have to be adapted and it\n","will be awkward to have to treat the weights and threshold separately.\n","\n","A simple solution consists in creating a third input whose input is\n","always equal to 1. The weight of the third input $w_3$ can then be\n","chosen to be $-\\theta$. This means that we have to create a three input\n","perceptron *without threshold* that is capable of learning the following\n","dataset.\n","\n","\n","It is clear that the perceptron that is defined by:\n","\n","$$o = \\mathcal{H}( w_1 x_1 + w_2 x_2 + w_3 x_3)$$\n","\n","for $w_1 = w_2 = 1, w_3 = -1.5$ classifies all points correctly and\n","essentially performs the same computation as the perceptron from Eq\n","3.3.\n","\n","| $x_1$ | $x_2$ | $x_3$ | $o$ |\n","|-------|-------|-------|-----|\n","| 0     | 0     | 1     | 0   |\n","| 0     | 1     | 1     | 0   |\n","| 1     | 0     | 1     | 0   |\n","| 1     | 1     | 1     | 1   |\n","\n","**Table 3.2: The dataset of the AND classification problem for a perceptron\n","  with three inputs without threshold.**\n","\n","This trick also works in higher dimensions. In general, for any\n","arbitrary hyperplane in an $N$-dimensional space, we can find a\n","corresponding one that goes through the origin of an $N+1$ dimensional\n","space.\n","\n","This is highly convenient, because computationally the calculation of a\n","perceptron output requires the evaluation of the scalar product:\n","\n","$$\\boldsymbol{w}^T\\boldsymbol{x}$$\n","\n","Remember that this is a\n","condensed way of writing:\n","\n","$$\\sum^N_{i=1} w_i x_i$$\n","\n","So, to summarise: we need a threshold or bias to obtain a working\n","classifier but can easily implement this by concatenating the input\n","vector with an extra unit whose value is always one, and therefore do\n","not need to consider thresholds as anything else but an extra weight in\n","actual calculations.\n","\n","\n","# The PLA (Perceptron Learning Algorithm)\n","------------------------\n","\n","It is clear that if a dataset is linearly separable that a perceptron\n","can be found to classify it correctly. This is true in higher\n","dimensional spaces as well. In $N$ dimensions, we need $N$ weights and\n","one threshold $\\theta$. So, we can use a perceptron with $N$ inputs to\n","implement any plane\n","\n","$$w_1 x_1 + w_2 x_2 + \\cdots w_N x_N = \\theta$$\n","\n","as a\n","decision boundary, or alternatively, we can use a perceptron with $N+1$\n","inputs as long as we guarantee that one of the inputs is clamped to the\n","value 1.\n","\n","If a hyperplane can be found that separates data points into two\n","classes, again, we call this dataset linearly separable and by\n","definition, a perceptron can be found that will classify a linearly\n","separable dataset correctly. In higher dimensional spaces the dataset\n","becomes more difficult to visualise, so the graphical method that we\n","used to determine the decision boundary for the **AND** problem is no\n","longer suitable. Ideally, we would want an algorithm that is capable of\n","automatically finding the weights given the dataset. For linearly\n","separable datasets a simple algorithm exists that iterates through the\n","dataset and finds a correct set of weights in a finite number of steps.\n","\n","The algorithm is of great historical significance and bears an\n","interesting relationship to *logistic regression*, which you already knew. Nowadays, it is no longer the method of choice due to a\n","few drawbacks that we will discuss below. But because it is easy to\n","implement and can serve as a model for more complex algorithms, we will\n","present it here.\n","\n","Assume that we have $D$ data points in an $N$ dimensional space, and\n","that each point has received a classification into one of two mutually\n","exclusive classes. (This is just a way of saying that every data point\n","belongs to either class '0' or '1', but not to both).\n","\n","The algorithm is as follows. Start with a perceptron with $N+1$ inputs,\n","one of which is clamped to value +1, and $N+1$ weights so that a\n","threshold is unnecessary.\n","\n","1.  **Start**: Choose a random set of weights: $\\boldsymbol{w}_0$. For later reference, denote the current set of weights by\n","   $\\boldsymbol{w}_i$, so a the start\n","   $\\boldsymbol{w}_i = \\boldsymbol{w}_0$.\n","\n","2.  **Continue:** Pick a random data point\n","  $(\\boldsymbol{x}_j, d_j), j = 1, \\cdots, D$, where\n","  $\\boldsymbol{x}_j$ is a point for which classification\n","  $d_j \\in \\left\\{ 0, 1 \\right\\}$ is given.\n","\n","3.  Evaluate $w_{i}^{T} x_{j} .$ If $\\left\\{\\begin{array}{ll}w^{T} x_{j}<0 & \\text { AND } \\quad d_{j}=0: \\text { goto Continue } \\\\ w^{T} x_{j}<0 & \\text { AND } \\quad d_{j}=1: \\text { goto Add } \\\\ w^{T} x_{j} \\geq 0 & \\text { AND } \\quad d_{j}=1: \\text { goto Continue } \\\\ w^{T} x_{j} \\geq 0 & \\text { AND } \\quad d_{j}=0: \\text { goto Subtract }\\end{array}\\right.$\n","\n","4.  **Add:** $\\boldsymbol{w}_{i+1} = \\boldsymbol{w}_i + \\boldsymbol{x}$;\n","   Goto Continue\n","\n","5.  **Subtract:**\n","   $\\boldsymbol{w}_{i+1} = \\boldsymbol{w}_i - \\boldsymbol{x}$; Goto\n","   Continue\n","\n","This is the perceptron algorithm as presented by Minsky & Papert. Their book is of great historical interest, not least because\n","it has been accused of setting neural network research by decade! The\n","presentation is clumsy (goto's!), but they key idea is present and\n","simple: if the classification is correct, leave the weights alone,\n","otherwise add or subtract the input pattern so that the weights may do\n","better on the same pattern next time around. You might at this point\n","object by saying that improving the classifier to perform better on one\n","particular pattern may make it worse for other patterns, and that this\n","intuition may be misguided. It is therefore important that this\n","algorithm can be shown to converge for a linearly separable dataset.\n","\n","\n","# The Perceptron Theorem\n","\n","The *perceptron theorem* states that for a linearly separable dataset,\n","the algorithm visits **ADD** and **SUBTRACT** a finite number of times\n","(and then has converged, even if according to the algorithm presented\n","above it is stuck in a endless loop (Minsky and Papert's presentation is\n","really clumsy).\n","\n","The *Perceptron Theorem* essentially states that the perceptron\n","algorithm will always converge on linearly separable data. Multiple\n","solutions are usually possible and the Perceptron Theorem states nothing\n","about which solution ultimately will be found. Due to the Perceptron\n","Theorem the perceptron algorithm is actually one of the very few\n","provable results on neural networks. The proof is instructive: it\n","contains a number of ideas that may help you about neural networks at a\n","higher level and that also show up in *support vector machines*. For\n","these reasons and its great historical importance, we present it here (you can igore this part if you like).\n","\n","We assume that we have a data space of dimension $N$. We assume that we\n","have a number of data points that can belong to two classes:\n","$\\mathcal{C}_1$ and $\\mathcal{C}_2$. Each data point belongs to one and\n","only one class. We assume that this data is linearly separable, that is,\n","there exist $N$ weights and a bias that separates the points of the two\n","classes. Again, we will use vector notation. There exists a vector\n","$\\boldsymbol{w}$ of dimension $N$ and a bias $\\theta$ such that for all\n","$\\boldsymbol{x}_i \\in \\mathcal{C}_1$, we have\n","$\\boldsymbol{w} \\cdot \\boldsymbol{x}_i - \\theta \\ge 0$ and for all\n","$\\boldsymbol{x}_j \\in \\mathcal{C}_2$, we have\n","$\\boldsymbol{w} \\cdot \\boldsymbol{x}_j - \\theta < 0$.\n","\n","We will simply this: first, we want to get rid of the bias in the way we\n","described earlier. That is, we take every data point $\\boldsymbol{x}_i$\n","and extend it by adding the value '1' to the sequence of numbers that is\n","represented by the vector $\\boldsymbol{x}_i$, which yields an\n","$N+1$-dimensional vector. Similarly, we 'add' another entry to our\n","weight vector $\\boldsymbol{w}$, making it an $N+1$ dimensional vector.\n","\n","The advantage of this is that linear separability becomes even easier to\n","formulate. It is now a hyperplane *through the origin*, separating the\n","points of the two classes in an $N+1$ dimensional space. So, there exist\n","an $N+1$ dimensional vector $\\boldsymbol{w}^{*}$ such that\n","${\\boldsymbol{w}^*}^T\\boldsymbol{x_i} \\ge 0$ for all $\\boldsymbol{x}_i \\in \\mathcal{C}_1$ and ${\\boldsymbol{w}^*}^T\\boldsymbol{x}_j<0$ for all $\\boldsymbol{x}_j \\in \\mathcal{C}_2$.\n","\n","This now allows a further simplification: take a pattern $\\boldsymbol{x}_j$ from class $\\mathcal{C}_2$. Remove it from class $\\mathcal{C}_2$ and add the pattern $-\\boldsymbol{x}_j$ to class\n","$\\mathcal{C}_1$. Repeat this for all patterns in $\\mathcal{C}_2$, so\n","that this class becomes empty and class $\\mathcal{C}_1$ is extended by\n","negative versions of patterns that were previously in $\\mathcal{C}_2$.\n","\n","That this is allowed should be obvious: if ${\\boldsymbol{w}^*}^T\\boldsymbol \\ge 0$ then\n","$-{\\boldsymbol{w}^*}^T\\boldsymbol < 0$. The conclusion therefore is that\n","we can replace any two class dataset that is linearly separable by a one\n","class dataset that consists of patterns that are all on one side of a\n","hyperplane through the origin. This hyperplane is as yet unknown, but\n","linear separability implies it exists.\n","\n","Lastly, we normalise all input patterns $\\mathcal{x}_i$, so that they\n","have length 1. This does not affect the classification, as\n","multiplication by a constant positive factor leaves the classification\n","of $\\boldsymbol{x}_i$ unchanged.\n","\n","We can now work with a simplified version of the algorithm.\n","\n","1.  **Start**: Choose a random set of weights: $\\boldsymbol{w}_0$. For\n","    later reference, denote the current set of weights by\n","   $\\boldsymbol{w}_i$, so a the start\n","   $\\boldsymbol{w}_i = \\boldsymbol{w}_0$.\n","\n","2.  **Continue:** Pick a random data point\n","  $(\\boldsymbol{x}_j, d_j), j = 1, \\cdots, D$, where\n","  $\\boldsymbol{x}_j$ is a point for which classification\n","  $d_j \\in \\left\\{ 0, 1 \\right\\}$ is given.\n","\n","3.  **Evaluate** $w_{i}^{T} x_{j} .$ If $\\left\\{\\begin{array}{cc}w^{T} x_{j} \\geq 0 & \\text { goto Continue } \\\\ w^{T} x_{j}<0 & \\text { goto Add }\\end{array}\\right.$\n","\n","4.  **Add:** $\\boldsymbol{w}_{i+1} = \\boldsymbol{w}_i + \\boldsymbol{x}$;\n","  Goto Continue\n","\n","In this setting, we can formulate precisely what we mean by linear\n","separability.\n","\n","**Definition:** there exists an as yet unknown weight vector $\\boldsymbol{w}^*$ such\n","that for each $\\boldsymbol{x}_i \\in \\mathcal{C}_1$,\n","${\\boldsymbol{w}^*}^T \\boldsymbol{x}_i \\ge \\delta$ for some $\\delta >0$.\n","Without loss of generality, we will assume that this vector is\n","normalised, i.e.,\n","\n","$$\n","\\left|w^{*}\\right|=1\n","$$\n","\n","We can multiply\n","$\\boldsymbol{w}$ by any positive factor; this does not affect\n","classification.\n","\n","**Perceptron Theorem:** Imagine now that we apply the algorithm and find a sequence of weights.\n","We start with a set of weights $\\boldsymbol{w}_0$, which according to\n","the algorithm are random numbers. We then cycle through set\n","$\\mathcal{C}_i$ trying out randomly selected patterns. Whenever the\n","algorithm goes through **ADD**, a new set of weights will be produced,\n","moving from $\\boldsymbol{w}_i$ to $\\boldsymbol{w}_{i+1}$. In this way we\n","produce a sequence of updated weights. The perceptron theorem states\n","that for linearly separable data this sequence is finite, i.e. at some\n","point the algorithm will have found a set of weights that classifies all\n","input patterns correctly.\n","\n","In order to prove this, we will consider the following quantity:\n","\n","$$\\cos \\angle(\\boldsymbol{w}^*, \\boldsymbol{w}_i) \\equiv \\frac{ {\\boldsymbol{w}^*}^T  \\boldsymbol{w}_i}{\\mid \\boldsymbol{w}_i \\mid}$$\n","\n","By the definition of the scalar product, this is indeed a cosine, so we\n","know that $0 \\le \\cos \\angle(\\boldsymbol{w}^*, \\boldsymbol{w}_i) \\le 1$.\n","Because we do not know $\\boldsymbol{w}^*$, we cannot calculate this\n","quantity directly, but we can say something about the way it changes,\n","when we move from $\\boldsymbol{w}_i$ to $\\boldsymbol{w}_{i+1}$.\n","\n","We will treat the numerator and denominator separately. What happens to\n","the numerator when we move from\n","$\\boldsymbol{w}_{i} \\rightarrow \\boldsymbol{w}_{i+1}$? We know that this\n","must happen in the **ADD** branch so, we must have hit a pattern\n","$\\boldsymbol{x}$ that is wrongly classified.\n","\n","$${\\boldsymbol{w}^*}^T \\boldsymbol{w}_{i+1} = {\\boldsymbol{w}^*}^T (\\boldsymbol{w}_i + \\boldsymbol{x})  = {\\boldsymbol{w}^*}^T \\boldsymbol{w}_i + {\\boldsymbol{w}^*}^T \\boldsymbol{x}$$\n","\n","Although we do not know $\\boldsymbol{w}^*$, we can say something about\n","${\\boldsymbol{w}^*}^T \\boldsymbol{x}$. This quantity is positive since\n","the pattern $\\boldsymbol{x} \\in \\mathcal{C}_1$ and by definition\n","${\\boldsymbol{w}^*}^T \\boldsymbol{x} \\ge \\delta$ for some $\\delta > 0$.\n","So,\n","\n","$${\\boldsymbol{w}^*}^T \\boldsymbol{w}_{i+1} >= {\\boldsymbol{w}^*}^T \\boldsymbol{w}_i + \\delta$$\n","\n","Now, $\\delta$ is a property of the dataset, which is fixed, finite and\n","positive. The numerator increases by *at least* a fixed positive amount,\n","each time the algorithm goes through **ADD**.\n","\n","What about the denominator? The square of the denominator is given by\n","\n","$$\\boldsymbol{w}^T_{i+1} \\boldsymbol{w}^T_{i+1} = (\\boldsymbol{w}_i^T + \\boldsymbol{x}^T)(\\boldsymbol{w}_i + \\boldsymbol{x}) = \\boldsymbol{w}^T_{i}\\boldsymbol{w}_i +2 \\boldsymbol{w}^T_i \\boldsymbol{x} + 1$$\n","\n","Here we used that the vectors in our training set are normalised. Since\n","a weight update took place, it must have been the case that\n","$\\boldsymbol{w}^T_i \\boldsymbol{x} <0$, otherwise, no update would have\n","taken place. We are therefore certain that:\n","\n","$$\\boldsymbol{w}^T_{i+1} \\boldsymbol{w}_{i+1} <  \\boldsymbol{w}^T_{i}\\boldsymbol{w}_i  + 1$$\n","\n","This implies that the quantity\n","\n","$$\\frac{ {\\boldsymbol{w}^*}^T  \\boldsymbol{w}_i}{\\mid \\boldsymbol{w}_i \\mid}$$\n","\n","has increased by at least $\\sqrt{M}\\delta$ after $M$ updates. Since this\n","quantity, being equal to a cosine, must remain smaller than one, only a\n","finite number of updates, at most $M =\\frac{1}{\\delta^2}$ can be made,\n","otherwise we get a contradiction. This proves the perceptron theorem.\n","\n","\n","\n"],"metadata":{"id":"_hK7A-37lUtF"}}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.8"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}