{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Computation - 2021"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial - Variational Auto-Encoders (VAEs)\n",
    "\n",
    "**Aims of this tutorial**:\n",
    "- Implement and train Variational Auto-Encoders (VAEs) in Pytorch.\n",
    "- Investigate how the learned latent space looks.\n",
    "- Investigate whether we can synthesize new data with Variational Auto-Encoders.\n",
    "- Investigate whether Variational Auto-Encoders trained with unlabelled data are useful to improve training of Supervised Classifiers when labelled data are limited.\n",
    "\n",
    "It may be long, but it should be easy to complete. The core points investigated here are of **high importance and part of the assessable material for the course**.\n",
    "\n",
    "**Prerequisites**:\n",
    "- Familiar with python, numpy, and basic PyTorch.\n",
    "- Familiar with MNIST, Multi-Layer-Perceptrons (MLPs), and AutoEncoders (previous tutorial).\n",
    "\n",
    "\n",
    "**Notes**:\n",
    "- Docs for Pytorch's functions you will need:  \n",
    "https://pytorch.org/docs/stable/tensors.html  \n",
    "https://pytorch.org/docs/stable/nn.html  \n",
    "- Some helper functions for loading and plotting data are given in `./utils` folder. They will be used out of the box below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminary: Loading and refreshing MNIST\n",
    "\n",
    "Loading and inspecting MNIST data. Same as previous tutorial..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# The below is for auto-reloading external modules after they are changed, such as those in ./utils.\n",
    "# Issue: http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "from utils.data_utils import get_mnist # Helper function. Use it out of the box.\n",
    "\n",
    "# Constants\n",
    "DATA_DIR = './data/mnist' # Location we will keep the data.\n",
    "SEED = 111111\n",
    "\n",
    "# If datasets are not at specified location, they will be downloaded.\n",
    "train_imgs, train_lbls = get_mnist(data_dir=DATA_DIR, train=True, download=True)\n",
    "test_imgs, test_lbls = get_mnist(data_dir=DATA_DIR, train=False, download=True)\n",
    "\n",
    "print(\"[train_imgs] Type: \", type(train_imgs), \"|| Shape:\", train_imgs.shape, \"|| Data type: \", train_imgs.dtype )\n",
    "print(\"[train_lbls] Type: \", type(train_lbls), \"|| Shape:\", train_lbls.shape, \"|| Data type: \", train_lbls.dtype )\n",
    "print('Class labels in train = ', np.unique(train_lbls))\n",
    "\n",
    "print(\"[test_imgs] Type: \", type(test_imgs), \"|| Shape:\", test_imgs.shape, \" || Data type: \", test_imgs.dtype )\n",
    "print(\"[test_lbls] Type: \", type(test_lbls), \"|| Shape:\", test_lbls.shape, \" || Data type: \", test_lbls.dtype )\n",
    "print('Class labels in test = ', np.unique(test_lbls))\n",
    "\n",
    "N_tr_imgs = train_imgs.shape[0] # N hereafter. Number of training images in database.\n",
    "H_height = train_imgs.shape[1] # H hereafter\n",
    "W_width = train_imgs.shape[2] # W hereafter\n",
    "C_classes = len(np.unique(train_lbls)) # C hereafter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above we see that data have been loaded in *numpy arrays*.    \n",
    "Arrays with images have **shape ( N = number of images, H = height, W = width )**.  \n",
    "Arrays with labels have **shape ( N = number of images)**, holding one integer per image, the digit's class.\n",
    "\n",
    "MNIST comprises of a **train set (N_tr = 60000) images** and a **test set (N_te = 10000) images**.  \n",
    "We will use the train set for unsupervised learning. The test set will only be used for evaluating generalisation of classifiers towards the end of the tutorial.\n",
    "\n",
    "Lets plot a few image in one collage to have a look..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from utils.plotting import plot_grid_of_images # Helper functions, use out of the box.\n",
    "plot_grid_of_images(train_imgs[0:100], n_imgs_per_row=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the intensities in the images take **values from 0 to 255**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminary: Data pre-processing\n",
    "\n",
    "A first step in almost all pipelines is to pre-process the data, to make them more appropriate for a model.\n",
    "\n",
    "Below, we will perform 3 points:  \n",
    "a) Change the labels from an integer representation to a **one-hot representation** of the **C=10 classes**.\\\n",
    "b) Re-scale the **intensities** in the images, from the range \\[0,255\\], to be instead in the range \\[-1,+1\\].\\\n",
    "c) **Vectorise the 2D images into 1D vectors for the MLP**, which only gets vectors as input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a) Change representation of labels to one-hot vectors of length C=10.\n",
    "train_lbls_onehot = np.zeros(shape=(train_lbls.shape[0], C_classes ) )\n",
    "train_lbls_onehot[ np.arange(train_lbls_onehot.shape[0]), train_lbls ] = 1\n",
    "test_lbls_onehot = np.zeros(shape=(test_lbls.shape[0], C_classes ) )\n",
    "test_lbls_onehot[ np.arange(test_lbls_onehot.shape[0]), test_lbls ] = 1\n",
    "print(\"BEFORE: [train_lbls]        Type: \", type(train_lbls), \"|| Shape:\", train_lbls.shape, \" || Data type: \", train_lbls.dtype )\n",
    "print(\"AFTER : [train_lbls_onehot] Type: \", type(train_lbls_onehot), \"|| Shape:\", train_lbls_onehot.shape, \" || Data type: \", train_lbls_onehot.dtype )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# b) Re-scale image intensities, from [0,255] to [-1, +1].\n",
    "# This commonly facilitates learning:\n",
    "# A zero-centered signal with small magnitude allows avoiding exploding/vanishing problems easier.\n",
    "from utils.data_utils import normalize_int_whole_database # Helper function. Use out of the box.\n",
    "train_imgs = normalize_int_whole_database(train_imgs, norm_type=\"minus_1_to_1\")\n",
    "test_imgs = normalize_int_whole_database(test_imgs, norm_type=\"minus_1_to_1\")\n",
    "\n",
    "# Lets plot one image.\n",
    "from utils.plotting import plot_image, plot_images # Helper function, use out of the box.\n",
    "index = 0  # Try any, up to 60000\n",
    "print(\"Plotting image of index: [\", index, \"]\")\n",
    "print(\"Class label for this image is: \", train_lbls[index])\n",
    "print(\"One-hot label representation: [\", train_lbls_onehot[index], \"]\")\n",
    "plot_image(train_imgs[index])\n",
    "# Notice the magnitude of intensities. Black is now negative and white is positive float.\n",
    "# Compare with intensities of figure further above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# c) Flatten the images, from 2D matrices to 1D vectors. MLPs take feature-vectors as input, not 2D images.\n",
    "train_imgs_flat = train_imgs.reshape([train_imgs.shape[0], -1]) # Preserve 1st dim (S = num Samples), flatten others.\n",
    "test_imgs_flat = test_imgs.reshape([test_imgs.shape[0], -1])\n",
    "print(\"Shape of numpy array holding the training database:\")\n",
    "print(\"Original : [N, H, W] = [\", train_imgs.shape , \"]\")\n",
    "print(\"Flattened: [N, H*W]  = [\", train_imgs_flat.shape , \"]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Variational Auto-Encoder\n",
    "\n",
    "In this task, you are called to implement the architecture and losses of a Variational Auto-Encoder.\n",
    "**Fill in the blanks where requested**, to create the below architecture:\n",
    "\n",
    "![title](./documentation/vae_2d.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class Network():\n",
    "    \n",
    "    def backward_pass(self, loss):\n",
    "        # Performs back propagation and computes gradients\n",
    "        # With PyTorch, we do not need to compute gradients analytically for parameters were requires_grads=True, \n",
    "        # Calling loss.backward(), torch's Autograd automatically computes grads of loss wrt each parameter p,...\n",
    "        # ... and **puts them in p.grad**. Return them in a list.\n",
    "        loss.backward()\n",
    "        grads = [param.grad for param in self.params]\n",
    "        return grads\n",
    "    \n",
    "    \n",
    "class VAE(Network):\n",
    "    def __init__(self, rng, D_in, D_hid_enc, D_bottleneck, D_hid_dec):\n",
    "        # Construct and initialize network parameters\n",
    "        D_in = D_in # Dimension of input feature-vectors. Length of a vectorised image.\n",
    "        D_hid_1 = D_hid_enc # Dimension of Encoder's hidden layer\n",
    "        D_hid_2 = D_bottleneck\n",
    "        D_hid_3 = D_hid_dec  # Dimension of Decoder's hidden layer\n",
    "        D_out = D_in # Dimension of Output layer.\n",
    "        \n",
    "        self.D_bottleneck = D_bottleneck  # Keep track of it, we will need it.\n",
    "        \n",
    "        ##### TODO: Initialize the VAE's parameters. Also see forward_pass(...)) #####################\n",
    "        # Dimensions of parameter tensors are (number of neurons + 1) per layer, to account for +1 bias.\n",
    "        # -- (Encoder) layer 1\n",
    "        w1_init = rng.normal(loc=0.0, scale=0.01, size=(D_in+1, D_hid_1))\n",
    "        # -- (Encoder) layer 2, predicting p(z|x)\n",
    "        w2_mu_init = rng.normal(loc=0.0, scale=0.01, size=(D_hid_1+1, D_hid_2))  # Weights for predicting means.\n",
    "        w2_std_init = rng.normal(loc=0.0, scale=0.01, size=(??????+1, D_hid_2))  # <----- weights for predicting std\n",
    "        # -- (Decoder) layer 3\n",
    "        w3_init = rng.normal(loc=0.0, scale=0.01, size=(D_hid_2+1, D_hid_3))\n",
    "        # -- (Decoder) layer 4, the output layer\n",
    "        w4_init = rng.normal(loc=0.0, scale=0.01, size=(D_hid_3+1, D_out))\n",
    "        \n",
    "        # Pytorch tensors, parameters of the model\n",
    "        # Use the above numpy arrays as of random floats as initialization for the Pytorch weights.\n",
    "        # (Encoder)\n",
    "        w1 = torch.tensor(w1_init, dtype=torch.float, requires_grad=True)\n",
    "        # (Encoder) Layer 2, predicting p(z|x)\n",
    "        w2_mu = torch.tensor(?????????, dtype=torch.float, requires_grad=True)   # <------- ?????\n",
    "        w2_std = torch.tensor(w2_std_init, dtype=torch.float, requires_grad=True)\n",
    "        # (Decoder)\n",
    "        w3 = torch.tensor(w3_init, dtype=torch.float, requires_grad=True)\n",
    "        w4 = torch.tensor(w4_init, dtype=torch.float, requires_grad=True)\n",
    "        #########################################################################################\n",
    "            \n",
    "        # Keep track of all trainable parameters:\n",
    "        self.params = [w1, w2_mu, w2_std, w3, w4]\n",
    "\n",
    "        \n",
    "    \n",
    "    def encode(self, batch_imgs):\n",
    "        # batch_imgs: Numpy array or Pytorch tensor of shape: [number of inputs, dimensionality of x]\n",
    "        [w1, w2_mu, w2_std, w3, w4] = self.params\n",
    "        \n",
    "        batch_imgs_t = torch.tensor(batch_imgs, dtype=torch.float) if type(batch_imgs) is np.ndarray else batch_imgs\n",
    "        \n",
    "        unary_feature_for_bias = torch.ones(size=(batch_imgs_t.shape[0], 1)) # [N, 1] column vector.\n",
    "        x = torch.cat((batch_imgs_t, unary_feature_for_bias), dim=1) # Extra feature=1 for bias.\n",
    "        \n",
    "        # ========== TODO: Fill in the gaps with the correct parameters of the VAE ========\n",
    "        # Encoder's Layer 1\n",
    "        h1_preact = x.mm(w1)\n",
    "        h1_act = h1_preact.clamp(min=0)\n",
    "        # Encoder's Layer 2 (predicting p(z|x) of Z coding):\n",
    "        h1_ext = torch.cat((h1_act, unary_feature_for_bias), dim=1)\n",
    "        # ... mu\n",
    "        h2_mu_preact = ???????.mm(w2_mu)   # <-------------------------------\n",
    "        h2_mu_act = h2_mu_preact #h2_preact.clamp(min=0)\n",
    "        # ... log(std). Why do we do this, instead of directly predicting std deviation? See lecture slides.\n",
    "        h2_logstd_preact = h1_ext.mm(??????????)  # <------------------------\n",
    "        h2_logstd_act = h2_logstd_preact  # No (linear) activation function in this tutorial, but can use any.\n",
    "        # ==============================================================================\n",
    "        \n",
    "        z_coding = (h2_mu_act, h2_logstd_act)\n",
    "        \n",
    "        return z_coding\n",
    "        \n",
    "        \n",
    "    def decode(self, z_codes):\n",
    "        # z_codes: numpy array or pytorch tensor, shape [N, dimensionality of Z]\n",
    "        [w1, w2_mu, w2_std, w3, w4] = self.params\n",
    "        \n",
    "        z_codes_t = torch.tensor(z_codes, dtype=torch.float) if type(z_codes) is np.ndarray else z_codes\n",
    "        \n",
    "        unary_feature_for_bias = torch.ones(size=(z_codes_t.shape[0], 1)) # [N, 1] column vector.\n",
    "        \n",
    "        # ========== TODO: Fill in the gaps with the correct parameters of the VAE ========\n",
    "        # Decoder's 1st layer (Layer 3 of whole VAE):\n",
    "        h2_ext = torch.cat((z_codes_t, unary_feature_for_bias), dim=1)\n",
    "        h3_preact = h2_ext.mm(???????)  # <------------------------\n",
    "        h3_act = h3_preact.clamp(min=0)\n",
    "        # Decoder's 2nd layer (Layer 4 of whole VAE): The output layer.\n",
    "        h3_ext = torch.cat((h3_act, unary_feature_for_bias), dim=1)\n",
    "        h4_preact = h3_ext.mm(w4)\n",
    "        h4_act = torch.tanh(h4_preact)\n",
    "        # ==============================================================================\n",
    "        \n",
    "        # Output\n",
    "        x_pred = h4_act\n",
    "        \n",
    "        return x_pred\n",
    "        \n",
    "        \n",
    "    def sample_with_reparameterization(self, z_mu, z_logstd):\n",
    "        # Reparameterization trick to sample from N(mu, var) using N(0,1) as intermediate step.\n",
    "        # param z_mu: Tensor. Mean of the predicted Gaussian p(z|x). Shape: [Num samples, Dimensionality of Z]\n",
    "        # param z_logstd: Tensor. Log of standard deviation of predicted Gaussian p(z|x). [Num samples, Dim of Z]\n",
    "        # return: Tensor. [Num samples, Dim of Z]\n",
    "        \n",
    "        N_samples = z_mu.shape[0]\n",
    "        Z_dims = z_mu.shape[1]\n",
    "\n",
    "        # ========== TODO: Fill in the gaps to complete the reparameterization trick ========\n",
    "        z_std = torch.exp(???????)       #   <------------------- compute std from log(std)\n",
    "        eps = torch.randn(size=[N_samples, Z_dims])  # torch.randn_like(std)\n",
    "        z_samples = ??????? * z_std + z_mu    #           <---------------- Re-parameterization trick\n",
    "        # ==============================================================================\n",
    "        \n",
    "        return z_samples\n",
    "        \n",
    "        \n",
    "    def forward_pass(self, batch_imgs):\n",
    "        # Performed at every batch during training.\n",
    "        # Takes an input batch, encodes it, samples a code from p(z|x) with reparameterization, decodes it.\n",
    "        # Returns: Reconstruction x_pred, predicted means z_mu, predicted log(std) z_logstd, sampled codes z_samples.\n",
    "        batch_imgs_t = torch.tensor(batch_imgs, dtype=torch.float)  # Makes numpy array to pytorch tensor.\n",
    "        \n",
    "        # ========== TODO: Call the appropriate functions, as you defined them above ========\n",
    "        # Encoder\n",
    "        z_mu, z_logstd = self.????????(batch_imgs_t)  # <----------------------- ????????????\n",
    "        z_samples = self.??????????(z_mu, z_logstd)  # <------------- ????????????\n",
    "        # Decoder\n",
    "        x_pred = self.?????????(z_samples)  # <------------- ????????????\n",
    "        # ===================================================================================\n",
    "        \n",
    "        return (x_pred, z_mu, z_logstd, z_samples)\n",
    "        \n",
    "        \n",
    "def reconstruction_loss(x_pred, x_real, eps=1e-7):\n",
    "    # x_pred: [N, D_out] Prediction returned by forward_pass. Numpy array of shape [N, D_out]\n",
    "    # x_real: [N, D_in]\n",
    "    \n",
    "    # If number array is given, change it to a Torch tensor.\n",
    "    x_pred = torch.tensor(x_pred, dtype=torch.float) if type(x_pred) is np.ndarray else x_pred\n",
    "    x_real = torch.tensor(x_real, dtype=torch.float) if type(x_real) is np.ndarray else x_real\n",
    "    \n",
    "    ######## TODO: Complete the calculation of Reconstruction loss for each sample ###########\n",
    "    loss_recon = torch.mean(torch.square(???????? - x_real), dim=1)  # <---------- same as for AEs\n",
    "    ##########################################################################################\n",
    "    \n",
    "    cost = torch.mean(loss_recon, dim=0) # Expectation of loss: Mean over samples (axis=0).\n",
    "    \n",
    "    return cost\n",
    "\n",
    "\n",
    "def regularizer_loss(mu, log_std):\n",
    "    # mu: Tensor, [number of samples, dimensionality of Z]. Predicted means per z dimension\n",
    "    # log_std: Tensor, [number of samples, dimensionality of Z]. Predicted log(std.dev.) per z dimension.\n",
    "    \n",
    "    ######## TODO: Complete the calculation of the Regularizer for each sample ###########\n",
    "    std = torch.exp(log_std)  # Compute std.dev. from log(std.dev.)\n",
    "    reg_loss_per_sample = 0.5 * torch.sum(????**2 + std**2 - 1 - 2 * log_std, dim = 1)  # <------ See lecture slides\n",
    "    reg_loss = torch.mean(reg_loss_per_sample, dim = 0)  # Mean over samples.\n",
    "    ##########################################################################################\n",
    "    \n",
    "    return reg_loss\n",
    "\n",
    "\n",
    "def vae_loss(x_real, x_pred, z_mu, z_logstd, lambda_rec=1., lambda_reg=0.005, eps=1e-7):\n",
    "    \n",
    "    rec_loss = reconstruction_loss(x_pred, x_real, eps=1e-7)\n",
    "    reg_loss = regularizer_loss(z_mu, z_logstd)\n",
    "    \n",
    "    ################### TODO: compute the total loss: #####################################\n",
    "    # ...by weighting the reconstruction loss by lambda_rec, and the Regularizer by lambda_reg\n",
    "    weighted_rec_loss = lambda_rec * ????????\n",
    "    weighted_reg_loss = lambda_reg * ????????\n",
    "    total_loss = weighted_rec_loss + weighted_reg_loss\n",
    "    #######################################################################################\n",
    "    \n",
    "    return total_loss, weighted_rec_loss, weighted_reg_loss\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If this task is completed correctly, you should be able to run the cell and get no errors. Though no output will be given yet. We will use this in the next task, and then we will find out if everything went well :-)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Unsupervised training of VAE\n",
    "\n",
    "Below you are given the main training function, which performs gradient descent in unsupervised fashion.\n",
    "\n",
    "In the below code, a random batch of images is given to the VAE for a forward_pass (encode, sampling via reparameterization trick, decode). Then, it returns the reconstruction of the sample (x_pred), the predicted mean and logarithm(of standard deviation) of the distribution p(z|x) of codes z for the code of sample x. It also returns the code z passed to the decoder, which here is a sample from the predicted p(z|x) for each sample.\n",
    "\n",
    "Then, the total loss of the VAE is calculated via vae_loss(), implemented above, and minimized via Adam.\n",
    "\n",
    "Fill in the 2 blanks in the code, to simply pass the correct parameters (predicted means (mu) and log(std.dev)) to the loss function (vae_loss()), so that it can get optimized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.plotting import plot_train_progress_VAE, plot_grids_of_images  # Use out of the box\n",
    "\n",
    "\n",
    "def get_random_batch(train_imgs, train_lbls, batch_size, rng):\n",
    "    # train_imgs: Images. Numpy array of shape [N, H * W]\n",
    "    # train_lbls: Labels of images. None, or Numpy array of shape [N, C_classes], one hot label for each image.\n",
    "    # batch_size: integer. Size that the batch should have.\n",
    "    \n",
    "    indices = range(0, batch_size)  # Remove this line after you fill-in and un-comment the below. \n",
    "    indices = rng.randint(low=0, high=train_imgs.shape[0], size=batch_size, dtype='int32')\n",
    "    \n",
    "    train_imgs_batch = train_imgs[indices]\n",
    "    if train_lbls is not None:  # Enables function to be used both for supervised and unsupervised learning\n",
    "        train_lbls_batch = train_lbls[indices]\n",
    "    else:\n",
    "        train_lbls_batch = None\n",
    "    return [train_imgs_batch, train_lbls_batch]\n",
    "\n",
    "\n",
    "def unsupervised_training_VAE(net,\n",
    "                             loss_func,\n",
    "                             lambda_rec,\n",
    "                             lambda_reg,\n",
    "                             rng,\n",
    "                             train_imgs_all,\n",
    "                             batch_size,\n",
    "                             learning_rate,\n",
    "                             total_iters,\n",
    "                             iters_per_recon_plot=-1):\n",
    "    # net: Instance of a model. See classes: Autoencoder, MLPClassifier, etc further below\n",
    "    # loss_func: Function that computes the loss. See functions: reconstruction_loss or cross_entropy.\n",
    "    # lambda_rec: weighing of reconstruction loss in total loss. Total = lambda_rec * rec_loss + lambda_reg * reg_loss\n",
    "    # lambda_reg: same as above, but for regularizer\n",
    "    # rng: numpy random number generator\n",
    "    # train_imgs_all: All the training images. Numpy array, shape [N_tr, H, W]\n",
    "    # batch_size: Size of the batch that should be processed per SGD iteration by a model.\n",
    "    # learning_rate: self explanatory.\n",
    "    # total_iters: how many SGD iterations to perform.\n",
    "    # iters_per_recon_plot: Integer. Every that many iterations the model predicts training images ...\n",
    "    #                      ...and we plot their reconstruction. For visual observation of the results.\n",
    "    loss_total_to_plot = []\n",
    "    loss_rec_to_plot = []\n",
    "    loss_reg_to_plot = []\n",
    "    \n",
    "    optimizer = optim.Adam(net.params, lr=learning_rate)  # Will use PyTorch's Adam optimizer out of the box\n",
    "        \n",
    "    for t in range(total_iters):\n",
    "        # Sample batch for this SGD iteration\n",
    "        x_batch, _ = get_random_batch(train_imgs_all, None, batch_size, rng)\n",
    "        \n",
    "        ################### TODO: compute the total loss: ################################################\n",
    "        # Pass parameters of the predicted distribution per x (mean mu and log(std.dev) to the loss function\n",
    "        \n",
    "        # Forward pass: Encodes, samples via reparameterization trick, decodes\n",
    "        x_pred, z_mu, z_logstd, z_codes = net.forward_pass(x_batch)\n",
    "\n",
    "        # Compute loss:\n",
    "        total_loss, rec_loss, reg_loss = loss_func(x_batch, x_pred, ??????, ??????, lambda_rec, lambda_reg) # <-------------\n",
    "        ####################################################################################################\n",
    "        # Pytorch way\n",
    "        optimizer.zero_grad()\n",
    "        _ = net.backward_pass(total_loss)\n",
    "        optimizer.step()\n",
    "        \n",
    "        # ==== Report training loss and accuracy ======\n",
    "        total_loss_np = total_loss if type(total_loss) is type(float) else total_loss.item()  # Pytorch returns tensor. Cast to float\n",
    "        rec_loss_np = rec_loss if type(rec_loss) is type(float) else rec_loss.item()\n",
    "        reg_loss_np = reg_loss if type(reg_loss) is type(float) else reg_loss.item()\n",
    "        if t%10==0:  # Print every 10 iterations\n",
    "            print(\"[iter:\", t, \"]: Total training Loss: {0:.2f}\".format(total_loss_np))\n",
    "        loss_total_to_plot.append(total_loss_np)\n",
    "        loss_rec_to_plot.append(rec_loss_np)\n",
    "        loss_reg_to_plot.append(reg_loss_np)\n",
    "        \n",
    "        # Every few iterations, show reconstructions\n",
    "        if t==total_iters-1 or t%iters_per_recon_plot == 0:\n",
    "            # Reconstruct all images, to plot reconstructions.\n",
    "            x_pred_all, z_mu_all, z_logstd_all, z_codes_all = net.forward_pass(train_imgs_all)\n",
    "            # Cast tensors to numpy arrays\n",
    "            x_pred_all_np = x_pred_all if type(x_pred_all) is np.ndarray else x_pred_all.detach().numpy()\n",
    "            \n",
    "            # Predicted reconstructions have vector shape. Reshape them to original image shape.\n",
    "            train_imgs_resh = train_imgs_all.reshape([train_imgs_all.shape[0], H_height, W_width])\n",
    "            x_pred_all_np_resh = x_pred_all_np.reshape([train_imgs_all.shape[0], H_height, W_width])\n",
    "            \n",
    "            # Plot a few images, originals and predicted reconstructions.\n",
    "            plot_grids_of_images([train_imgs_resh[0:100], x_pred_all_np_resh[0:100]],\n",
    "                                  titles=[\"Real\", \"Reconstructions\"],\n",
    "                                  n_imgs_per_row=10,\n",
    "                                  dynamically=True)\n",
    "            \n",
    "    # In the end of the process, plot loss.\n",
    "    plot_train_progress_VAE(loss_total_to_plot, loss_rec_to_plot, loss_reg_to_plot, iters_per_point=1, y_lims=[1., 1., None])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you completed the above correctly you should get no error message here. Finally, lets use the above and implementation of VAE in Task 1, to train a VAE!\n",
    "\n",
    "Fill in the below gap, to make the VAE shown in figure of Task1 with a 2-dimensional Z representation..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################### TODO: Fill in the blank ##############################\n",
    "# Create the network\n",
    "rng = np.random.RandomState(seed=SEED)\n",
    "vae = VAE(rng=rng,\n",
    "          D_in=H_height*W_width,\n",
    "          D_hid_enc=256,\n",
    "          D_bottleneck=??????,  # <--- Set to correct value for instantiating VAE shown & implemented in Task 1. Note: We treat D as dimensionality of Z, rather than number of neurons.\n",
    "          D_hid_dec=256)\n",
    "########################################################################\n",
    "# Start training\n",
    "unsupervised_training_VAE(vae,\n",
    "                          vae_loss,\n",
    "                          lambda_rec=1.0,  # <-------- lambda_rec, weight on reconstruction loss.\n",
    "                          lambda_reg=0.005,  # <------- lambda_reg, weight on regularizer. 0.005 works ok.\n",
    "                          rng=rng,\n",
    "                          train_imgs_all=train_imgs_flat,\n",
    "                          batch_size=40,\n",
    "                          learning_rate=3e-3,\n",
    "                          total_iters=1000,\n",
    "                          iters_per_recon_plot=50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above requires you to have completed both Task 1 and Task 2. If everything is completed correctly, you should see the model getting trained and the total training loss printed every few iterations.\n",
    "\n",
    "In the end of training, after 1000 iterations, you will see 3 curve, one for the TOTAL training loss, one for the *weighted* reconstruction loss, and one for the *weighted* regularization loss (*weighted* = after multiplication with the weights *lambda_rec* and *lambda_reg* respectively, when computing the total loss. If everything is done well, the total and reconstruction loss are expected to decrease down to approximately 0.25, and the regularizer down to approx 0.01-0.02.\n",
    "\n",
    "You should also see printed side by side a set of real images, and their reconstructed version.\\\n",
    "In the end, the reconstructions should start being reasonable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: Encode training data in Z representation and examine\n",
    "\n",
    "We now have a trained VAE with 2-dimensional representation Z from Task 2. We will here use it to encode training data and obtain the means and standard deviations of the predicted distributions for the codes p(z|x). We will then plot the predicted means for p(z|x) for each sample x, in a 2D plot, to observe how codes are clustered.\n",
    "\n",
    "Note: Fill in the 1 blank below, run the code, and observe output...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def encode_training_images(net,\n",
    "                           imgs_flat,\n",
    "                           lbls,\n",
    "                           batch_size,\n",
    "                           total_iterations=None,\n",
    "                           plot_2d_embedding=True,\n",
    "                           plot_hist_mu_std_for_dim=0):\n",
    "    # This function encodes images, plots the first 2 dimensions of the codes in a plot, and finally...\n",
    "    # ... returns the minimum and maximum values of the codes for each dimensions of Z.\n",
    "    # ... We will use  this at a layer task.\n",
    "    # Arguments:\n",
    "    # imgs_flat: Numpy array of shape [Number of images, H * W]\n",
    "    # lbls: Numpy array of shape [number of images], with 1 integer per image. The integer is the class (digit).\n",
    "    # total_iterations: How many batches to encode. We will use this so that we dont encode and plot ...\n",
    "    # ... the whoooole training database, because the plot will get cluttered with 60000 points.\n",
    "    \n",
    "    # If total iterations is None, the function will just iterate over all data, by breaking them into batches.    \n",
    "    if total_iterations is None:\n",
    "        total_iterations = (train_imgs_flat.shape[0] - 1) // batch_size + 1\n",
    "    \n",
    "    z_mu_all = []\n",
    "    z_std_all = []\n",
    "    lbls_all = []\n",
    "    for t in range(total_iterations):\n",
    "        # Sample batch for this SGD iteration\n",
    "        x_batch = imgs_flat[t*batch_size: (t+1)*batch_size]\n",
    "        lbls_batch = lbls[t*batch_size: (t+1)*batch_size]  # Just to color the embeddings (z codes) in the plot.\n",
    "        \n",
    "        ####### TODO: Fill in the blank ##################################\n",
    "        # Encode a batch of x inputs:\n",
    "        z_mu, z_logstd = net.encode(????????)  # <------------------------\n",
    "        #################################################################\n",
    "        z_mu_np = z_mu if type(z_mu) is np.ndarray else z_mu.detach().numpy()\n",
    "        z_logstd_np = z_logstd if type(z_logstd) is np.ndarray else z_logstd.detach().numpy()\n",
    "        \n",
    "        z_mu_all.append(z_mu_np)\n",
    "        z_std_all.append(np.exp(z_logstd_np))\n",
    "        lbls_all.append(lbls_batch)\n",
    "        \n",
    "    z_mu_all = np.concatenate(z_mu_all)  # Make list of arrays in one array by concatenating along dim=0 (image index)\n",
    "    z_std_all = np.concatenate(z_std_all)\n",
    "    lbls_all = np.concatenate(lbls_all)\n",
    "    \n",
    "    if plot_2d_embedding:\n",
    "        print(\"Z-Space and the MEAN of the predicted p(z|x) for each sample (std.devs not shown)\")\n",
    "        # Plot the codes with different color per class in a scatter plot:\n",
    "        plt.scatter(z_mu_all[:,0], z_mu_all[:,1], c=lbls_all, alpha=0.5)  # Plot the first 2 dimensions.\n",
    "        plt.show()\n",
    "    \n",
    "    print(\"Histogram of values of the predicted MEANS\")\n",
    "    plt.hist(z_mu_all[:,plot_hist_mu_std_for_dim], bins=20)\n",
    "    plt.show()\n",
    "    print(\"Histogram of values of the predicted STANDARD DEVIATIONS\")\n",
    "    plt.hist(z_std_all[:,plot_hist_mu_std_for_dim], bins=20)\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "# Encode and plot\n",
    "encode_training_images(vae,\n",
    "                       train_imgs_flat,\n",
    "                       train_lbls,\n",
    "                       batch_size=100,\n",
    "                       total_iterations=200,\n",
    "                       plot_2d_embedding=True,\n",
    "                       plot_hist_mu_std_for_dim=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If all went well, you should see 3 plots:\n",
    "- top plot should show the 2D space of Z with 1 point per sample x. Only means are shown, not corresponding std.deviations.\n",
    "- A histogram of the values of predicted means (means for both dimensions of z aggregated)\n",
    "- A histogram of the values of predicted standard deviations (std.devs for both dimensions of z aggregated)\n",
    "\n",
    "\n",
    "**Questions:**\n",
    "- Around what value are they mostly centered? Why?\n",
    "- What range of values do the means span? Why?\n",
    "- What range of values do the standard deviations span? Why do they tend to be smaller than 1?\n",
    "- How does the form of the top plot compare with the same plot for basic Auto-Encoders? Compare range of values, existence of gaps between classes, and holes in the general space.\n",
    "- Observe the way the different samples are clustered by this VAE. Is this VAE as good for clustering as the basic AE from previous tutorial? Do you think that VAEs, in general, are better or worse than AEs in clustering datapoints? (Reminder: Clustering = similar points to similar codes, dissimilar points well separated and mapped to dissimilar codes) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4: Train VAE from Task 1 and 2 only with Reconstruction loss:\n",
    "\n",
    "The code below is complete. Just run it and compare results with those of previous Tasks, where the VAE was trained both with a reconstruction and the regularizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the network\n",
    "rng = np.random.RandomState(seed=SEED)\n",
    "vae_2 = VAE(rng=rng,\n",
    "            D_in=H_height*W_width,\n",
    "            D_hid_enc=256,\n",
    "            D_bottleneck=2,\n",
    "            D_hid_dec=256)\n",
    "# Start training\n",
    "unsupervised_training_VAE(vae_2,\n",
    "                          vae_loss,\n",
    "                          lambda_rec=1.0,\n",
    "                          lambda_reg=0.0,  # Essentially not minimizing regularizer. Only reconstruction.\n",
    "                          rng=rng,\n",
    "                          train_imgs_all=train_imgs_flat,\n",
    "                          batch_size=40,\n",
    "                          learning_rate=3e-3,\n",
    "                          total_iters=1000,\n",
    "                          iters_per_recon_plot=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Questions:**\n",
    "- Observe the Reconstruction loss for the VAE trained only with the reconstruction loss (here) and the VAE trained to minimize both the Reconstruction loss and the Regularizer (Task 2). How do they compare? (if not much difference is visually obvious, which one do you think should be smaller in theory?)\n",
    "- Which of the two VAEs do you expect should achieve better reconstruction in theory? VAE_2 from this task, trained only with reconstruction loss, or VAE from Task 2, trained also using the regularizer?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode and plot\n",
    "encode_training_images(vae_2, # The second VAE, trained only with Reconstruction loss.\n",
    "                       train_imgs_flat,\n",
    "                       train_lbls,\n",
    "                       batch_size=100,\n",
    "                       total_iterations=200,\n",
    "                       plot_2d_embedding=True,\n",
    "                       plot_hist_mu_std_for_dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Questions:**\n",
    "- Compare the 2D plots of codings in Z-space between this VAE and the one trained with both recon and regularizer loss in Task 3. What do you oberse in the way the different samples are encoded and grouped? How do you relate this to the basic AutoEncoder from previous tutorial?\n",
    "- Compare predicted means of p(z|x). Compare the values they take with those from Task 3. Does the reconstruction loss encourage them to keep low values? Is there a benefit for reconstruction if means get higher values?\n",
    "- Observe standard deviations predicted for p(z|x) in this task (reconstruction loss only). What values do they take? In comparision to values in Task-3, these values should be significant smaller. Why does the reconstruction loss encourage as small as possible standard deviations?\n",
    "- How do you relate this model with the basic Auto-Encoder?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 5: Train VAE from Task 1 and 2 to minimize only the Regularizer\n",
    "\n",
    "The code below is complete. Just run it and compare results with those of previous Tasks 2,3,4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the network\n",
    "rng = np.random.RandomState(seed=SEED)\n",
    "vae_3 = VAE(rng=rng,\n",
    "            D_in=H_height*W_width,\n",
    "            D_hid_enc=256,\n",
    "            D_bottleneck=2,\n",
    "            D_hid_dec=256)\n",
    "# Start training\n",
    "unsupervised_training_VAE(vae_3,\n",
    "                          vae_loss,\n",
    "                          lambda_rec=0.0,  # <------- No reconstruction loss. Only regularizer\n",
    "                          lambda_reg=0.005,\n",
    "                          rng=rng,\n",
    "                          train_imgs_all=train_imgs_flat,\n",
    "                          batch_size=40,\n",
    "                          learning_rate=3e-3,\n",
    "                          total_iters=1000,\n",
    "                          iters_per_recon_plot=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Questions:**\\\n",
    "    - How good are reconstructions? Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode and plot\n",
    "encode_training_images(vae_3, # The second VAE, trained only with Reconstruction loss.\n",
    "                       train_imgs_flat,\n",
    "                       train_lbls,\n",
    "                       batch_size=100,\n",
    "                       total_iterations=200,\n",
    "                       plot_2d_embedding=True,\n",
    "                       plot_hist_mu_std_for_dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Questions:**\n",
    "- Observe the predicted means of p(z|x). What values do they take? Why?\n",
    "- Observe the standard deviations of p(z|x). What values do they take? Why?\n",
    "- What type of information do you think this encoder has learned to represent about the data in space of Z?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 6: Train a VAE with a larger bottleneck layer\n",
    "\n",
    "Below, we train a VAE with a bottleneck layer of 32 dimensions (1 mu and std.dev predicted for each), and train it appropriately, both with the reconstruction and the regularizer. \n",
    "\n",
    "The code is complete. Just run it and observe the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same as in Task 2, but using a bottle neck with 32 dimension\n",
    "\n",
    "# Create the network\n",
    "rng = np.random.RandomState(seed=SEED)\n",
    "vae_wide = VAE(rng=rng,\n",
    "          D_in=H_height*W_width,\n",
    "          D_hid_enc=256,\n",
    "          D_bottleneck=32,  # <-----------------------------------\n",
    "          D_hid_dec=256)\n",
    "# Start training\n",
    "unsupervised_training_VAE(vae_wide,\n",
    "                          vae_loss,\n",
    "                          1.0,  # alpha on the recon loss.\n",
    "                          0.005,  # 0.005 works well for synthesis! 0.0005 better for smooth z values for 32n.\n",
    "                          rng,\n",
    "                          train_imgs_flat,\n",
    "                          batch_size=40,\n",
    "                          learning_rate=3e-3,  # 3e-3\n",
    "                          total_iters=1000,\n",
    "                          iters_per_recon_plot=50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Questions:**\n",
    "- Compare the Reconstruction loss at the end of training with the reconstruction loss achieved by the corresponding basic AE in the previous Tutorial, in Task 4. If a VAE and an AE have the same architectures, which one do you expect to achieve lower reconstruction loss on the training data? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 7: Synthesizing (generating) new data with a VAE\n",
    "\n",
    "Below we will use a VAE to generate new data.\n",
    "\n",
    "![title](./documentation/vae_synthesis.png)\n",
    "\n",
    "A trained VAE has learned, via the regularizer, to encode samples in such a way so that the distribution of codes z matches the 'prior' distribution p(z)=N(0,I) (Gaussian with 0 mean and 1 std deviation in all dimensions of space Z).\n",
    "\n",
    "To synthesize new data:\n",
    "- We sample a code z from the 'prior' p(z) = N(0,I).\n",
    "- We decode it with the VAE.\n",
    "\n",
    "**FILL IN THE BLANKS** in the below code, to enable it to sample from the N(0,I) normal distribution to synthesize data:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def synthesize(enc_dec_net,\n",
    "               rng,\n",
    "               n_samples):\n",
    "    # enc_dec_net: Network with encoder and decoder, pretrained.\n",
    "    # n_samples: how many samples to produce.\n",
    "    \n",
    "    z_dims = enc_dec_net.D_bottleneck  # Dimensionality of z codes (and input to decoder).\n",
    "    \n",
    "    ############################## TODO: Fill in the blanks #############################\n",
    "    # Create samples of z from Gaussian N(0,I), where means are 0 and standard deviations are 1 in all dimensions.\n",
    "    z_samples = np.random.normal(loc=?????, scale=?????, size=[n_samples, z_dims])\n",
    "    #####################################################################################\n",
    "    \n",
    "    z_samples_t = torch.tensor(z_samples, dtype=torch.float)\n",
    "    x_samples = enc_dec_net.decode(z_samples_t)\n",
    "    \n",
    "    x_samples_np = x_samples if type(x_samples) is np.ndarray else x_samples.detach().numpy()  # torch to numpy\n",
    "    \n",
    "    for x_sample in x_samples_np:\n",
    "        plot_image(x_sample.reshape([H_height, W_width]))\n",
    "       \n",
    "    \n",
    "# Lets finally run the synthesis and see what happens...\n",
    "rng = np.random.RandomState(seed=SEED)\n",
    "\n",
    "synthesize(vae_wide,\n",
    "           rng,\n",
    "           n_samples=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If everything was filled correctly, you should see above images created by the decoder for the randomly sampled z-codes.\n",
    "\n",
    "**Questions:**\n",
    "\n",
    "- Compare the above results with those obtained from the basic AE with similar architecture in the previous Tutorial (Task 5)? What do you observe? What is the main property of the VAE to achieve this, when it comes to the area where we sample codes from?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 8: For a given x, reconstruct random samples from the predicted posterior p(z|x).\n",
    "\n",
    "Given an input x, the encoder of a VAE predicts the distribution p(z|x), which explains which values of z are the \"probable\" codes for x. During training, random z samples are sampled via the reparameterization trick from p(z|x), and the decoder is trained to decode them all to reconstruct x. \n",
    "\n",
    "If p(z|x) is parameterized as a Gaussian, as commonly done in VAE (and in this tutorial), **the predicted mean is the most probable code, and will also be sampled the most**. The probability of a code being sampled decreases as we move away from the mean, with a rate dependent on the predicted standard deviation of p(z|x). Therefore, one could wonder how well does the decoder learn to reconstruct z codes from whole p(z|x) (not just the mean), and how do they look. We explore this here.\n",
    "\n",
    "In the below:\n",
    "- We will encode a single image x\n",
    "- We will encode it with the pre-trained VAE (32D Z) from Task 6, to predict mean and std.dev of p(z|x).\n",
    "- We will sample codes z from p(z|x) and reconstruct based on them.\n",
    "\n",
    "FILL IN THE BLANKS below, to enable the code to sample from predicted distribution p(z|x) for each sample x: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_variations_of_x(enc_dec_net,\n",
    "                           imgs_flat,\n",
    "                           idx_img_x,\n",
    "                           rng,\n",
    "                           n_samples):\n",
    "    # enc_dec_net: Network with encoder and decoder, pretrained.\n",
    "    # imgs_flat:\n",
    "    # idx_img_x:\n",
    "    # n_samples: how many samples to produce.\n",
    "    \n",
    "    img_x_nparray = imgs_flat[idx_img_x:idx_img_x+1]  # Shape: [num samples = 1, H * W]\n",
    "    \n",
    "    # Encode:\n",
    "    z_mu, z_logstd = enc_dec_net.encode(img_x_nparray)  # expects array shape [N, dims_z]\n",
    "    \n",
    "    z_dims = z_mu.shape[1]  # Dimensionality of z codes (and input to decoder).\n",
    "    z_mu = z_mu.detach().numpy()  # Maky pytorch tensor a numpy\n",
    "    z_logstd = z_logstd.detach().numpy()\n",
    "    \n",
    "    ############# TODO: Fill in the blanks ##################################\n",
    "    # Samples z values from the predicted probability of z for this sample x: p(z|x) = N(mu(x), std^2(x))\n",
    "    z_std = np.exp(?????????)   # <------ what you need is returned by the encoding above -------------\n",
    "    z_samples = np.random.normal(loc=???????, scale=z_std, size=[n_samples, z_dims]) # <------------------\n",
    "    #########################################################################\n",
    "    \n",
    "    x_samples = enc_dec_net.decode(z_samples)\n",
    "    \n",
    "    x_samples_np = x_samples if type(x_samples) is np.ndarray else x_samples.detach().numpy()  # torch to numpy\n",
    "    \n",
    "    print(\"Real input to encoder:\")\n",
    "    plot_image(img_x_nparray.reshape([H_height, W_width]))   \n",
    "    print(\"Reconstructions based on samples from p(z|x=input):\")\n",
    "    plot_grid_of_images(x_samples_np.reshape([n_samples, H_height, W_width]),\n",
    "                        n_imgs_per_row=10,\n",
    "                        dynamically=False)\n",
    "    print(\"Going to plot all the reconstructed variations one by one, for easier visual investigation:\")\n",
    "    for x_sample in x_samples_np:\n",
    "        plot_image(x_sample.reshape([H_height, W_width]))\n",
    "    \n",
    "    diff = img_x_nparray[0] - x_samples_np[0]\n",
    "    \n",
    "# Lets finally run the synthesis and see what happens...\n",
    "rng = np.random.RandomState(seed=SEED)\n",
    "\n",
    "sample_variations_of_x(vae_wide,  # The VAE with 32 dimensional Z.\n",
    "                       train_imgs_flat,\n",
    "                       idx_img_x=1,  # We will encode the image with index 1, and then reconstruct it.\n",
    "                       rng=rng,\n",
    "                       n_samples=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Questions:**\n",
    "- Do the reconstructions look like the original digit? Has the decoder learned to reconstruct from whole p(z|x)?\n",
    "- Feel free to experiment with other digits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 9: Interpolate between x_1 and x_2 in space Z.\n",
    "\n",
    "Here, we want to create variations of an input more \"systematically\" (not random as above). We want to create images that look partly as an input x1 and partly as an input x2, by interpolating between x1 and x2 in the latent space of Z codes.\n",
    "\n",
    "Steps:\n",
    "\n",
    "0. We are given a pre-trained VAE.\n",
    "1. We will encode x1\n",
    "2. We will encode x2\n",
    "3. We will create z codes by walking between mu(x1) and mu(x2) for various alpha values:\\\n",
    "    z = mu(x1) + alpha * (mu(x2) - mu(x1))\n",
    "4. We will decode these z codes to look at how the images look.\n",
    "\n",
    "The code below is complete. Run it and observe the output.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpolate_between_x1_x2(enc_dec_net,\n",
    "                              imgs_flat,\n",
    "                              idx_x1,\n",
    "                              idx_x2,\n",
    "                              rng):\n",
    "    # enc_dec_net: Network with encoder and decoder, pretrained.\n",
    "    # imgs_flat: [number of images, H * W]\n",
    "    # idx_x1: index of x1: x1 = imgs_flat[idx_x1]\n",
    "    # idx_x2: index of x2: x2 = imgs_flat[idx_x2]\n",
    "    # n_samples: how many samples to produce.\n",
    "    \n",
    "    img_x1_nparray = imgs_flat[idx_x1]\n",
    "    img_x2_nparray = imgs_flat[idx_x2]\n",
    "    z_mus, z_logstds = enc_dec_net.encode(np.array([img_x1_nparray, img_x2_nparray]))\n",
    "    z_mus = z_mus.detach().numpy()\n",
    "    \n",
    "    z_mu1 = z_mus[0]  # np vector with [z-dims] elements\n",
    "    z_mu2 = z_mus[1]\n",
    "    \n",
    "    z_dims = z_mu1.shape[0]  # Dimensionality of z codes (and input to decoder).\n",
    "    \n",
    "    # Reconstruct x1 and x2 based on mu codes:\n",
    "    x_samples = enc_dec_net.decode(np.array([z_mu1, z_mu2]))\n",
    "    x_samples = x_samples.detach().numpy()\n",
    "    x1_rec = x_samples[0]\n",
    "    x2_rec = x_samples[1]\n",
    "    \n",
    "    # Interpolate:\n",
    "    alphas = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\n",
    "    \n",
    "    alphas_np = np.ones([11, z_dims], dtype=\"float16\")  # [number of interpolated samples = 11, z-dimensions]\n",
    "    for row_idx in range(alphas_np.shape[0]):\n",
    "        alphas_np[row_idx] = alphas_np[row_idx] * alphas[row_idx]  # now whole 1st row == 0.0, 2nd row == 0.1, ...\n",
    "    \n",
    "    # Interpolate new z values\n",
    "    zs_to_decode = z_mu1 + alphas_np * (z_mu2 - z_mu1)\n",
    "    \n",
    "    x_samples= enc_dec_net.decode(zs_to_decode)\n",
    "    \n",
    "    x_samples_np = x_samples if type(x_samples) is np.ndarray else x_samples.detach().numpy()  # torch to numpy\n",
    "    \n",
    "    print(\"Inputs to encoder:\")\n",
    "    plot_images([img_x1_nparray.reshape([H_height, W_width]), img_x2_nparray.reshape([H_height, W_width])],\n",
    "               titles=[\"Real x1\", \"Real x2\"])\n",
    "    print(\"Reconstructions of x1 and x2 based on their most likely predicted z codes (corresponding mus):\")\n",
    "    plot_images([x1_rec.reshape([H_height, W_width]), x2_rec.reshape([H_height, W_width])],\n",
    "               titles=[\"Recon of x1\", \"Recon of x2\"])\n",
    "    print(\"Decodings based on z samples interpolated between mu(x1) and mu(x2) predicted by encoder:\")\n",
    "    plot_grid_of_images(x_samples_np.reshape([11, H_height, W_width]),\n",
    "                        n_imgs_per_row=11,\n",
    "                        dynamically=False)\n",
    "    print(\"Going to plot all the reconstructed variations one by one, for easier visual investigation:\")\n",
    "    for x_sample in x_samples_np:\n",
    "        plot_image(x_sample.reshape([H_height, W_width]))\n",
    "    \n",
    "    \n",
    "# Lets finally run the synthesis and see what happens...\n",
    "rng = np.random.RandomState(seed=SEED)\n",
    "\n",
    "interpolate_between_x1_x2(vae_wide,\n",
    "                          train_imgs_flat,\n",
    "                          idx_x1=1,\n",
    "                          idx_x2=3,\n",
    "                          rng=rng)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Questions:**\n",
    "- As the one digit \"morphs\" into the other, do the intermediate interpolations look like garbage or like digits? Why? How do you explain this based on what you have seen in Task 3?\n",
    "- How do you think this result would compare with similar results if performed with a standard AE? How would they compare if you would train the VAE only with the Reconstruction loss?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 10: Learning from Unlabelled data with a VAE, to complement Supervised Classifier when Labelled data are limited: Lets first train a supervised Classifier 'from scratch'\n",
    "\n",
    "We saw in the previous Tutorial 2 approaches for **using a pre-trained AE to improve performance of a Supervised Classifier, when labelled data are limited**. Approach 1: Use weights of AE's encoder as \"frozen\" feature extractor (with the classifier attached and trained on top), and Approach 2: Use weights of AE's encoder to \"initialize\" the corresponding layers of a Classifier, and then \"refine\" the whole classifier with the labelled data.\n",
    "\n",
    "We saw this clearly improved performance when done with a basic Auto-Encoder.\n",
    "\n",
    "Here, we will attempt exactly the same with a VAE.\n",
    "\n",
    "In this task, we will create and train a fully-supervised MLP classifier only **on very limited (100) labelled data**. This is to compare the performance of this classifier with what we achieve when complementing it with unlabelled data using a VAE (in later Task).\n",
    "\n",
    "![title](./documentation/classifier_scratch.png)\n",
    "\n",
    "**The below code for creating a classifier, cross entropy loss, and training loop is complete.**\\\n",
    "This is **EXACTLY the same as the code of corresponding Task 6 of the previous Tutorial. Just run it.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier_3layers(Network):\n",
    "    def __init__(self, D_in, D_hid_1, D_hid_2, D_out, rng):\n",
    "        D_in = D_in\n",
    "        D_hid_1 = D_hid_1\n",
    "        D_hid_2 = D_hid_2\n",
    "        D_out = D_out\n",
    "        \n",
    "        # === NOTE: Notice that this is exactly the same architecture as encoder of AE in Task 4 ====\n",
    "        w_1_init = rng.normal(loc=0.0, scale=0.01, size=(D_in+1, D_hid_1))\n",
    "        w_2_init = rng.normal(loc=0.0, scale=0.01, size=(D_hid_1+1, D_hid_2))\n",
    "        w_out_init = rng.normal(loc=0.0, scale=0.01, size=(D_hid_2+1, D_out))\n",
    "        \n",
    "        w_1 = torch.tensor(w_1_init, dtype=torch.float, requires_grad=True)\n",
    "        w_2 = torch.tensor(w_2_init, dtype=torch.float, requires_grad=True)\n",
    "        w_out = torch.tensor(w_out_init, dtype=torch.float, requires_grad=True)\n",
    "        \n",
    "        self.params = [w_1, w_2, w_out]\n",
    "        \n",
    "        \n",
    "    def forward_pass(self, batch_inp):\n",
    "        # compute predicted y\n",
    "        [w_1, w_2, w_out] = self.params\n",
    "        \n",
    "        # In case input is image, make it a tensor.\n",
    "        batch_imgs_t = torch.tensor(batch_inp, dtype=torch.float) if type(batch_inp) is np.ndarray else batch_inp\n",
    "        \n",
    "        unary_feature_for_bias = torch.ones(size=(batch_imgs_t.shape[0], 1)) # [N, 1] column vector.\n",
    "        x = torch.cat((batch_imgs_t, unary_feature_for_bias), dim=1) # Extra feature=1 for bias.\n",
    "        \n",
    "        # === NOTE: This is the same architecture as encoder of AE in Task 4, with extra classification layer ===\n",
    "        # Layer 1\n",
    "        h1_preact = x.mm(w_1)\n",
    "        h1_act = h1_preact.clamp(min=0)\n",
    "        # Layer 2 (corresponds to bottleneck of the AE):\n",
    "        h1_ext = torch.cat((h1_act, unary_feature_for_bias), dim=1)\n",
    "        h2_preact = h1_ext.mm(w_2)\n",
    "        h2_act = h2_preact.clamp(min=0)\n",
    "        # Output classification layer\n",
    "        h2_ext = torch.cat((h2_act, unary_feature_for_bias), dim=1)\n",
    "        h_out = h2_ext.mm(w_out)\n",
    "        \n",
    "        logits = h_out\n",
    "        \n",
    "        # === Addition of a softmax function for \n",
    "        # Softmax activation function.\n",
    "        exp_logits = torch.exp(logits)\n",
    "        y_pred = exp_logits / torch.sum(exp_logits, dim=1, keepdim=True) \n",
    "        # sum with Keepdim=True returns [N,1] array. It would be [N] if keepdim=False.\n",
    "        # Torch broadcasts [N,1] to [N,D_out] via repetition, to divide elementwise exp_h2 (which is [N,D_out]).\n",
    "        \n",
    "        return y_pred\n",
    "\n",
    "    \n",
    "def cross_entropy(y_pred, y_real, eps=1e-7):\n",
    "    # y_pred: Predicted class-posterior probabilities, returned by forward_pass. Numpy array of shape [N, D_out]\n",
    "    # y_real: One-hot representation of real training labels. Same shape as y_pred.\n",
    "    \n",
    "    # If number array is given, change it to a Torch tensor.\n",
    "    y_pred = torch.tensor(y_pred, dtype=torch.float) if type(y_pred) is np.ndarray else y_pred\n",
    "    y_real = torch.tensor(y_real, dtype=torch.float) if type(y_real) is np.ndarray else y_real\n",
    "    \n",
    "    x_entr_per_sample = - torch.sum( y_real*torch.log(y_pred+eps), dim=1)  # Sum over classes, axis=1\n",
    "    \n",
    "    loss = torch.mean(x_entr_per_sample, dim=0) # Expectation of loss: Mean over samples (axis=0).\n",
    "    return loss\n",
    "\n",
    "\n",
    "\n",
    "from utils.plotting import plot_train_progress_2\n",
    "\n",
    "def train_classifier(classifier,\n",
    "                     pretrained_VAE,\n",
    "                     loss_func,\n",
    "                     rng,\n",
    "                     train_imgs,\n",
    "                     train_lbls,\n",
    "                     test_imgs,\n",
    "                     test_lbls,\n",
    "                     batch_size,\n",
    "                     learning_rate,\n",
    "                     total_iters,\n",
    "                     iters_per_test=-1):\n",
    "    # Arguments:\n",
    "    # classifier: A classifier network. It will be trained by this function using labelled data.\n",
    "    #             Its input will be either original data (if pretrained_VAE=0), ...\n",
    "    #             ... or the output of the feature extractor if one is given.\n",
    "    # pretrained_VAE: A pretrained AutoEncoder that will *not* be trained here.\n",
    "    #      It will be used to encode input data.\n",
    "    #      The classifier will take as input the output of this feature extractor.\n",
    "    #      If pretrained_VAE = None: The classifier will simply receive the actual data as input.\n",
    "    # train_imgs: Vectorized training images\n",
    "    # train_lbls: One hot labels\n",
    "    # test_imgs: Vectorized testing images, to compute generalization accuracy.\n",
    "    # test_lbls: One hot labels for test data.\n",
    "    # batch_size: batch size\n",
    "    # learning_rate: come on...\n",
    "    # total_iters: how many SGD iterations to perform.\n",
    "    # iters_per_test: We will 'test' the model on test data every few iterations as specified by this.\n",
    "    \n",
    "    values_to_plot = {'loss':[], 'acc_train': [], 'acc_test': []}\n",
    "    \n",
    "    optimizer = optim.Adam(classifier.params, lr=learning_rate)\n",
    "        \n",
    "    for t in range(total_iters):\n",
    "        # Sample batch for this SGD iteration\n",
    "        train_imgs_batch, train_lbls_batch = get_random_batch(train_imgs, train_lbls, batch_size, rng)\n",
    "        \n",
    "        # Forward pass\n",
    "        if pretrained_VAE is None:\n",
    "            inp_to_classifier = train_imgs_batch\n",
    "        else:\n",
    "            ############### TODO FOR TASK-11 #########################################\n",
    "            # FILL IN THE BLANK, to provide as input to the classifier the predicted MEAN of p(z|x) for each x.\n",
    "            # Why? Because the mean is the most likely (probable) code z for x!!\n",
    "            #\n",
    "            z_codes_mu, z_codes_logstd = pretrained_VAE.encode(train_imgs_batch)  # AE encodes. Output will be given to Classifier\n",
    "            inp_to_classifier = ???????????????  # <----------------------------------------\n",
    "            ############################################################################\n",
    "            \n",
    "        y_pred = classifier.forward_pass(inp_to_classifier)\n",
    "        \n",
    "        # Compute loss:\n",
    "        y_real = train_lbls_batch\n",
    "        loss = loss_func(y_pred, y_real)  # Cross entropy\n",
    "        \n",
    "        # Backprop and updates.\n",
    "        optimizer.zero_grad()\n",
    "        grads = classifier.backward_pass(loss)\n",
    "        optimizer.step()\n",
    "        \n",
    "        \n",
    "        # ==== Report training loss and accuracy ======\n",
    "        # y_pred and loss can be either np.array, or torch.tensor (see later). If tensor, make it np.array.\n",
    "        y_pred_numpy = y_pred if type(y_pred) is np.ndarray else y_pred.detach().numpy()\n",
    "        y_pred_lbls = np.argmax(y_pred_numpy, axis=1) # y_pred is soft/probability. Make it a hard one-hot label.\n",
    "        y_real_lbls = np.argmax(y_real, axis=1)\n",
    "        \n",
    "        acc_train = np.mean(y_pred_lbls == y_real_lbls) * 100. # percentage\n",
    "        \n",
    "        loss_numpy = loss if type(loss) is type(float) else loss.item()\n",
    "        if t%10 == 0:\n",
    "            print(\"[iter:\", t, \"]: Training Loss: {0:.2f}\".format(loss), \"\\t Accuracy: {0:.2f}\".format(acc_train))\n",
    "        \n",
    "        # =============== Every few iterations, test accuracy ================#\n",
    "        if t==total_iters-1 or t%iters_per_test == 0:\n",
    "            if pretrained_VAE is None:\n",
    "                inp_to_classifier_test = test_imgs\n",
    "            else:\n",
    "                z_codes_test_mu, z_codes_test_logstd = pretrained_VAE.encode(test_imgs)\n",
    "                inp_to_classifier_test = z_codes_test_mu\n",
    "                \n",
    "            y_pred_test = classifier.forward_pass(inp_to_classifier_test)\n",
    "            \n",
    "            # ==== Report test accuracy ======\n",
    "            y_pred_test_numpy = y_pred_test if type(y_pred_test) is np.ndarray else y_pred_test.detach().numpy()\n",
    "            \n",
    "            y_pred_lbls_test = np.argmax(y_pred_test_numpy, axis=1)\n",
    "            y_real_lbls_test = np.argmax(test_lbls, axis=1)\n",
    "            acc_test = np.mean(y_pred_lbls_test == y_real_lbls_test) * 100.\n",
    "            print(\"\\t\\t\\t\\t\\t\\t\\t\\t Testing Accuracy: {0:.2f}\".format(acc_test))\n",
    "            \n",
    "            # Keep list of metrics to plot progress.\n",
    "            values_to_plot['loss'].append(loss_numpy)\n",
    "            values_to_plot['acc_train'].append(acc_train)\n",
    "            values_to_plot['acc_test'].append(acc_test)\n",
    "                \n",
    "    # In the end of the process, plot loss accuracy on training and testing data.\n",
    "    plot_train_progress_2(values_to_plot['loss'], values_to_plot['acc_train'], values_to_plot['acc_test'], iters_per_test)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now below, we create an instance of this 3-layered classifier and train it on 100 labeled samples. We evaluate generalization on Test samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Classifier from scratch (initialized randomly)\n",
    "\n",
    "# Create the network\n",
    "rng = np.random.RandomState(seed=SEED)\n",
    "net_classifier_from_scratch = Classifier_3layers(D_in=H_height*W_width,\n",
    "                                                 D_hid_1=256,\n",
    "                                                 D_hid_2=32,\n",
    "                                                 D_out=C_classes,\n",
    "                                                 rng=rng)\n",
    "# Start training\n",
    "train_classifier(net_classifier_from_scratch,\n",
    "                 None,  # No pretrained AE\n",
    "                 cross_entropy,\n",
    "                 rng,\n",
    "                 train_imgs_flat[:100],\n",
    "                 train_lbls_onehot[:100],\n",
    "                 test_imgs_flat,\n",
    "                 test_lbls_onehot,\n",
    "                 batch_size=40,\n",
    "                 learning_rate=3e-3,\n",
    "                 total_iters=1000,\n",
    "                 iters_per_test=20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is \"exactly\" the same as the corresponding Task 6 in the previous Tutorial. Simply run it, and note down the final Accuracy on the Test data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 11: Use pre-trained VAE as 'feature-extractor' for a supervised Classifier when labels are limited.\n",
    "\n",
    "Approach-1: We take the encoder of the pre-trained VAE and place an untrained, small (1 layer) Classifier on top. The Classifier receives as input, codes that the VAE's encoder predicts when given input x. We then use the limited labelled data for training. Importantly, we only train the small Classifier. The encoder is used as 'frozen' (does not get trained further) feature extractor. See next figure for a visual explanation.\n",
    "\n",
    "![title](./documentation/vae_refine_1.png)\n",
    "\n",
    "**TODO: **\n",
    "This is the same as Task 7 of previous Tutorial on AEs, **with one important peculiarity**: When the encoder predicts a whole distribution of codes for each x, p(z|x), what code should we use as oputput of the \"feature extractor\" (encoder of VAE) and as input to the classifier? Note: We want the classifier to be \"deterministic\", not stochastic, so we wont be sampling. Probably we want the most probable code z for each x...\n",
    "\n",
    "**Go back** to Task 10 and the function **train_classifier(...)** defined therein. Fill in the gap, choosing which code to use as input to the Classifier. **AFTER** you have done that, run the code below..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Train classifier on top of pre-trained AE encoder\n",
    "\n",
    "class Classifier_1layer(Network):\n",
    "    # Classifier with just 1 layer, the classification layer\n",
    "    def __init__(self, D_in, D_out, rng):\n",
    "        # D_in: dimensions of input\n",
    "        # D_out: dimension of output (number of classes)\n",
    "        \n",
    "        w_out_init = rng.normal(loc=0.0, scale=0.01, size=(D_in+1, D_out))\n",
    "        w_out = torch.tensor(w_out_init, dtype=torch.float, requires_grad=True)\n",
    "        self.params = [w_out]\n",
    "        \n",
    "        \n",
    "    def forward_pass(self, batch_inp):\n",
    "        # compute predicted y\n",
    "        [w_out] = self.params\n",
    "        \n",
    "        # In case input is image, make it a tensor.\n",
    "        batch_inp_t = torch.tensor(batch_inp, dtype=torch.float) if type(batch_inp) is np.ndarray else batch_inp\n",
    "        \n",
    "        unary_feature_for_bias = torch.ones(size=(batch_inp_t.shape[0], 1))  # [N, 1] column vector.\n",
    "        batch_inp_ext = torch.cat((batch_inp_t, unary_feature_for_bias), dim=1)  # Extra feature=1 for bias.\n",
    "        \n",
    "        # Output classification layer\n",
    "        logits = batch_inp_ext.mm(w_out)\n",
    "        \n",
    "        # Output layer activation function\n",
    "        # Softmax activation function.\n",
    "        exp_logits = torch.exp(logits)\n",
    "        y_pred = exp_logits / torch.sum(exp_logits, dim=1, keepdim=True) \n",
    "        # sum with Keepdim=True returns [N,1] array. It would be [N] if keepdim=False.\n",
    "        # Torch broadcasts [N,1] to [N,D_out] via repetition, to divide elementwise exp_h2 (which is [N,D_out]).\n",
    "        \n",
    "        return y_pred\n",
    "    \n",
    "    \n",
    "    \n",
    "# Create the network\n",
    "rng = np.random.RandomState(seed=SEED) # Random number generator\n",
    "# As input, it will be getting z-codes from the AE with 32-neurons bottleneck from Task 4.\n",
    "classifier_1layer = Classifier_1layer(vae_wide.D_bottleneck,  # Input dimension is dimensions of AE's Z\n",
    "                                      C_classes,\n",
    "                                      rng=rng)\n",
    "\n",
    "train_classifier(classifier_1layer,\n",
    "                 vae_wide,  # Pretrained AE, to use as feature extractor.\n",
    "                 cross_entropy,\n",
    "                 rng,\n",
    "                 train_imgs_flat[:100],\n",
    "                 train_lbls_onehot[:100],\n",
    "                 test_imgs_flat,\n",
    "                 test_lbls_onehot,\n",
    "                 batch_size=40,\n",
    "                 learning_rate=3e-3,\n",
    "                 total_iters=1000,\n",
    "                 iters_per_test=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you completed the task appropriately, you should see the model getting trained and performance reported at the bottom. The expected TRAINING accuracy is approximately 80%, and the TESTING accuracy is around 53% in the end of training.\n",
    "\n",
    "**Questions:**\n",
    "- Compare with test accuracy from Task 10, when training the classifier from Scratch. Did this improve performance?\n",
    "- Compare with the results obtained with the standard AE in the previous Tutorial (Task 7). What do you observe?\n",
    "- Is the AE or VAE better for pretraining a Classifier? How can you theoreticall justify this?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 12: Use parameters of VAE's encoder to initialize weights of a supervised Classifier, followed by refinement using limited labels\n",
    "\n",
    "Approach-2: The second approach is to build a Classifier that has the same architecture as the encoder of the VAE, followed by an extra classification layer. We first train the VAE (already done in Task 6). Then, we **use the pre-trained weights of the VAE's encoder to initialize the corresponding parameters of the Classifier**. The classification layer of the Classifier is initialized randomly. Then, **with the limited labelled data, we refine (train) all the parameters of the classifier**.\n",
    "\n",
    "![title](./documentation/vae_refine_2.png)\n",
    "\n",
    "This is the same as Task 8 of the previous Tutorial on AEs, **with one important peculiarity** (related to Task 11 here): Since the Classifier needs to be deterministic, the **we do not use the weights that predict the standard-deviation** in the VAE's encoder. We **only use the neurons that predict the mean of p(z|x)** (the most likely code) to initilize the corresponding layers of the Supervised Classifier. \n",
    "\n",
    "**The code below is complete.**\\\n",
    "Read it, understand it, run it, and try to answer the questions below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-train a classifier.\n",
    "\n",
    "# The below classifier has THE SAME architecture as the 3-layer Classifier that we trained...\n",
    "# ... in a purely supervised manner in Task-10.\n",
    "# This is done by inheriting the class (Classifier_3layers), therefore uses THE SAME forward_pass() function.\n",
    "# THE ONLY DIFFERENCE is in the construction __init__.\n",
    "# This 'pretrained' classifier receives as input a pretrained autoencoder (pretrained_VAE) from Task 6.\n",
    "# It then uses the parameters of the AE's encoder to initialize its own parameters, rather than random initialization.\n",
    "# The model is then trained all together.\n",
    "class Classifier_3layers_pretrained(Classifier_3layers):\n",
    "    def __init__(self, pretrained_VAE, D_in, D_out, rng):\n",
    "        D_in = D_in\n",
    "        D_hid_1 = 256\n",
    "        D_hid_2 = 32\n",
    "        D_out = D_out\n",
    "\n",
    "        w_out_init = rng.normal(loc=0.0, scale=0.01, size=(D_hid_2+1, D_out))\n",
    "        \n",
    "        [vae_w1, vae_w2_mu, vae_w2_std, vae_w3, vae_w4] = pretrained_VAE.params  # Pre-trained parameters of pre-trained VAE.\n",
    "        \n",
    "        w_1 = torch.tensor(vae_w1, dtype=torch.float, requires_grad=True)\n",
    "        w_2 = torch.tensor(vae_w2_mu, dtype=torch.float, requires_grad=True)\n",
    "        w_out = torch.tensor(w_out_init, dtype=torch.float, requires_grad=True)\n",
    "        \n",
    "        self.params = [w_1, w_2, w_out]\n",
    "        \n",
    "# Create the network\n",
    "rng = np.random.RandomState(seed=SEED) # Random number generator\n",
    "classifier_3layers_pretrained = Classifier_3layers_pretrained(vae_wide,  # The AE pre-trained in Task 4.\n",
    "                                                              train_imgs_flat.shape[1],\n",
    "                                                              C_classes,\n",
    "                                                              rng=rng)\n",
    "\n",
    "# Start training\n",
    "# NOTE: Only the 3-layer pretrained classifier is used, and will be trained all together.\n",
    "# No frozen feature extractor.\n",
    "train_classifier(classifier_3layers_pretrained,  # classifier that will be trained.\n",
    "                 None,  # No pretrained AE to act as 'frozen' feature extractor.\n",
    "                 cross_entropy,\n",
    "                 rng,\n",
    "                 train_imgs_flat[:100],\n",
    "                 train_lbls_onehot[:100],\n",
    "                 test_imgs_flat,\n",
    "                 test_lbls_onehot,\n",
    "                 batch_size=40,\n",
    "                 learning_rate=3e-3,\n",
    "                 total_iters=1000,\n",
    "                 iters_per_test=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Questions:**\n",
    "\n",
    "- Does this approach improve over results in Tasks 10 and 11?\n",
    "- How does it compare with the same experiments with the basic AE in the previous tutorial?\n",
    "- After all these experiments, what do you conclude: Is the basic AE or the VAE a better method for pre-training weights using unlabelled data, and then using them to improve performance of a Supervised Classifier with limited labels? Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This notebook:\n",
    "Copyright 2021, University of Birmingham  \n",
    "Tutorial for Neural Computation  \n",
    "For issues e-mail: k.kamnitsas@bham.ac.uk"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
