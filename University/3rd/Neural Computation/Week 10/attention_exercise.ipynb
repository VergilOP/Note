{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3c679a24",
   "metadata": {},
   "source": [
    "# Neural Computation Exercises: Attention\n",
    "In this execise your task is to implement the attention mechanism as it is used in the transformer model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "314f38a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6232e500",
   "metadata": {},
   "source": [
    "Below, we are generating the data we will use:\n",
    "* *keys* is a list of key vectors\n",
    "* *values* is a list of value vectors\n",
    "* *queries* is a list of query vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "89c565d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_data(keyLength = 8, valueLength = 16, items = 32, seed = 42, numItems = 64):\n",
    "    np.random.seed(seed)\n",
    "    keys = []\n",
    "    values = []\n",
    "    queries = []\n",
    "\n",
    "    for i in range(numItems):\n",
    "        if i%8 == 0:\n",
    "            baseKeyQuery = np.random.randn(keyLength)*0.5 \n",
    "            baseValue = np.random.rand(valueLength)*1 -0.5\n",
    "        key = baseKeyQuery + np.random.randn(keyLength)*0.2\n",
    "        query = baseKeyQuery + np.random.randn(keyLength)*0.2\n",
    "        value = baseValue + np.random.rand(valueLength)*5 -2.5\n",
    "        keys.append(key)\n",
    "        queries.append(query)\n",
    "        values.append(value)\n",
    "    return keys,values,queries\n",
    "    \n",
    "    \n",
    "keys, values, queries = make_data(keyLength = 8, valueLength = 16, items = 32, seed = 42, numItems = 64)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa0504e6",
   "metadata": {},
   "source": [
    "## Exercise 1: Implement attention for single query\n",
    "Complete the function below.\n",
    "The function should take a **list of keys** and a **list of values** as well as a **single query vector** as input.\n",
    "It should then compute and return:\n",
    "* a vector containing the attention scores for all keys with respect to the query vector as described in the lecture\n",
    "* the result of the attention function as described in the lecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 696,
   "id": "c6c81376",
   "metadata": {},
   "outputs": [],
   "source": [
    "def attentionQuery(query, keys, values):\n",
    "    # please implement\n",
    "    return attention, result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c201eee9",
   "metadata": {},
   "source": [
    "## Exercise 2: Apply the function and plot the results\n",
    "Apply the function to make a query using the first query-vector (*queries[0]*).\n",
    "Make two barplots using [plt.plot](https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.bar.html):\n",
    "* One plot showing the attention-scores (y-axis) for all key-indexes (x-axis).\n",
    "* One showing the resulting of the attention operation showing the vector-element-values (y-axis) as function of  element-index (x-axis).  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eb041c5",
   "metadata": {},
   "source": [
    "Here, we use the function to compute the query for the first query vector. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 697,
   "id": "3ec74ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please apply the function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "505aa090",
   "metadata": {},
   "source": [
    "Plotting the resulting attention scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c2729060",
   "metadata": {},
   "outputs": [],
   "source": [
    "# please make the 1st plot\n",
    "# plt.xlabel('Key-index')\n",
    "# plt.ylabel('Attention-score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a14312b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# please make the 2nd plot\n",
    "# plt.xlabel('Element-index')\n",
    "# plt.ylabel('Element-value')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82b805a4",
   "metadata": {},
   "source": [
    "## Exercise 3: Matrix-based implementation\n",
    "The goal of this exercise is to implement a matrix-based version of the attention function.\n",
    "We begin, by combining the lists of keys, values, and queries into matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 700,
   "id": "9ac0bf5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# please implement\n",
    "# keys_mat = \n",
    "# values_mat = \n",
    "# queries_mat = "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b79047c",
   "metadata": {},
   "source": [
    "Complete the function below.\n",
    "The function should take a **matrix of keys** and a **matrix of values** as well as a **matrix of queries** as input.\n",
    "One key/value/query vector should be stored in each row.\n",
    "It should then compute and return:\n",
    "* a matrix containing attention-scores as described in the lecture\n",
    "* a matrix containing the result of the attention function as described in the lecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 701,
   "id": "d5d0cb9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def attentionQueryMatrix(queries, keys, values):\n",
    "    # please implement\n",
    "    return attention, result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0876fc1a",
   "metadata": {},
   "source": [
    "## Exercise 4: Apply the function and plot the results\n",
    "Apply the function to make a query using all queries.\n",
    "Make two plots:\n",
    "* One plot showing all attention-scores using a heatmap. You can use [plt.imshow](https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.imshow.html). Label the [x-axes](https://matplotlib.org/3.1.1/api/_as_gen/matplotlib.pyplot.xlabel.html) and [y-axes](https://matplotlib.org/3.1.1/api/_as_gen/matplotlib.pyplot.ylabel.html), which corresponds to the index of the query-vector, which to the index of the key-vector? Add a [colour bar](https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.colorbar.html).\n",
    "* One showing the results of the attention operation corresponding to the first query (*queries[0]*) this plot showld be identical to the corresponding one from Exercise 2. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "776f5316",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please apply the function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "223f5bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# please make the 1st plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "460ab9eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# please make the 2nd plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b082b2ec",
   "metadata": {},
   "source": [
    "## Exercise 5: Masked attention\n",
    "Implement a masked version of your matrix based function that ensures, no query can pay **attention to future items**. I.e., make sure that the corresponding attention scores are zero.\n",
    "Note that this is a bit different form the explanation in the lecture video, where also attention to the token itself were forbidden. This is due to the [shift of the output](https://discuss.pytorch.org/t/meaning-of-outputs-shifted-right/63124) that was not considered in the lecture.\n",
    "The entries for each query should still sum to 1.\n",
    "\n",
    "**Tip:** you can use a [meshgrid](https://numpy.org/doc/stable/reference/generated/numpy.meshgrid.html) to achieve this. You don't have to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 705,
   "id": "0bdc70fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def maskedAttentionQueryMatrix(queries, keys, values):\n",
    "    # please implement\n",
    "    return attention, result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b6c47b2",
   "metadata": {},
   "source": [
    "## Exercise 6: Apply the masked  function and plot the results\n",
    "Apply the new function to make a query using all queries.\n",
    "Make the same two plots as in Exercise 4:\n",
    "* One plot showing all attention-scores using a heatmap. You can use [plt.imshow](https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.imshow.html). Label the [x-axes](https://matplotlib.org/3.1.1/api/_as_gen/matplotlib.pyplot.xlabel.html) and [y-axes](https://matplotlib.org/3.1.1/api/_as_gen/matplotlib.pyplot.ylabel.html), which corresponds to the index of the query-vector, which to the index of the key-vector? Add a [colour bar](https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.colorbar.html).\n",
    "* One showing the results of the attention operation corresponding to the first query (*queries[0]*) this plot showld be identical to the corresponding one from Exercise 2. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c2e5f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please apply the function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "66bdb520",
   "metadata": {},
   "outputs": [],
   "source": [
    "# please make the 1st plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aa034cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# please make the 2nd plot"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
