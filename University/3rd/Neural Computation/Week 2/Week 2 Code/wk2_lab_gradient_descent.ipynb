{"cells":[{"cell_type":"markdown","metadata":{"id":"HAPEI7lGD-6V"},"source":["### Neural Computation Week 2 Exercise: Gradient Descent for Linear Regression\n","\n","In this exercise, we'll develop implementations of gradient descent and its minibatch algorithms. As in Week 1 Exercise on linear regression, we will use the `Boston Housing` dataset. Instead of using the exact solution, which was explored in Week 1, here we use gradient descent and minibatch gradient descent to solve this problem.\n","\n","In this exercise, you will learn the following\n","* implement the `gradient descent` method\n","* implement the `minibatch gradient descent` method"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XFkr16uwD-6W"},"outputs":[],"source":["import matplotlib\n","import numpy as np\n","import random\n","import warnings\n","import matplotlib.pyplot as plt\n","from sklearn import preprocessing   # for normalization"]},{"cell_type":"markdown","metadata":{"id":"Ahrd3urGD-6X"},"source":["## Boston Housing Data\n","\n","We will use the Boston Housing data, similar to Week 1. We can import the dataset and preprocess it as follows. Note we add a feature of $1$ to `x_input` to get a n x (d+1) matrix `x_in`"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KI-3mloND-6X"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n","raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n","boston_data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n","target = raw_df.values[1::2, 2]\n","\n","data = boston_data;\n","x_input = data  # a data matrix\n","y_target = target; # a vector for all outputs\n","# add a feature 1 to the dataset, then we do not need to consider the bias and weight separately\n","x_in = np.concatenate([np.ones([np.shape(x_input)[0], 1]), x_input], axis=1)\n","# we normalize the data so that each has regularity\n","x_in = preprocessing.normalize(x_in)"]},{"cell_type":"markdown","metadata":{"id":"GxwsltkTD-6X"},"source":["## Linear Model\n","\n","A linear regression model in one variable has the following form\n","$$\n","f(x)=\\mathbf{w}^\\top \\mathbf{x}.\n","$$\n","The following function computes the output of the linear model on a data matrix of size n x (d+1)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"verfEXlFD-6X"},"outputs":[],"source":["def linearmat_2(w, X):\n","    '''\n","    a vectorization of linearmat_1 in Week 1 lab.\n","    Input: w is a weight parameter (including the bias), and X is a data matrix (n x (d+1)) (including the feature)\n","    Output: a vector containing the predictions of linear models\n","    '''\n","    return np.dot(X, w)"]},{"cell_type":"markdown","metadata":{"id":"sCmT0kPyD-6X"},"source":["## Cost Function\n","\n","We defined the following `mean square error` function for a linear regression problem using the square loss:\n","$$\n","C(\\mathbf{y}, \\mathbf{t}) = \\frac{1}{2n}(\\mathbf{y}-\\mathbf{t})^\\top (\\mathbf{y}-\\mathbf{t}).\n","$$\n","The python implementation is as follows"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LbLpUqNnD-6X"},"outputs":[],"source":["def cost(w, X, y):\n","    '''\n","    Evaluate the cost function in a vectorized manner for\n","    inputs `X` and outputs `y`, at weights `w`.\n","    '''\n","    residual = y - linearmat_2(w, X)  # get the residual\n","    err = np.dot(residual, residual) / (2 * len(y)) # compute the error\n","\n","    return err"]},{"cell_type":"markdown","metadata":{"id":"giwjgplcD-6Y"},"source":["## Gradient Computation\n","\n","Our methods require to use the gradient of the `cost` function. As discussed in the lecture, the gradient can be computed by\n","$$\\nabla C(\\mathbf{w}) =\\frac{1}{n}X^\\top\\big(X\\mathbf{w}-\\mathbf{y}\\big)$$\n","In the following, we present the python implementation on the gradient computation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XItwc25GD-6Z"},"outputs":[],"source":["# Vectorized gradient function\n","def gradfn(weights, X, y):\n","    '''\n","    Given `weights` - a current \"Guess\" of what our weights should be\n","          `X` - matrix of shape (N,d+1) of input features including the feature $1$\n","          `y` - target y values\n","    Return gradient of each weight evaluated at the current value\n","    '''\n","\n","    y_pred = np.dot(X, weights)\n","    error = y_pred - y\n","    return np.dot(X.T, error) / len(y)"]},{"cell_type":"markdown","metadata":{"id":"byhEt9gtD-6Z"},"source":["## Gradient Descent\n","\n","Gradient Descent iteratively updates the model by moving along the negative direction\n","$$\\mathbf{w}^{(t+1)} \\leftarrow \\mathbf{w}^{(t)} - \\eta\\nabla C(\\mathbf{w}^{(t)}),$$\n","where $\\eta$ is a learning rate and $\\nabla C(w^{(t)})$ is the gradient evaluated at current parameter value $\\mathbf{w}^{(t)}$. In the following, we give the python implementation of the gradient descent on the linear regression problem. Here, we use `idx_res` to store the indices of iterations where we have computed the cost, and use `err_res` to store the cost of models at these iterations. These will be used to plot how the `cost` will behave `versus iteration` number."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ClknC79mD-6Z"},"outputs":[],"source":["def solve_via_gradient_descent(X, y, print_every=100,\n","                               niter=5000, eta=1):\n","    '''\n","    Given `X` - matrix of shape (N,D) of input features\n","          `y` - target y values\n","          `print_every` - we report performance every 'print_every' iterations\n","          `niter` - the number of iterates allowed\n","          `eta` - learning rate\n","    Solves for linear regression weights with gradient descent.\n","\n","    Return\n","        `w` - weights after `niter` iterations\n","        `idx_res` - the indices of iterations where we compute the cost\n","        `err_res` - the cost at iterations indicated by idx_res\n","    '''\n","    N, D = np.shape(X)\n","    # initialize all the weights to zeros\n","    w = np.zeros([D])\n","    idx_res = []\n","    err_res = []\n","    for k in range(niter):\n","        # TODO: Insert your code to update w by gradient descent\n","\n","\n","\n","        # we report the progress every print_every iterations\n","        if k % print_every == print_every - 1:\n","            t_cost = cost(w, X, y)\n","            print('error after %d iteration: %s' % (k, t_cost))\n","            idx_res.append(k)\n","            err_res.append(t_cost)\n","    return w, idx_res, err_res"]},{"cell_type":"markdown","metadata":{"id":"dwMp8I5PD-6Z"},"source":["Now we apply **gradient descent** to solve the **Boston House Price** prediction problem, and get the weight `w_gd`, the indices `idx_gd` and the errors 'err_gd' on these indices"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pp4vKL-ID-6Z"},"outputs":[],"source":["w_gd, idx_gd, err_gd = solve_via_gradient_descent( X=x_in, y=y_target)"]},{"cell_type":"markdown","metadata":{"id":"OkQiEBTkD-6Z"},"source":["## Minibatch Gradient Descent\n","\n","The optimization problem in ML often has a **sum** structure in the sense\n","$$\n","C(\\mathbf{w})=\\frac{1}{n}\\sum_{i=1}^nC_i(\\mathbf{w}),\n","$$\n","where $C_i(\\mathbf{w})$ is the loss of the model $\\mathbf{w}$ on the $i$-th example. In our Boston House Price prediction problem, $C_i$ takes the form $C_i(\\mathbf{w})=\\frac{1}{2}(\\mathbf{w}^\\top\\mathbf{x}^{(i)}-y^{(i)})^2$.\n","\n","Gradient descent requires to go through all training examples to compute a single gradient, which may be time consuming if the sample size is large. Minibatch gradient descent improves the efficiency by using a subset of training examples to build an **approximate** gradient. At each iteration, it first randomly draws a set $B_t\\subseteq\\{1,2,\\ldots,n\\}$ of size $b$, where we often call $b$ the minibatch size. Then it builds an approximate gradient by\n","$$\n","\\nabla^{B_t}(\\mathbf{w}^{(t)})=\\frac{1}{b}\\sum_{i\\in B_t}\\nabla C_i(\\mathbf{w}^{(t)})\n","$$\n","Now, it updates the model by\n","$$\n","\\mathbf{w}^{(t+1)}=\\mathbf{w}^{(t)}-\\eta_t\\nabla^{B_t}(\\mathbf{w}^{(t)}).\n","$$\n","It is recommended to use $b\\in[20,100]$. Depending on different $b$, minibatch gradient descent recovers several algorithms\n","\\begin{align*}\n","  b<n \\Rightarrow \\text{Minibatch gradient descent}\\\\\n","  b=1 \\Rightarrow \\text{Stochastic gradient descent}\n","\\end{align*}\n","In the following, we request you to finish the following implementation of the `minibatch gradient descent` on the linear regression problem. To search a subset of $\\{1,2,\\ldots,n\\}$, we recommend you to use the function `random.sample`. The syntax is `random.sample(sequence, k)`, which returns $k$ length new list of elements chosen from the `sequence`. More details can be found  [here](https://www.geeksforgeeks.org/python-random-sample-function/)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vcRmyb5fD-6a"},"outputs":[],"source":["def solve_via_minibatch(X, y, print_every=100,\n","                               niter=5000, eta=1, batch_size=50):\n","    '''\n","    Solves for linear regression weights with nesterov momentum.\n","    Given `X` - matrix of shape (N,D) of input features\n","          `y` - target y values\n","          `print_every` - we report performance every 'print_every' iterations\n","          `niter` - the number of iterates allowed\n","          `eta` - learning rate\n","          `batch_size` - the size of minibatch\n","    Return\n","        `w` - weights after `niter` iterations\n","        `idx_res` - the indices of iterations where we compute the cost\n","        `err_res` - the cost at iterations\n","    '''\n","    N, D = np.shape(X)\n","    # initialize all the weights to zeros\n","    w = np.zeros([D])\n","    idx_res = []\n","    err_res = []\n","    tset = list(range(N))\n","    for k in range(niter):\n","        # TODO: Insert your code to update w by minibatch gradient descent\n","        idx = random.sample(tset, batch_size)\n","        #sample batch of data\n","        sample_X = X[idx, :]\n","        sample_y = y[idx]\n","\n","\n","        if k % print_every == print_every - 1:\n","            t_cost = cost(w, X, y)\n","            print('error after %d iteration: %s' % (k, t_cost))\n","            idx_res.append(k)\n","            err_res.append(t_cost)\n","    return w, idx_res, err_res"]},{"cell_type":"markdown","metadata":{"id":"GPGO4T4oD-6a"},"source":["Now we apply minibatch gradient descent to solve the Boston House Price prediction problem."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BxhJ_QNMD-6a"},"outputs":[],"source":["w_batch, idx_batch, err_batch = solve_via_minibatch( X=x_in, y=y_target)"]},{"cell_type":"markdown","metadata":{"id":"Wk47begLD-6a"},"source":["### Comparison between Minibatch Gradient Descent and Gradient Descent\n","\n","We can now compare the behavie of Minibatch Gradient Descent and Gradient Descent. In particular, we will show how the `cost` of models found by the algorithm at different iterations would behave with respect to the iteration number."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Rzbh80OiD-6a"},"outputs":[],"source":["plt.plot(idx_batch, err_batch, color=\"red\", linewidth=2.5, linestyle=\"-\", label=\"minibatch\")\n","plt.plot(idx_gd, err_gd, color=\"blue\", linewidth=2.5, linestyle=\"-\", label=\"gradient descent\")\n","plt.legend(loc='upper right', prop={'size': 12})\n","plt.title('comparison between minibatch gradient descent and gradient descent')\n","plt.xlabel(\"number of iterations\")\n","plt.ylabel(\"cost\")\n","plt.grid()\n","plt.show()"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}