{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Computation - 2021"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial - AutoEncoders in Pytorch\n",
    "\n",
    "**Aims of this tutorial**:\n",
    "- Implement and train basic Auto-Encoders in Pytorch.\n",
    "- Investigate how the learned latent space looks.\n",
    "- Investigate whether we can synthesize new data with basic Auto-Encoders.\n",
    "- See how we can use Auto-Encoders trained with Unsupervised Learning to improve training of Supervised Classifiers when labelled data are limited.\n",
    "\n",
    "It may look long, but it should be easy to complete. Understanding the core points of this tutorial are of **high importance and is part of the assessable material for the course**. Invest some time to study and understand it, and don't hesitate to ask if you don't understand something.\n",
    "\n",
    "**Prerequisites**:\n",
    "- Familiar with python, numpy, and basic PyTorch.\n",
    "- Familiar with MNIST, Multi-Layer-Perceptrons (MLPs), and how to train MLPs (forward/backward pass) in PyTorch.\n",
    "\n",
    "\n",
    "**Notes**:\n",
    "- Docs for Pytorch's functions you will need:  \n",
    "https://pytorch.org/docs/stable/tensors.html  \n",
    "https://pytorch.org/docs/stable/nn.html  \n",
    "- Some helper functions for loading and plotting data are given in `./utils` folder. They will be used out of the box below.\n",
    "\n",
    "** **THE CODE BELOW WILL NOT RUN BY DEFAULT.** ** \\\n",
    "This is because it includes **blanks** (noted with **???**) that you need to fill appropriately where requested."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminary: Loading and refreshing MNIST\n",
    "\n",
    "We will be using MNIST data in this tutorial. Because the images are small, the database allows small networks to be quickly trained using CPU. Anything larger afterwards will require GPUs.\n",
    "\n",
    "Important point to understand is the structure of the loaded data. Especially the **shape** of the loaded numpy arrays, because we need to manipulate it carefully, when processing it with neural networks.\n",
    "\n",
    "Lets load and inspect the data..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# The below is for auto-reloading external modules after they are changed, such as those in ./utils.\n",
    "# Issue: http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "from utils.data_utils import get_mnist # Helper function. Use it out of the box.\n",
    "\n",
    "# Constants\n",
    "DATA_DIR = './data/mnist' # Location we will keep the data.\n",
    "SEED = 111111\n",
    "\n",
    "# If datasets are not at specified location, they will be downloaded.\n",
    "train_imgs, train_lbls = get_mnist(data_dir=DATA_DIR, train=True, download=True)\n",
    "test_imgs, test_lbls = get_mnist(data_dir=DATA_DIR, train=False, download=True)\n",
    "\n",
    "print(\"[train_imgs] Type: \", type(train_imgs), \"|| Shape:\", train_imgs.shape, \"|| Data type: \", train_imgs.dtype )\n",
    "print(\"[train_lbls] Type: \", type(train_lbls), \"|| Shape:\", train_lbls.shape, \"|| Data type: \", train_lbls.dtype )\n",
    "print('Class labels in train = ', np.unique(train_lbls))\n",
    "\n",
    "print(\"[test_imgs] Type: \", type(test_imgs), \"|| Shape:\", test_imgs.shape, \" || Data type: \", test_imgs.dtype )\n",
    "print(\"[test_lbls] Type: \", type(test_lbls), \"|| Shape:\", test_lbls.shape, \" || Data type: \", test_lbls.dtype )\n",
    "print('Class labels in test = ', np.unique(test_lbls))\n",
    "\n",
    "N_tr_imgs = train_imgs.shape[0] # N hereafter. Number of training images in database.\n",
    "H_height = train_imgs.shape[1] # H hereafter\n",
    "W_width = train_imgs.shape[2] # W hereafter\n",
    "C_classes = len(np.unique(train_lbls)) # C hereafter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above we see that data have been loaded in *numpy arrays*.    \n",
    "Arrays with images have **shape ( N = number of images, H = height, W = width )**.  \n",
    "Arrays with labels have **shape ( N = number of images)**, holding one integer per image, the digit's class.\n",
    "\n",
    "MNIST comprises of a **train set (N_tr = 60000) images** and a **test set (N_te = 10000) images**.  \n",
    "We will use the train set for unsupervised learning. The test set will only be used for evaluating generalisation of classifiers towards the end of the tutorial.\n",
    "\n",
    "Lets plot a few image in one collage to have a look..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from utils.plotting import plot_grid_of_images # Helper functions, use out of the box.\n",
    "plot_grid_of_images(train_imgs[0:100], n_imgs_per_row=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the intensities in the images take **values from 0 to 255**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminary: Data pre-processing\n",
    "\n",
    "A first step in almost all pipelines is to pre-process the data, to make them more appropriate for a model.\n",
    "\n",
    "Below, we will perform 3 points:  \n",
    "a) Change the labels from an integer representation to a **one-hot representation** of the **C=10 classes**.\\\n",
    "b) Re-scale the **intensities** in the images, from the range \\[0,255\\], to be instead in the range \\[-1,+1\\].\\\n",
    "c) **Vectorise the 2D images into 1D vectors for the MLP**, which only gets vectors as input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a) Change representation of labels to one-hot vectors of length C=10.\n",
    "train_lbls_onehot = np.zeros(shape=(train_lbls.shape[0], C_classes ) )\n",
    "train_lbls_onehot[ np.arange(train_lbls_onehot.shape[0]), train_lbls ] = 1\n",
    "test_lbls_onehot = np.zeros(shape=(test_lbls.shape[0], C_classes ) )\n",
    "test_lbls_onehot[ np.arange(test_lbls_onehot.shape[0]), test_lbls ] = 1\n",
    "print(\"BEFORE: [train_lbls]        Type: \", type(train_lbls), \"|| Shape:\", train_lbls.shape, \" || Data type: \", train_lbls.dtype )\n",
    "print(\"AFTER : [train_lbls_onehot] Type: \", type(train_lbls_onehot), \"|| Shape:\", train_lbls_onehot.shape, \" || Data type: \", train_lbls_onehot.dtype )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# b) Re-scale image intensities, from [0,255] to [-1, +1].\n",
    "# This commonly facilitates learning:\n",
    "# A zero-centered signal with small magnitude allows avoiding exploding/vanishing problems easier.\n",
    "from utils.data_utils import normalize_int_whole_database # Helper function. Use out of the box.\n",
    "train_imgs = normalize_int_whole_database(train_imgs, norm_type=\"minus_1_to_1\")\n",
    "test_imgs = normalize_int_whole_database(test_imgs, norm_type=\"minus_1_to_1\")\n",
    "\n",
    "# Lets plot one image.\n",
    "from utils.plotting import plot_image # Helper function, use out of the box.\n",
    "index = 0  # Try any, up to 60000\n",
    "print(\"Plotting image of index: [\", index, \"]\")\n",
    "print(\"Class label for this image is: \", train_lbls[index])\n",
    "print(\"One-hot label representation: [\", train_lbls_onehot[index], \"]\")\n",
    "plot_image(train_imgs[index])\n",
    "# Notice the magnitude of intensities. Black is now negative and white is positive float.\n",
    "# Compare with intensities of figure further above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# c) Flatten the images, from 2D matrices to 1D vectors. MLPs take feature-vectors as input, not 2D images.\n",
    "train_imgs_flat = train_imgs.reshape([train_imgs.shape[0], -1]) # Preserve 1st dim (S = num Samples), flatten others.\n",
    "test_imgs_flat = test_imgs.reshape([test_imgs.shape[0], -1])\n",
    "print(\"Shape of numpy array holding the training database:\")\n",
    "print(\"Original : [N, H, W] = [\", train_imgs.shape , \"]\")\n",
    "print(\"Flattened: [N, H*W]  = [\", train_imgs_flat.shape , \"]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Unsupervised training with SGD for Auto-Encoders\n",
    "\n",
    "Below you are given the main training function, which performs gradient descent in unsupervised fashion. This will be called by all following parts of the tutorial.\n",
    "\n",
    "The function takes a model and data, and performs an iteration of stochastic gradient descent.\n",
    "\n",
    "In the below, change the code to make gradient descent stochastic, by sampling a **random** batch per iteration instead of constantly the same training samples. (just a warmup, for you to read the training function :-))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.plotting import plot_train_progress_1, plot_grids_of_images  # Use out of the box\n",
    "\n",
    "\n",
    "def get_random_batch(train_imgs, train_lbls, batch_size, rng):\n",
    "    # train_imgs: Images. Numpy array of shape [N, H * W]\n",
    "    # train_lbls: Labels of images. None, or Numpy array of shape [N, C_classes], one hot label for each image.\n",
    "    # batch_size: integer. Size that the batch should have.\n",
    "    \n",
    "    ####### TODO: Sample a random batch of images for training. Fill in the blanks (???) ######### \n",
    "    indices = rng.randint(low=0, high=train_imgs.shape[????], size=??????, dtype='int32')\n",
    "    ##############################################################################################\n",
    "    \n",
    "    train_imgs_batch = train_imgs[indices]\n",
    "    if train_lbls is not None:  # Enables function to be used both for supervised and unsupervised learning\n",
    "        train_lbls_batch = train_lbls[indices]\n",
    "    else:\n",
    "        train_lbls_batch = None\n",
    "    return [train_imgs_batch, train_lbls_batch]\n",
    "\n",
    "\n",
    "def unsupervised_training_AE(net,\n",
    "                             loss_func,\n",
    "                             rng,\n",
    "                             train_imgs_all,\n",
    "                             batch_size,\n",
    "                             learning_rate,\n",
    "                             total_iters,\n",
    "                             iters_per_recon_plot=-1):\n",
    "    # net: Instance of a model. See classes: Autoencoder, MLPClassifier, etc further below\n",
    "    # loss_func: Function that computes the loss. See functions: reconstruction_loss or cross_entropy.\n",
    "    # rng: numpy random number generator\n",
    "    # train_imgs_all: All the training images. Numpy array, shape [N_tr, H * W]\n",
    "    # batch_size: Size of the batch that should be processed per SGD iteration by a model.\n",
    "    # learning_rate: self explanatory.\n",
    "    # total_iters: how many SGD iterations to perform.\n",
    "    # iters_per_recon_plot: Integer. Every that many iterations the model predicts training images ...\n",
    "    #                      ...and we plot their reconstruction. For visual observation of the results.\n",
    "    loss_values_to_plot = []\n",
    "    \n",
    "    optimizer = optim.Adam(net.params, lr=learning_rate)  # Will use PyTorch's Adam optimizer out of the box\n",
    "        \n",
    "    for t in range(total_iters):\n",
    "        # Sample batch for this SGD iteration\n",
    "        x_imgs, _ = get_random_batch(train_imgs_all, None, batch_size, rng)\n",
    "        \n",
    "        # Forward pass\n",
    "        x_pred, z_codes = net.forward_pass(x_imgs)\n",
    "\n",
    "        # Compute loss:\n",
    "        loss = loss_func(x_pred, x_imgs)\n",
    "        \n",
    "        # Pytorch way\n",
    "        optimizer.zero_grad()\n",
    "        _ = net.backward_pass(loss)\n",
    "        optimizer.step()\n",
    "        \n",
    "        # ==== Report training loss and accuracy ======\n",
    "        loss_np = loss if type(loss) is type(float) else loss.item()  # Pytorch returns tensor. Cast to float\n",
    "        print(\"[iter:\", t, \"]: Training Loss: {0:.2f}\".format(loss))\n",
    "        loss_values_to_plot.append(loss_np)\n",
    "        \n",
    "        # =============== Every few iterations, show reconstructions ================#\n",
    "        if t==total_iters-1 or t%iters_per_recon_plot == 0:\n",
    "            # Reconstruct all images, to plot reconstructions.\n",
    "            x_pred_all, z_codes_all = net.forward_pass(train_imgs_all)\n",
    "            # Cast tensors to numpy arrays\n",
    "            x_pred_all_np = x_pred_all if type(x_pred_all) is np.ndarray else x_pred_all.detach().numpy()\n",
    "            \n",
    "            # Predicted reconstructions have vector shape. Reshape them to original image shape.\n",
    "            train_imgs_resh = train_imgs_all.reshape([train_imgs_all.shape[0], H_height, W_width])\n",
    "            x_pred_all_np_resh = x_pred_all_np.reshape([train_imgs_all.shape[0], H_height, W_width])\n",
    "            \n",
    "            # Plot a few images, originals and predicted reconstructions.\n",
    "            plot_grids_of_images([train_imgs_resh[0:100], x_pred_all_np_resh[0:100]],\n",
    "                                  titles=[\"Real\", \"Reconstructions\"],\n",
    "                                  n_imgs_per_row=10,\n",
    "                                  dynamically=True)\n",
    "            \n",
    "    # In the end of the process, plot loss.\n",
    "    plot_train_progress_1(loss_values_to_plot, iters_per_point=1)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the above should give no output yet. But we will use this in task 2, so hopefully you completed Task 1 right :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Auto-Encoder\n",
    "\n",
    "In this task, you are called to create the architecture of an Auto-Encoder.\n",
    "Make necessary modifications where requested, to create the below architecture:\n",
    "\n",
    "![title](./documentation/ae_bottleneck_2.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "\n",
    "class Network():\n",
    "    \n",
    "    def backward_pass(self, loss):\n",
    "        # Performs back propagation and computes gradients\n",
    "        # With PyTorch, we do not need to compute gradients analytically for parameters were requires_grads=True, \n",
    "        # Calling loss.backward(), torch's Autograd automatically computes grads of loss wrt each parameter p,...\n",
    "        # ... and **puts them in p.grad**. Return them in a list.\n",
    "        loss.backward()\n",
    "        grads = [param.grad for param in self.params]\n",
    "        return grads\n",
    "    \n",
    "class Autoencoder(Network):\n",
    "    def __init__(self, rng, D_in, D_hid_enc, D_bottleneck, D_hid_dec):\n",
    "        # Construct and initialize network parameters\n",
    "        D_in = D_in # Dimension of input feature-vectors. Length of a vectorised image.\n",
    "        D_hid_1 = D_hid_enc # Dimension of Encoder's hidden layer\n",
    "        D_hid_2 = D_bottleneck\n",
    "        D_hid_3 = D_hid_dec  # Dimension of Decoder's hidden layer\n",
    "        D_out = D_in # Dimension of Output layer.\n",
    "        \n",
    "        self.D_bottleneck = D_bottleneck  # Keep track of it, we will need it.\n",
    "        \n",
    "        ##### TODO: Initialize the Auto-Encoder's parameters. Also see forward_pass(...)) ########\n",
    "        # Dimensions of parameter tensors are (number of neurons + 1) per layer, to account for +1 bias.\n",
    "        w1_init = rng.normal(loc=0.0, scale=0.01, size=(D_in+1, ?????))\n",
    "        w2_init = rng.normal(loc=0.0, scale=0.01, size=(D_hid_1+1, D_hid_2))\n",
    "        w3_init = rng.normal(loc=0.0, scale=0.01, size=(?????+1, D_hid_3))\n",
    "        w4_init = rng.normal(loc=0.0, scale=0.01, size=(D_hid_3+1, D_out))\n",
    "        # Pytorch tensors, parameters of the model\n",
    "        # Use the above numpy arrays as of random floats as initialization for the Pytorch weights.\n",
    "        w1 = torch.tensor(w1_init, dtype=torch.float, requires_grad=True)\n",
    "        w2 = torch.tensor(w2_init, dtype=torch.float, requires_grad=True)\n",
    "        w3 = torch.tensor(???????, dtype=torch.float, requires_grad=True)\n",
    "        w4 = torch.tensor(w4_init, dtype=torch.float, requires_grad=True)\n",
    "        # Keep track of all trainable parameters:\n",
    "        self.params = [w1, w2, w3, w4]\n",
    "        ###########################################################################\n",
    "        \n",
    "        \n",
    "    def forward_pass(self, batch_imgs):\n",
    "        # Get parameters\n",
    "        [w1, w2, w3, w4] = self.params\n",
    "        \n",
    "        batch_imgs_t = torch.tensor(batch_imgs, dtype=torch.float)  # Makes pytorch array to pytorch tensor.\n",
    "        \n",
    "        unary_feature_for_bias = torch.ones(size=(batch_imgs.shape[0], 1)) # [N, 1] column vector.\n",
    "        x = torch.cat((batch_imgs_t, unary_feature_for_bias), dim=1) # Extra feature=1 for bias.\n",
    "        \n",
    "        #### TODO: Implement the operations at each layer #####\n",
    "        # Layer 1\n",
    "        h1_preact = x.mm(w1)\n",
    "        h1_act = h1_preact.clamp(min=0)\n",
    "        # Layer 2 (bottleneck):\n",
    "        h1_ext = torch.cat((h1_act, unary_feature_for_bias), dim=1)\n",
    "        h2_preact = h1_ext.mm(w2)\n",
    "        h2_act = h2_preact.clamp(min=0)   # <--------- This is the Representation Z\n",
    "        # Layer 3:\n",
    "        h2_ext = torch.cat((h2_act, unary_feature_for_bias), dim=1)\n",
    "        h3_preact = h2_ext.mm(w3)\n",
    "        h3_act = h3_preact.clamp(min=0)\n",
    "        # Layer 4 (output):\n",
    "        h3_ext = torch.cat((h3_act, unary_feature_for_bias), dim=1)\n",
    "        h4_preact = h3_ext.mm(w4)\n",
    "        h4_act = torch.tanh(h4_preact)\n",
    "        # Output layer\n",
    "        x_pred = h4_act\n",
    "        #######################################################\n",
    "        \n",
    "        ### TODO: Get bottleneck's activations ######\n",
    "        # Bottleneck actications\n",
    "        z_code = ?????????\n",
    "        #############################################\n",
    "                \n",
    "        return (x_pred, z_code)\n",
    "        \n",
    "        \n",
    "def reconstruction_loss(x_pred, x_real, eps=1e-7):\n",
    "    # x_pred: [N, D_out] Prediction returned by forward_pass. Numpy array of shape [N, D_out]\n",
    "    # x_real: [N, D_in]\n",
    "    \n",
    "    # If number array is given, change it to a Torch tensor.\n",
    "    x_pred = torch.tensor(x_pred, dtype=torch.float) if type(x_pred) is np.ndarray else x_pred\n",
    "    x_real = torch.tensor(x_real, dtype=torch.float) if type(x_real) is np.ndarray else x_real\n",
    "    \n",
    "    ######## TODO: Complete the calculation of Reconstruction loss for each sample ###########\n",
    "    loss_recon = torch.mean(torch.square(????? - x_real), dim=1)\n",
    "    ##########################################################################################\n",
    "    \n",
    "    cost = torch.mean(loss_recon, dim=0) # Expectation of loss: Mean over samples (axis=0).\n",
    "    return cost\n",
    "\n",
    "\n",
    "# Create the network\n",
    "rng = np.random.RandomState(seed=SEED)\n",
    "autoencoder_thin = Autoencoder(rng=rng,\n",
    "                               D_in=H_height*W_width,\n",
    "                               D_hid_enc=256,\n",
    "                               D_bottleneck=2,\n",
    "                               D_hid_dec=256)\n",
    "# Start training\n",
    "unsupervised_training_AE(autoencoder_thin,\n",
    "                         reconstruction_loss,\n",
    "                         rng,\n",
    "                         train_imgs_flat,\n",
    "                         batch_size=40,\n",
    "                         learning_rate=3e-3,\n",
    "                         total_iters=1000,\n",
    "                         iters_per_recon_plot=50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If this task is completed correctly, the AutoEncoder should get trained, and you should see the loss decreasing.\\\n",
    "In the end of training, after 1000 iterations, you will see a curve of the training loss.\\\n",
    "The loss should decrease down to approximately 0.2.\n",
    "\n",
    "You should also see printed side by side a set of real images, and their reconstructed version.\\\n",
    "In the end, the reconstructions should start being reasonable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: Encode all training samples in the latent (bottleneck) representation\n",
    "\n",
    "We have now a trained auto-encoder from the above task. We will now use it to encode training data, obtain the codes z for the training data, and plot the first 2 dimensions of Z in a 2D plot to observe how the codes are clustered. (Note: The original notebook should have implementation in the above task of an AE with a bottleneck layer with 2 neurons, therefore making plotting in 2D space easy).\n",
    "\n",
    "Note: The code below is fully implemented. Run it, and observe the output. Think about the questions below the results...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def encode_and_get_min_max_z(net,\n",
    "                             imgs_flat,\n",
    "                             lbls,\n",
    "                             batch_size,\n",
    "                             total_iterations=None,\n",
    "                             plot_2d_embedding=True):\n",
    "    # This function encodes images, plots the first 2 dimensions of the codes in a plot, and finally...\n",
    "    # ... returns the minimum and maximum values of the codes for each dimensions of Z.\n",
    "    # ... We will use  this at a layer task.\n",
    "    # Arguments:\n",
    "    # imgs_flat: Numpy array of shape [Number of images, H * W]\n",
    "    # lbls: Numpy array of shape [number of images], with 1 integer per image. The integer is the class (digit).\n",
    "    # total_iterations: How many batches to encode. We will use this so that we dont encode and plot ...\n",
    "    # ... the whoooole training database, because the plot will get cluttered with 60000 points.\n",
    "    # Returns:\n",
    "    # min_z: numpy array, vector with [dimensions-of-z] elements. Minimum value per dimension of z.\n",
    "    # max_z: numpy array, vector with [dimensions-of-z] elements. Maximum value per dimension of z.\n",
    "    \n",
    "    # If total iterations is None, the function will just iterate over all data, by breaking them into batches.    \n",
    "    if total_iterations is None:\n",
    "        total_iterations = (train_imgs_flat.shape[0] - 1) // batch_size + 1\n",
    "    \n",
    "    z_codes_all = []\n",
    "    lbls_all = []\n",
    "    for t in range(total_iterations):\n",
    "        # Sample batch for this SGD iteration\n",
    "        x_batch = imgs_flat[t*batch_size: (t+1)*batch_size]\n",
    "        lbls_batch = lbls[t*batch_size: (t+1)*batch_size]\n",
    "        \n",
    "        # Forward pass\n",
    "        x_pred, z_codes = net.forward_pass(x_batch)\n",
    "\n",
    "        z_codes_np = z_codes if type(z_codes) is np.ndarray else z_codes.detach().numpy()\n",
    "        \n",
    "        z_codes_all.append(z_codes_np)  # List of np.arrays\n",
    "        lbls_all.append(lbls_batch)\n",
    "    \n",
    "    z_codes_all = np.concatenate(z_codes_all)  # Make list of arrays in one array by concatenating along dim=0 (image index)\n",
    "    lbls_all = np.concatenate(lbls_all)\n",
    "    \n",
    "    if plot_2d_embedding:\n",
    "        # Plot the codes with different color per class in a scatter plot:\n",
    "        plt.scatter(z_codes_all[:,0], z_codes_all[:,1], c=lbls_all, alpha=0.5)  # Plot the first 2 dimensions.\n",
    "        plt.show()\n",
    "    \n",
    "    # Get the minimum and maximum values of z per dimension (neuron) of Z. We will use this at a later task\n",
    "    min_z = np.min(z_codes_all, axis=0)  # min and max for each dimension of z, over all samples.\n",
    "    max_z = np.max(z_codes_all, axis=0)  # Numpy array (vector) of shape [number of z dimensions]\n",
    "    \n",
    "    return min_z, max_z\n",
    "\n",
    "\n",
    "# Encode training samples, and get the min and max values of the z codes (for each dimension)\n",
    "min_z, max_z = encode_and_get_min_max_z(autoencoder_thin,\n",
    "                                        train_imgs_flat,\n",
    "                                        train_lbls,\n",
    "                                        batch_size=100,\n",
    "                                        total_iterations=100)\n",
    "print(\"Min Z value per dimension of bottleneck:\", min_z)\n",
    "print(\"Max Z value per dimension of bottleneck:\", max_z)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If all went well, you should see a plot where the codes span from 0 to +80 (approx).\\\n",
    "\n",
    "**Questions:**\n",
    "- Why dont we have negative values in these codes? Is this a general property of AutoEncoders, or of the specific implementation?\\\n",
    "- What do you observe about how codes of different classes (colors) are grouped? Do similar samples seem to be encoded far or close? Why is the AE learning in this way?\n",
    "- Is the space of Z full of samples everywhere, or are there holes? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4: Train an Auto-Encoder with a larger bottleneck layer\n",
    "\n",
    "The smaller the bottleneck layer (less neurons), the less information is allowed to be encoded for the inputs.\\\n",
    "We previously constructed and trained an AE with only 2 neurons in the bottleneck layer. Quite a restriction!\\\n",
    "\n",
    "We will now train an AE with a larger bottleneck layer to observe the difference in loss and how well are images reconstructed.\n",
    "\n",
    "Below, train an AE with a bottleneck layer of 32 neurons. Then think about the questions further below..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The below is a copy paste from Task 2.\n",
    "# ========== TODO: Use a bottleneck of 32 neurons and train it =================\n",
    "\n",
    "# Create the network\n",
    "rng = np.random.RandomState(seed=SEED)\n",
    "autoencoder_wide = Autoencoder(rng=rng,\n",
    "                               D_in=H_height*W_width,\n",
    "                               D_hid_enc=256,\n",
    "                               D_bottleneck=????????,  # <--------- Width of Bottleneck\n",
    "                               D_hid_dec=256)\n",
    "# Start training\n",
    "unsupervised_training_AE(autoencoder_wide,\n",
    "                         reconstruction_loss,\n",
    "                         rng,\n",
    "                         train_imgs_flat,\n",
    "                         batch_size=40,\n",
    "                         learning_rate=3e-3,\n",
    "                         total_iters=1000,\n",
    "                         iters_per_recon_plot=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Questions:**\n",
    "- Compare the loss at the end of the training in comparison to the loss of the AE with bottleneck with 2 neurons. Is it higher or lower? Why?\n",
    "- How do the reconstructed images compare with those from Task 2?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 5: Is basic Auto-Encoder appropriate for synthesizing new data?\n",
    "\n",
    "An Encoder of an AE learns a mapping from x (image space) to z (code).\\\n",
    "A Decoder of an AE learns a mapping from z (code) to x (image space).\n",
    "\n",
    "We could try to use the Decoder of a pre-trained basic AE to try and synthesize non-existing data.\\\n",
    "How?\\\n",
    "We could sample a random code z, and decode it back to image space with the decoder.\\\n",
    "Will it work? Lets explore...\n",
    "\n",
    "![title](./documentation/synthesize_data.png)\n",
    "\n",
    "**We will create a new network, that is a Decoder-only network.**\\\n",
    "We will **NOT train** it.\\\n",
    "Instead, we will use the **pre-trained weights of the decoder of the AE from the previous Task 4**,\\\n",
    "to initialize the weights of this new Decoder.\\\n",
    "This is equivalent to taking the previous pre-trained AE, and throwing away its encoder part.\\\n",
    "We will then use this Decoder to **map randomly sampled codes z back to image space**, generating novel images.\\\n",
    "We will finally judge whether this approach is a solid approach to image synthesis (generation).\n",
    "\n",
    "Lets go step by step... Lets first create the decoder...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder():\n",
    "    def __init__(self, pretrained_ae):\n",
    "        ############ TODO: Fill in the gaps. The aim is: ... ############\n",
    "        # ... to use the weights of the pre-trained AE's DECODER,... ####\n",
    "        # ... to initialize this Decoder.                            ####\n",
    "        # Reminder: pretrained_ae.params[LAYER] contrains the params of the corresponding layer. See Task 2.\n",
    "        w1 = torch.tensor(pretrained_ae.params[????], dtype=torch.float, requires_grad=False)\n",
    "        w2 = torch.tensor(pretrained_ae.params[3], dtype=torch.float, requires_grad=False)\n",
    "        self.params = [w1, w2]\n",
    "        ###########################################################################\n",
    "        \n",
    "        \n",
    "    def decode(self, z_batch):\n",
    "        # Reconstruct a batch of images from a batch of z codes.\n",
    "        # z_batch: Random codes. Numpy array of shape: [batch size, number of z dimensions]\n",
    "        [w1, w2] = self.params\n",
    "        \n",
    "        z_batch_t = torch.tensor(z_batch, dtype=torch.float)  # Making a Pytorch tensor from Numpy array.\n",
    "        # Adding an activation with value 1, for the bias. Similar to Task 2.\n",
    "        unary_feature_for_bias = torch.ones(size=(z_batch_t.shape[0], 1)) # [N, 1] column vector.\n",
    "        \n",
    "        ##### TODO: Fill in the gaps, to REPLICATE the decoder of the AE from Task 4 #####\n",
    "        # Hidden Layer of Decoder:\n",
    "        z_batch_act_ext = torch.cat((z_batch_t, unary_feature_for_bias), dim=1)\n",
    "        h1_preact = z_batch_act_ext.mm(w1)\n",
    "        h1_act = h1_preact.???????(min=0) # <--------------- RELU, like the AE's decoder\n",
    "        # Output Layer:\n",
    "        h1_ext = torch.cat((h1_act, unary_feature_for_bias), dim=1)\n",
    "        h2_preact = h1_ext.???????(w2)\n",
    "        h2_act = torch.tanh(h2_preact)\n",
    "        ##################################################################################\n",
    "        # Output\n",
    "        x_pred = h2_act\n",
    "        \n",
    "        return x_pred\n",
    "        \n",
    "# Lets instantiate this Decoder, using the pre-trained AE with 32-dims (\"wider\") bottleneck:\n",
    "net_decoder_pretrained = Decoder(autoencoder_wide)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If this were implemented correctly, there should be no output (except a \"UserWarning\" from pytorch perhaps).\n",
    "If you see any other problem, then you may have done something wrong.\n",
    "\n",
    "Assuming that the above did not return any issue (except perhaps a warning), lets continue...\n",
    "\n",
    "We want to sample random Z and give it to the decoder, to synthesize a new image.\n",
    "\n",
    "**But, from what range of values should we draw Z codes???**\\\n",
    "We have to find what range of values is covered by the embeddings when training the original AutoEncoder!!!\\\n",
    "We previously saw this for the AE with 2-dim Z. Now lets do it for the AE with 32-dim ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: This function was implemented in Task 3. We simply call it again, but for a different AE, the wider.\n",
    "\n",
    "# Encode training samples, and get the min and max values of the z codes (for each dimension)\n",
    "min_z_wider, max_z_wider = encode_and_get_min_max_z(autoencoder_wide,\n",
    "                                                    train_imgs_flat,\n",
    "                                                    train_lbls,\n",
    "                                                    batch_size=100,\n",
    "                                                    total_iterations=None,  # So that it runs over all data.\n",
    "                                                    plot_2d_embedding=False)  # Code is 32-Dims. Cant plot in 2D\n",
    "print(\"Min Z value per dimension:\", min_z_wider)\n",
    "print(\"Max Z value per dimension:\", max_z_wider)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Questions:**\n",
    "\n",
    "Compare min and max values per dimension. **What do you observe?** Dont be surprised if some dimensions of Z have \"collapsed\" and are just 0. Happens during training of neural networks that some neurons just \"die\"...\n",
    "\n",
    "**Now lets use this range \\[min,max\\] of values, to finally try and synthesize some new images...**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def synthesize(net_decoder,\n",
    "               rng,\n",
    "               z_min,\n",
    "               z_max,\n",
    "               n_samples):\n",
    "    # net_decoder: Decoder with pre-trained weights.\n",
    "    # z_min: numpy array (vector) of shape [dimensions-of-z]\n",
    "    # z_max: numpy array (vector) of shape [dimensions-of-z]\n",
    "    # n_samples: how many samples to produce.\n",
    "    \n",
    "    assert len(z_min.shape) == 1 and len(z_max.shape) == 1\n",
    "    assert z_min.shape[0] == z_max.shape[0]\n",
    "    \n",
    "    z_dims = z_min.shape[0]  # Dimensionality of z codes (and input to decoder).\n",
    "    \n",
    "    # Create samples of z uniformly sampled from [z_min, z_max]\n",
    "    z_samples = np.random.random_sample([n_samples, z_dims])  # Returns samples from uniform([0, 1))\n",
    "    z_samples = z_samples * (z_max - z_min)  # Scales [0,1] range ==> [0,(max-min)] range\n",
    "    z_samples = z_samples + z_min  # Puts the [0,(max-min)] range ==> [min, max] range\n",
    "    \n",
    "    x_samples = net_decoder.decode(z_samples)\n",
    "    \n",
    "    x_samples_np = x_samples if type(x_samples) is np.ndarray else x_samples.detach().numpy()  # torch to numpy\n",
    "    \n",
    "    for x_sample in x_samples_np:\n",
    "        plot_image(x_sample.reshape([H_height, W_width]))\n",
    "       \n",
    "    \n",
    "# Lets finally run the synthesis and see what happens...\n",
    "rng = np.random.RandomState(seed=SEED)\n",
    "\n",
    "synthesize(net_decoder_pretrained,\n",
    "           rng,\n",
    "           min_z_wider,  # From further above\n",
    "           max_z_wider,  # From further above\n",
    "           n_samples=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If everything was implemented correctly, you should see above images created by the decoder for the randomly sampled z-codes.\n",
    "\n",
    "**Questions:**\n",
    "\n",
    "- Observe the images. How many of them look realistic and good quality digits? Many? Half? Few?\n",
    "- Are they comparable in quality with the reconstructions obtained for the actual training data at Task 4?\n",
    "- If the quality of this synthesized data is not as good as you d expect from what you saw in Task 4, why do you think that is? Can you relate it with a characteristic of the plot of the embeddings in Task 3?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 6: Learning from Unlabelled data with AE, to complement Supervised Classifier when Labelled data are limited: Lets first train a supervised Classifier 'from scratch'\n",
    "\n",
    "We often want to train a **Classifier with labelled** data for a specific task. But labelled data are often **limited**, leading to sub-optimal performance of the Classifiers trained on them.\n",
    "\n",
    "AEs learn useful representations from unlabelled data, which are easy to collect in large numbers. We would like to use the learned 'knowledge' (parameters) of an Unsupervised AE, to train even stronger Classifiers with limited labelled data.\n",
    "\n",
    "How? We will see in the following 3 tasks.\n",
    "\n",
    "First, we will create and train a fully-supervised MLP classifier only **on very limited (100) labelled data**.\n",
    "\n",
    "The goal is to compare the performance of this classifier with what we achieve when complementing it with unlabelled data using an AutoEncoder (in later Task).\n",
    "\n",
    "To make such a comparison \"fair\", **this fully-supervised MLP classifier will have exactly the same architecture as the encoder of the autoencoder_wide we built in Task 4, plus one additional classification layer**. This is exactly the **same as** in the task of pre-training the MLP classifier with using the AE weights.\n",
    "\n",
    "![title](./documentation/classifier_scratch.png)\n",
    "\n",
    "**The below code for creating a classifier, cross entropy loss, and training loop is complete.**\\\n",
    "Read it, and understand it, because we will use it for the next 2 tasks as well, to compare with performance achieved with classifier pre-trained with AE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier_3layers(Network):\n",
    "    def __init__(self, D_in, D_hid_1, D_hid_2, D_out, rng):\n",
    "        D_in = D_in\n",
    "        D_hid_1 = D_hid_1\n",
    "        D_hid_2 = D_hid_2\n",
    "        D_out = D_out\n",
    "        \n",
    "        # === NOTE: Notice that this is exactly the same architecture as encoder of AE in Task 4 ====\n",
    "        w_1_init = rng.normal(loc=0.0, scale=0.01, size=(D_in+1, D_hid_1))\n",
    "        w_2_init = rng.normal(loc=0.0, scale=0.01, size=(D_hid_1+1, D_hid_2))\n",
    "        w_out_init = rng.normal(loc=0.0, scale=0.01, size=(D_hid_2+1, D_out))\n",
    "        \n",
    "        w_1 = torch.tensor(w_1_init, dtype=torch.float, requires_grad=True)\n",
    "        w_2 = torch.tensor(w_2_init, dtype=torch.float, requires_grad=True)\n",
    "        w_out = torch.tensor(w_out_init, dtype=torch.float, requires_grad=True)\n",
    "        \n",
    "        self.params = [w_1, w_2, w_out]\n",
    "        \n",
    "        \n",
    "    def forward_pass(self, batch_inp):\n",
    "        # compute predicted y\n",
    "        [w_1, w_2, w_out] = self.params\n",
    "        \n",
    "        # In case input is image, make it a tensor.\n",
    "        batch_imgs_t = torch.tensor(batch_inp, dtype=torch.float) if type(batch_inp) is np.ndarray else batch_inp\n",
    "        \n",
    "        unary_feature_for_bias = torch.ones(size=(batch_imgs_t.shape[0], 1)) # [N, 1] column vector.\n",
    "        x = torch.cat((batch_imgs_t, unary_feature_for_bias), dim=1) # Extra feature=1 for bias.\n",
    "        \n",
    "        # === NOTE: This is the same architecture as encoder of AE in Task 4, with extra classification layer ===\n",
    "        # Layer 1\n",
    "        h1_preact = x.mm(w_1)\n",
    "        h1_act = h1_preact.clamp(min=0)\n",
    "        # Layer 2 (corresponds to bottleneck of the AE):\n",
    "        h1_ext = torch.cat((h1_act, unary_feature_for_bias), dim=1)\n",
    "        h2_preact = h1_ext.mm(w_2)\n",
    "        h2_act = h2_preact.clamp(min=0)\n",
    "        # Output classification layer\n",
    "        h2_ext = torch.cat((h2_act, unary_feature_for_bias), dim=1)\n",
    "        h_out = h2_ext.mm(w_out)\n",
    "        \n",
    "        logits = h_out\n",
    "        \n",
    "        # === Addition of a softmax function for \n",
    "        # Softmax activation function.\n",
    "        exp_logits = torch.exp(logits)\n",
    "        y_pred = exp_logits / torch.sum(exp_logits, dim=1, keepdim=True) \n",
    "        # sum with Keepdim=True returns [N,1] array. It would be [N] if keepdim=False.\n",
    "        # Torch broadcasts [N,1] to [N,D_out] via repetition, to divide elementwise exp_h2 (which is [N,D_out]).\n",
    "        \n",
    "        return y_pred\n",
    "\n",
    "    \n",
    "def cross_entropy(y_pred, y_real, eps=1e-7):\n",
    "    # y_pred: Predicted class-posterior probabilities, returned by forward_pass. Numpy array of shape [N, D_out]\n",
    "    # y_real: One-hot representation of real training labels. Same shape as y_pred.\n",
    "    \n",
    "    # If number array is given, change it to a Torch tensor.\n",
    "    y_pred = torch.tensor(y_pred, dtype=torch.float) if type(y_pred) is np.ndarray else y_pred\n",
    "    y_real = torch.tensor(y_real, dtype=torch.float) if type(y_real) is np.ndarray else y_real\n",
    "    \n",
    "    x_entr_per_sample = - torch.sum( y_real*torch.log(y_pred+eps), dim=1)  # Sum over classes, axis=1\n",
    "    \n",
    "    loss = torch.mean(x_entr_per_sample, dim=0) # Expectation of loss: Mean over samples (axis=0).\n",
    "    return loss\n",
    "\n",
    "\n",
    "\n",
    "from utils.plotting import plot_train_progress_2\n",
    "\n",
    "def train_classifier(classifier,\n",
    "                     pretrained_AE,\n",
    "                     loss_func,\n",
    "                     rng,\n",
    "                     train_imgs,\n",
    "                     train_lbls,\n",
    "                     test_imgs,\n",
    "                     test_lbls,\n",
    "                     batch_size,\n",
    "                     learning_rate,\n",
    "                     total_iters,\n",
    "                     iters_per_test=-1):\n",
    "    # Arguments:\n",
    "    # classifier: A classifier network. It will be trained by this function using labelled data.\n",
    "    #             Its input will be either original data (if pretrained_AE=0), ...\n",
    "    #             ... or the output of the feature extractor if one is given.\n",
    "    # pretrained_AE: A pretrained AutoEncoder that will *not* be trained here.\n",
    "    #      It will be used to encode input data.\n",
    "    #      The classifier will take as input the output of this feature extractor.\n",
    "    #      If pretrained_AE = None: The classifier will simply receive the actual data as input.\n",
    "    # train_imgs: Vectorized training images\n",
    "    # train_lbls: One hot labels\n",
    "    # test_imgs: Vectorized testing images, to compute generalization accuracy.\n",
    "    # test_lbls: One hot labels for test data.\n",
    "    # batch_size: batch size\n",
    "    # learning_rate: come on...\n",
    "    # total_iters: how many SGD iterations to perform.\n",
    "    # iters_per_test: We will 'test' the model on test data every few iterations as specified by this.\n",
    "    \n",
    "    values_to_plot = {'loss':[], 'acc_train': [], 'acc_test': []}\n",
    "    \n",
    "    optimizer = optim.Adam(classifier.params, lr=learning_rate)\n",
    "        \n",
    "    for t in range(total_iters):\n",
    "        # Sample batch for this SGD iteration\n",
    "        train_imgs_batch, train_lbls_batch = get_random_batch(train_imgs, train_lbls, batch_size, rng)\n",
    "        \n",
    "        # Forward pass\n",
    "        if pretrained_AE is None:\n",
    "            inp_to_classifier = train_imgs_batch\n",
    "        else:\n",
    "            _, z_codes = pretrained_AE.forward_pass(train_imgs_batch)  # AE encodes. Output will be given to Classifier\n",
    "            inp_to_classifier = z_codes\n",
    "            \n",
    "        y_pred = classifier.forward_pass(inp_to_classifier)\n",
    "        \n",
    "        # Compute loss:\n",
    "        y_real = train_lbls_batch\n",
    "        loss = loss_func(y_pred, y_real)  # Cross entropy\n",
    "        \n",
    "        # Backprop and updates.\n",
    "        optimizer.zero_grad()\n",
    "        grads = classifier.backward_pass(loss)\n",
    "        optimizer.step()\n",
    "        \n",
    "        \n",
    "        # ==== Report training loss and accuracy ======\n",
    "        # y_pred and loss can be either np.array, or torch.tensor (see later). If tensor, make it np.array.\n",
    "        y_pred_numpy = y_pred if type(y_pred) is np.ndarray else y_pred.detach().numpy()\n",
    "        y_pred_lbls = np.argmax(y_pred_numpy, axis=1) # y_pred is soft/probability. Make it a hard one-hot label.\n",
    "        y_real_lbls = np.argmax(y_real, axis=1)\n",
    "        \n",
    "        acc_train = np.mean(y_pred_lbls == y_real_lbls) * 100. # percentage\n",
    "        \n",
    "        loss_numpy = loss if type(loss) is type(float) else loss.item()\n",
    "        print(\"[iter:\", t, \"]: Training Loss: {0:.2f}\".format(loss), \"\\t Accuracy: {0:.2f}\".format(acc_train))\n",
    "        \n",
    "        # =============== Every few iterations, show reconstructions ================#\n",
    "        if t==total_iters-1 or t%iters_per_test == 0:\n",
    "            if pretrained_AE is None:\n",
    "                inp_to_classifier_test = test_imgs\n",
    "            else:\n",
    "                _, z_codes_test = pretrained_AE.forward_pass(test_imgs)\n",
    "                inp_to_classifier_test = z_codes_test\n",
    "                \n",
    "            y_pred_test = classifier.forward_pass(inp_to_classifier_test)\n",
    "            \n",
    "            # ==== Report test accuracy ======\n",
    "            y_pred_test_numpy = y_pred_test if type(y_pred_test) is np.ndarray else y_pred_test.detach().numpy()\n",
    "            \n",
    "            y_pred_lbls_test = np.argmax(y_pred_test_numpy, axis=1)\n",
    "            y_real_lbls_test = np.argmax(test_lbls, axis=1)\n",
    "            acc_test = np.mean(y_pred_lbls_test == y_real_lbls_test) * 100.\n",
    "            print(\"\\t\\t\\t\\t\\t\\t\\t\\t Testing Accuracy: {0:.2f}\".format(acc_test))\n",
    "            \n",
    "            # Keep list of metrics to plot progress.\n",
    "            values_to_plot['loss'].append(loss_numpy)\n",
    "            values_to_plot['acc_train'].append(acc_train)\n",
    "            values_to_plot['acc_test'].append(acc_test)\n",
    "                \n",
    "    # In the end of the process, plot loss accuracy on training and testing data.\n",
    "    plot_train_progress_2(values_to_plot['loss'], values_to_plot['acc_train'], values_to_plot['acc_test'], iters_per_test)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now below, lets finally create an instance of this 3-layered classifier.\n",
    "\n",
    "Fill in the number of neurons in the 2 hidden layers of the MLP Classifier, to be the same as the encoder of the AE (with the wide 32 bottleneck)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Classifier from scratch (initialized randomly)\n",
    "\n",
    "# Create the network\n",
    "rng = np.random.RandomState(seed=SEED)\n",
    "net_classifier_from_scratch = Classifier_3layers(D_in=H_height*W_width,\n",
    "                                                 D_hid_1=???????, # TODO: Use same as layer 1 of encoder of wide AE (Task 4)\n",
    "                                                 D_hid_2=???????,  # TODO: Use same as layer 2 of encoder of wide AE (Task 4)\n",
    "                                                 D_out=C_classes,\n",
    "                                                 rng=rng)\n",
    "# Start training\n",
    "train_classifier(net_classifier_from_scratch,\n",
    "                 None,  # No pretrained AE\n",
    "                 cross_entropy,\n",
    "                 rng,\n",
    "                 train_imgs_flat[:100],\n",
    "                 train_lbls_onehot[:100],\n",
    "                 test_imgs_flat,\n",
    "                 test_lbls_onehot,\n",
    "                 batch_size=40,\n",
    "                 learning_rate=3e-3,\n",
    "                 total_iters=1000,\n",
    "                 iters_per_test=20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If everything went as expected, you should see the training loss of the classifier going down, reaching (close to) 0. At the end, you will find a plot of the loss curve, the training accuracy (approx 100%), and the testing accuracy (approx 57%).\n",
    "\n",
    "**Questions**:\n",
    "\n",
    "- How do you explain the \"generalization gap\" between training and testing accuracy?\n",
    "- How do you think the amount of labelled data (above =100 by default) affect this gap? Feel free to experiment with other amounts of labelled data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 7: Use Unsupervised AE as 'pre-trained feature-extractor' for a supervised Classifier when labels are limited\n",
    "\n",
    "There are 2 methods that a pre-trained AE can be used to help training better Classifiers when labelled data are limited.\n",
    "\n",
    "Approach-1: We take the encoder of the pre-trained AE (already trained with unlabelled data) and place an untrained, small (often 1 layer) Classifier on top. The Classifier receives as input the output of the encoder, i.e. the codes z of data x. We then use the limited labelled data for training. Importantly, we only train the small Classifier. The encoder is used as 'frozen' (does not get trained further) feature extractor. This method allows training with limited labelled data because we only train the small Classifier (the encoder is frozen), therefore we are less likely to overfit in theory. See next figure for a visual explanation.\n",
    "\n",
    "![title](./documentation/encoder_frozen.png)\n",
    "\n",
    "Approach-2 will be explored in the next and final task.\n",
    "\n",
    "Complete and run the code below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Train classifier on top of pre-trained AE encoder\n",
    "\n",
    "class Classifier_1layer(Network):\n",
    "    # Classifier with just 1 layer, the classification layer\n",
    "    def __init__(self, D_in, D_out, rng):\n",
    "        # D_in: dimensions of input\n",
    "        # D_out: dimension of output (number of classes)\n",
    "        \n",
    "        #### TODO: Fill in the blanks ######################\n",
    "        w_out_init = rng.normal(loc=0.0, scale=0.01, size=(D_in+1, D_out))\n",
    "        w_out = torch.tensor(????????, dtype=torch.float, requires_grad=True)\n",
    "        ####################################################\n",
    "        self.params = [w_out]\n",
    "        \n",
    "        \n",
    "    def forward_pass(self, batch_inp):\n",
    "        # compute predicted y\n",
    "        [w_out] = self.params\n",
    "        \n",
    "        # In case input is image, make it a tensor.\n",
    "        batch_inp_t = torch.tensor(batch_inp, dtype=torch.float) if type(batch_inp) is np.ndarray else batch_inp\n",
    "        \n",
    "        unary_feature_for_bias = torch.ones(size=(batch_inp_t.shape[0], 1)) # [N, 1] column vector.\n",
    "        batch_inp_ext = torch.cat((batch_inp_t, unary_feature_for_bias), dim=1) # Extra feature=1 for bias.\n",
    "        \n",
    "        # Output classification layer\n",
    "        logits = batch_inp_ext.mm(w_out)\n",
    "        \n",
    "        # Output layer activation function\n",
    "        # Softmax activation function.\n",
    "        exp_logits = torch.exp(logits)\n",
    "        y_pred = exp_logits / torch.sum(exp_logits, dim=1, keepdim=True) \n",
    "        # sum with Keepdim=True returns [N,1] array. It would be [N] if keepdim=False.\n",
    "        # Torch broadcasts [N,1] to [N,D_out] via repetition, to divide elementwise exp_h2 (which is [N,D_out]).\n",
    "        \n",
    "        return y_pred\n",
    "    \n",
    "    \n",
    "    \n",
    "# Create the network\n",
    "rng = np.random.RandomState(seed=SEED) # Random number generator\n",
    "# As input, it will be getting z-codes from the AE with 32-neurons bottleneck from Task 4.\n",
    "classifier_1layer = Classifier_1layer(autoencoder_wide.D_bottleneck,  # Input dimension is dimensions of AE's Z\n",
    "                                      C_classes,\n",
    "                                      rng=rng)\n",
    "\n",
    "########### TODO: Fill in the gaps to start training ####################\n",
    "# Give to the function the 1-layer classifier, as well as the pre-trained AE that will work as feature extractor.\n",
    "# For the pre-trained AE, give the instance of 'wide' AE that has 32-neurons bottleneck, which you trained in Task 4.\n",
    "train_classifier(?????????????,\n",
    "                 autoencoder_wide,  # Pretrained AE, to use as feature extractor.\n",
    "                 cross_entropy,\n",
    "                 rng,\n",
    "                 train_imgs_flat[:100],\n",
    "                 train_lbls_onehot[:100],\n",
    "                 test_imgs_flat,\n",
    "                 test_lbls_onehot,\n",
    "                 batch_size=40,\n",
    "                 learning_rate=3e-3,   # 5e-3, is the best for 1-layer classifier and all data.\n",
    "                 total_iters=1000,\n",
    "                 iters_per_test=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you completed the task appropriately, you shoudl see the model getting trained and performance reported at the bottom. Training accuracy should approach 100% and test accuracy should be approximately 65%\n",
    "\n",
    "**Questions:**\n",
    "- Compare with test accuracy from Task 7. How do the 2 differ? How would you justify the difference?\n",
    "- Try the same experiments (Task 6 and Task 7) using different amounts of labelled data. What do you observe as the number of training data increases?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 8: Use parameters of an Unsupervised AE's encoder to initialize weights of a supervised Classifier, followed by refinement using limited labels\n",
    "\n",
    "Approach-2: The second approach is to build a Classifier that has the same architecture as the encoder of an AutoEncoder, followed by an extra classification layer. We first train the Autoencoder on unlabelled data (as done in Task 4). Then, we **use the pre-trained weights of the AE's encoder to initialize the corresponding parameters of the Classifier**. The classification layer of the Classifier is initialized randomly. Then, **with the limited labelled data, we refine (train) all the parameters of the classifier**. The advantage of this approach is that the Classifier begins with the \"knowledge\" (parameters) extracted from unlabelled data, and then all of it is refined with extra \"knowledge\" from the labelled data. See figure below for visual explanation.\n",
    "\n",
    "![title](./documentation/refinement.png)\n",
    "\n",
    "**The code below is complete.**\\\n",
    "Read it, understand it, run it, and try to answer the questions below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-train a classifier.\n",
    "\n",
    "# The below classifier has THE SAME architecture as the 3-layer Classifier that we trained...\n",
    "# ... in a purely supervised manner in Task-6.\n",
    "# This is done by inheriting the class (Classifier_3layers), therefore uses THE SAME forward_pass() function.\n",
    "# THE ONLY DIFFERENCE is in the construction __init__.\n",
    "# This 'pretrained' classifier receives as input a pretrained autoencoder (pretrained_AE) from Task 4.\n",
    "# It then uses the parameters of the AE's encoder to initialize its own parameters, rather than random initialization.\n",
    "# The model is then trained all together.\n",
    "class Classifier_3layers_pretrained(Classifier_3layers):\n",
    "    def __init__(self, pretrained_AE, D_in, D_out, rng):\n",
    "        D_in = D_in\n",
    "        D_hid_1 = 256\n",
    "        D_hid_2 = 32\n",
    "        D_out = D_out\n",
    "\n",
    "        w_out_init = rng.normal(loc=0.0, scale=0.01, size=(D_hid_2+1, D_out))\n",
    "        \n",
    "        w_1 = torch.tensor(pretrained_AE.params[0], dtype=torch.float, requires_grad=True)\n",
    "        w_2 = torch.tensor(pretrained_AE.params[1], dtype=torch.float, requires_grad=True)\n",
    "        w_out = torch.tensor(w_out_init, dtype=torch.float, requires_grad=True)\n",
    "        \n",
    "        self.params = [w_1, w_2, w_out]\n",
    "        \n",
    "# Create the network\n",
    "rng = np.random.RandomState(seed=SEED) # Random number generator\n",
    "classifier_3layers_pretrained = Classifier_3layers_pretrained(autoencoder_wide,  # The AE pre-trained in Task 4.\n",
    "                                                              train_imgs_flat.shape[1],\n",
    "                                                              C_classes,\n",
    "                                                              rng=rng)\n",
    "\n",
    "# Start training\n",
    "# NOTE: Only the 3-layer pretrained classifier is used, and will be trained all together.\n",
    "# No frozen feature extractor.\n",
    "train_classifier(classifier_3layers_pretrained,  # classifier that will be trained.\n",
    "                 None,  # No pretrained AE to act as 'frozen' feature extractor.\n",
    "                 cross_entropy,\n",
    "                 rng,\n",
    "                 train_imgs_flat[:100],\n",
    "                 train_lbls_onehot[:100],\n",
    "                 test_imgs_flat,\n",
    "                 test_lbls_onehot,\n",
    "                 batch_size=40,\n",
    "                 learning_rate=3e-3,\n",
    "                 total_iters=1000,\n",
    "                 iters_per_test=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Questions:**\n",
    "\n",
    "- How does generalization performance of this Classifier, pretrained via AE, and where all its parameters are refined with the limited labelled data, compares with the same classifier trained ONLY with labelled data (Task 6)?\n",
    "- How does it compare with the approach in Task 7? Is the generalization better or worse? Why?\n",
    "- How do you think this approach (Task 8) compares with previous approach (Task 7) when more or less labelled data are used? What is the advantage and disadvantage of this approach that can influence whether its better or worse when we use more or less labelled data?\n",
    "- Feel free to repeat Tasks 6,7,8 for different amounts of labelled data to investigate.\n",
    "\n",
    "**Note:** By default, the code of Tasks 6,7,8 above trains the model using only 100 out of the 60000 labelled training data. Feel free to repeat Tasks 6,7,8 for different amounts of labelled data, compare performance behaviour. This may help you answer the question above.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This notebook:\n",
    "Copyright 2021, University of Birmingham  \n",
    "Tutorial for Neural Computation  \n",
    "For issues e-mail: k.kamnitsas@bham.ac.uk"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
