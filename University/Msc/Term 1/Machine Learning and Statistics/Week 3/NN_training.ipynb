{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c2a114363b095e61b2375f46f128a583",
     "grade": false,
     "grade_id": "cell-02dc51a9b9e73a22",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Neural networks\n",
    "\n",
    "In this exercise, we will write a neural network with one hidden layer for a multi-class classification problem. We are only concerned about the functionality, not the performance. \n",
    "\n",
    "Please check the plot below for a sketch of the NN structure and the naming conventions\n",
    "![](NN_structure1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from IPython.display import IFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1/(1.0+np.exp(-z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5026708abe5f3b8d9b2225b0466d39cb",
     "grade": false,
     "grade_id": "cell-dff474dc8d1d6835",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Implement the softmax function which is used for multi-class classification problems. The function converts the outputs of the last layer of a neural network to probabilities of belonging to one of the classes in the classification. It takes a `nd x nc` matrix as input where `nd` is the number of data points and `nc` the number of classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "\n",
    "    mx = x.max( axis=1) # for numerical stability\n",
    "    e = np.exp(x.T-mx).T\n",
    "    s = np.sum(e,axis=1)\n",
    "    return (e.T/s).T\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d2c46adcfa133f5200e366254cf8b67c",
     "grade": true,
     "grade_id": "cell-30634d0fab35884a",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert np.isclose(softmax(np.array([[400,300,100]])), np.array([[1.00000000e+000, 3.72007598e-044, 5.14820022e-131]])).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "fe6f3d1233a878e0bf91ed3fec67665e",
     "grade": true,
     "grade_id": "cell-00c53897ece398e2",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "ztest = np.array([\n",
    "    [10,-10,-10,-10,-10],\n",
    "    [ 0.47227959,  0.08658337,  0.08738361,  0.14692136, -0.26323141]  \n",
    "])\n",
    "\n",
    "assert softmax(ztest).shape == ztest.shape\n",
    "assert np.isclose( softmax(ztest),  np.array([[9.99999992e-01, 2.06115361e-09, 2.06115361e-09, 2.06115361e-09,\n",
    "        2.06115361e-09],\n",
    "       [2.80738986e-01, 1.90896070e-01, 1.91048893e-01, 2.02768946e-01,\n",
    "        1.34547105e-01]])).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ed7d3dc2b7fa0dea3a79df920900df41",
     "grade": false,
     "grade_id": "cell-371d8d240354e9f3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "This is the inputs we will use to test our implementation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ni = 4\n",
    "nh = 5\n",
    "no = 7\n",
    "nd = 10\n",
    "np.random.seed(1212)\n",
    "v = np.random.random(size=(nh,ni+1))\n",
    "iin = np.random.random(size=(nd,ni))\n",
    "w = np.random.random(size=(no,nh+1))\n",
    "xin = np.random.random(size=(nd,nh))\n",
    "\n",
    "ys = [ [1],[0],[2],[2],[3],[4],[1],[6],[5],[4]]\n",
    "enc = OneHotEncoder(sparse=False, categories='auto')\n",
    "yij = enc.fit_transform(ys)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "717eb306d458c629d2d79d372e60e7a5",
     "grade": false,
     "grade_id": "cell-c036ea4764893a76",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Define the function `forwardPass` that calculates the output of a network layer with parameters `w` and inputs `inp`, a `nd x nf` matrix of `nd` data samples with `nf` input features. Use the `sigmoid` function (defined above) as your activation function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forwardPass(inp,w):\n",
    "    nd, nf = inp.shape\n",
    "    Xaug = np.empty( (nd, nf+1))\n",
    "    Xaug[:,0] = np.ones(nd)\n",
    "    Xaug[:,1:] = inp\n",
    "    z = np.dot(Xaug,w.T)\n",
    "    os = sigmoid(z)\n",
    "    return os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "234ce6eb708423fc79beacee90476a26",
     "grade": true,
     "grade_id": "cell-8d7fd1dbf45dc2a1",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert forwardPass(iin,v).shape == (nd, nh)\n",
    "assert np.isclose(forwardPass(iin,v),\n",
    "        np.array([[0.68837646, 0.7968613 , 0.79800127, 0.65126304, 0.7655118 ],\n",
    "       [0.81273724, 0.89213343, 0.86406507, 0.81389362, 0.86413682],\n",
    "       [0.82533989, 0.90025807, 0.84266121, 0.81861927, 0.85204489],\n",
    "       [0.82041648, 0.86399255, 0.86640616, 0.77089753, 0.85294415],\n",
    "       [0.82014636, 0.86158128, 0.80726324, 0.7968344 , 0.81375178],\n",
    "       [0.8229169 , 0.89412018, 0.86592284, 0.80538115, 0.86451929],\n",
    "       [0.85160111, 0.87919916, 0.8167712 , 0.82548824, 0.83057137],\n",
    "       [0.74336211, 0.86585661, 0.84263023, 0.73507403, 0.82932382],\n",
    "       [0.8342882 , 0.91019903, 0.91391614, 0.80546577, 0.90408941],\n",
    "       [0.71588926, 0.87475359, 0.92144777, 0.68106143, 0.89086182]])).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "882f09844b13de8ec642fde31ab05079",
     "grade": false,
     "grade_id": "cell-9b26fb034cf2a971",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Define the function `forwardPassSoftmax` that calculates the output of a network layer with parameters `w` and inputs `inp`, a `nd x nf` matrix of `nd` data samples with `nf` input features. Use the `softmax` function (defined above) as your activation function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forwardPassSoftmax(inp,w):\n",
    "    nd, nf = inp.shape\n",
    "    Xaug = np.empty( (nd, nf+1))\n",
    "    Xaug[:,0] = np.ones(nd)\n",
    "    Xaug[:,1:] = inp\n",
    "    z = np.dot(Xaug,w.T)\n",
    "    os = softmax(z)\n",
    "    return os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ebd3a28e1025de7b2ff96e6f4035fe8e",
     "grade": true,
     "grade_id": "cell-d7cc7f49db905301",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "hout = forwardPass(iin,v)\n",
    "assert forwardPassSoftmax(hout,w).shape == (nd, no)\n",
    "assert np.isclose(forwardPassSoftmax(hout,w),np.array([[0.07130809, 0.12409133, 0.11275842, 0.15968169, 0.16715723,\n",
    "        0.26521125, 0.099792  ],\n",
    "       [0.06725052, 0.12112668, 0.11300347, 0.15521793, 0.1656938 ,\n",
    "        0.28376125, 0.09394635],\n",
    "       [0.06783011, 0.12054124, 0.11408981, 0.15484997, 0.16421653,\n",
    "        0.28325473, 0.0952176 ],\n",
    "       [0.06792065, 0.12228852, 0.11538123, 0.15665073, 0.1683589 ,\n",
    "        0.27616102, 0.09323895],\n",
    "       [0.06888607, 0.12115008, 0.11676099, 0.15501673, 0.16230241,\n",
    "        0.2786129 , 0.09727082],\n",
    "       [0.06747572, 0.12117407, 0.11375229, 0.15555633, 0.16662511,\n",
    "        0.28177475, 0.09364173],\n",
    "       [0.06825527, 0.12057384, 0.1174965 , 0.15421526, 0.16208508,\n",
    "        0.28121404, 0.09616001],\n",
    "       [0.06903226, 0.12228289, 0.11112691, 0.15762792, 0.16737128,\n",
    "        0.27613176, 0.09642699],\n",
    "       [0.06636304, 0.1215225 , 0.11244603, 0.15617258, 0.1706804 ,\n",
    "        0.28243318, 0.09038227],\n",
    "       [0.06757224, 0.12412115, 0.10706434, 0.16057489, 0.17650805,\n",
    "        0.27296254, 0.09119679]])).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will check that our implementation works by comparing with `sklearn`'s implementation. We are not interested in the actual fit, but we need to call the fit function so that we have a working model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "classifier = MLPClassifier(\n",
    "    random_state=1,\n",
    "    hidden_layer_sizes=(nh,),\n",
    "    max_iter=10,\n",
    "    activation='logistic',\n",
    ")\n",
    "classifier.fit(iin,yij)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w1,w2 = classifier.coefs_\n",
    "w1, w2 = np.array(w1), np.array(w2)\n",
    "i1,i2 = classifier.intercepts_\n",
    "\n",
    "# combine the coefs and intercept values to weight vectors\n",
    "w1ni, w1no = w1.shape\n",
    "w1all = np.empty((w1no, w1ni+1))\n",
    "w1all[:,0] = i1\n",
    "w1all[:,1:] = w1.T\n",
    "w2ni, w2no = w2.shape\n",
    "w2all = np.empty((w2no, w2ni+1))\n",
    "w2all[:,0] = i2\n",
    "w2all[:,1:] = w2.T\n",
    "\n",
    "mySolution = forwardPassSoftmax(forwardPass(iin,w1all),w2all)\n",
    "classifier.out_activation_ = 'softmax'\n",
    "skSolution = classifier.predict_proba(iin)\n",
    "\n",
    "assert np.isclose(skSolution, mySolution).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "61b79b7f356fa5c461b336282b9a661b",
     "grade": false,
     "grade_id": "cell-14aacd9b957d4c13",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Define the following loss functions: \n",
    "- `Jyhat` is the loss as a function of the outputs of the output layer (the one with the softmax step.) \n",
    "- `Jx` is the loss as a function of the outputs of the hidden layer. \n",
    "- `J` is the loss as a function of the network input `I`, the parameters of the input-hidden connections `v`, the parameters of the hidden-output connections `w` and the one-hot encoded targets `yij`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# yij: true one-hot encoded classification of the input sample\n",
    "# yhat: outputs of the NN's output layer, including softmax\n",
    "def Jyhat(yhat,yij):\n",
    "    return - np.sum( yij * np.log(yhat) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5d2f860564727e7433c849e6eb1b3f51",
     "grade": true,
     "grade_id": "cell-40e0fac53890b2af",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "yhat = forwardPassSoftmax(forwardPass(iin,v),w)\n",
    "Jyhat(yhat,yij)\n",
    "assert np.isclose(Jyhat(yhat,yij) , 20.225746159683087)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# yij: true one-hot encoded classification of the input sample\n",
    "# X: inputs for the output layer (= outputs of the hidden layer)\n",
    "# w: weights for the conntection hidden-output\n",
    "def Jx(X,w,yij):\n",
    "    yhat = forwardPassSoftmax(X,w)\n",
    "    return Jyhat(yhat,yij) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e154afe23b49b50ed438e3f76d3b150e",
     "grade": true,
     "grade_id": "cell-6b666edb1bc04a9b",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "x = forwardPass(iin,v)\n",
    "Jx(x,w2all,yij)\n",
    "assert np.isclose(Jx(x,w,yij) , 20.225746159683087)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# yij: true one-hot encoded classification of the input sample\n",
    "# w: weights for the current layer\n",
    "# I: network input \n",
    "# v: weight of the connection input-hidden\n",
    "def J(I,v,w,yij):\n",
    "    x = forwardPass(I,v)\n",
    "    return Jx(x,w,yij)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c55c957307cf51f505470cb340746ea2",
     "grade": true,
     "grade_id": "cell-49700af7f0cf3e07",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "J(iin,w1all,w2all,yij)\n",
    "assert np.isclose(J(iin,v,w,yij) , 20.225746159683087)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Derivatives of the loss function\n",
    "\n",
    "We will now define different derivatives of the loss function. They will be useful for implementing gradient descent algorithm which updates the weights based on the derivatives of the loss function with respect to the weights. \n",
    "\n",
    "We will check our result numerically."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the derivative of the softmax function with respect to its arguments. If should return a `n x n` matrix, where `n` is the length ot the vector of arguments `z` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ds_dz(z):\n",
    "    # derivative of softmax function s(z_1,...,z_n) with respect to z)\n",
    "    zvec = np.array(z)\n",
    "    svec = softmax(np.array([zvec]))\n",
    "    return np.diag(svec[0]) -np.dot( svec.T , svec)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "48ca70df48b87e610e991d77c464ad7b",
     "grade": false,
     "grade_id": "cell-0de291184244b74c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "This checks the implementation of the derivative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a31dfbbb70fc9c382c971c9a9071e1cb",
     "grade": true,
     "grade_id": "cell-601b0565b203ea38",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "eps = 1e-8\n",
    "z = np.array([[0.1,0.2,0.3]])\n",
    "analytical = ds_dz(z[0])\n",
    "\n",
    "z2 = np.array([[0.1+eps,0.2,0.3]])\n",
    "numerical = (softmax(z2)-softmax(z))/eps\n",
    "\n",
    "assert np.isclose(numerical[0],analytical[0]).all() \n",
    "\n",
    "z2 = np.array([[0.1,0.2+eps,0.3]])\n",
    "numerical = (softmax(z2)-softmax(z))/eps\n",
    "\n",
    "assert np.isclose(numerical[0],analytical[1]).all() \n",
    "\n",
    "z2 = np.array([[0.1,0.2,0.3+eps]])\n",
    "numerical = (softmax(z2)-softmax(z))/eps\n",
    "\n",
    "assert np.isclose(numerical[0],analytical[2]).all() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = np.array([[0.1,0.2,0.3]])\n",
    "analytical = ds_dz(z[0])\n",
    "\n",
    "z2 = np.array([[0.1+eps,0.2,0.3]])\n",
    "numerical = (softmax(z2)-softmax(z))/eps\n",
    "\n",
    "analytical, numerical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1c924f9afa1a25d4787f7d75faf3cf77",
     "grade": false,
     "grade_id": "cell-fa39817a7ec91af5",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Define the function `dJ_dy` which is the derivative of the loss function with respect to the output of the softmax function. It should be a `nd x no` matrix (`no` output values for each of the `nd` events in our data sample). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dJ_dyhat(yhat, yij):\n",
    "    f1 = -(yij)/(yhat)\n",
    "    return f1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b2567cd8118c7baf9c0eaafd911ed9ed",
     "grade": true,
     "grade_id": "cell-747c26e7d934c9fa",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "yhat = forwardPassSoftmax(forwardPass(iin,v),w)\n",
    "assert dJ_dyhat(yhat,yij).shape == (nd, no)\n",
    "\n",
    "analytical = np.sum(dJ_dyhat(yhat, yij),axis=0)\n",
    "\n",
    "eps = 1e-6\n",
    "for i in range(no):\n",
    "    y2 = np.array(yhat)\n",
    "    y2[:,i] += eps\n",
    "    num = (Jyhat(y2,yij)-Jyhat(yhat,yij))/eps\n",
    "    assert np.isclose(num,analytical[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3804da8d08bc97e4eae0543dda3f792f",
     "grade": false,
     "grade_id": "cell-80addfa197eab460",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Define the function `dJ_dwij` which is the derivative of the loss function with respect to the weights between the hidden layer and the output layer. It should be a `nh+1 x no` matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dJ_dwij(xin,w,yij):    \n",
    "\n",
    "    nd, nh = xin.shape\n",
    "    _, no = yij.shape\n",
    "    yhat = forwardPassSoftmax(xin,w)\n",
    "    \n",
    "    Xaug = np.empty( (nd, nh+1)) # nd x (nf+1)\n",
    "    Xaug[:,1:] = xin\n",
    "    Xaug[:,0] = np.ones(nd)\n",
    "    z = np.dot(Xaug,w.T)\n",
    "\n",
    "    res = np.zeros( (nh+1, no))\n",
    "    djdy = dJ_dyhat(yhat, yij)  \n",
    "    \n",
    "    for k in range(nh+1):\n",
    "        for m in range(no):\n",
    "            for d in range(nd):\n",
    "                dydz = ds_dz( z[d])\n",
    "                for l in range(no):\n",
    "                    res[k,m] +=  djdy[d,l] * dydz[l,m]*Xaug[d,k] \n",
    "    \n",
    "    return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "79bcb4cf35947e49e9e4aa7386b60092",
     "grade": true,
     "grade_id": "cell-25ad4fe2799e8439",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "analytic = dJ_dwij(xin,w,yij)\n",
    "\n",
    "assert analytic.shape == (nh+1, no)\n",
    "\n",
    "for i in range(nh+1):\n",
    "    for j in range(no):\n",
    "        w2 = np.array(w)\n",
    "        w2[j,i] += eps \n",
    "        num = (Jx(xin,w2,yij)-Jx(xin,w,yij))/eps\n",
    "        assert np.isclose(analytic[i,j],num, rtol=1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "64a9de5eda3163f685ac1b4fea9f31ea",
     "grade": false,
     "grade_id": "cell-191cceb5ccc20276",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Define the function `dJ_dx` that is the derivative of the loss function with respect to the activation values of the hidden layer. It should be a `nd x nh` matrix. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dJ_dx(xin,w,yij):\n",
    "    nd, nh = xin.shape\n",
    "    Xaug = np.empty( (nd, nh+1))\n",
    "    Xaug[:,1:] = xin\n",
    "    Xaug[:,0] = np.ones(nd)\n",
    "    z = np.dot(Xaug,w.T)\n",
    "    _, nc = yij.shape\n",
    "    yhat = forwardPassSoftmax(xin,w)\n",
    "\n",
    "    dj_dx = np.zeros((nd,nh))\n",
    "    \n",
    "    dj_dy = dJ_dyhat(yhat, yij)\n",
    "    for i in range(nd):\n",
    "        dsdz = ds_dz(z[i])\n",
    "        for j in range(nh):\n",
    "            for k in range(nc):\n",
    "                for l in range(nc):\n",
    "                    dj_dx[i,j] += dj_dy[i,k] * dsdz[l,k] * w[l,j+1] \n",
    "    return dj_dx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "fe85f9cc2a76fef19dc9a7365b35a27c",
     "grade": true,
     "grade_id": "cell-8188e143baf8553f",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "analytical = dJ_dx(xin, w, yij)\n",
    "assert analytical.shape == (nd, nh)\n",
    "eps = 1e-7\n",
    "for i in range(ni):\n",
    "    for d in range(nd):\n",
    "        xin2 = np.array(xin)\n",
    "        xin2[d,i] += eps \n",
    "        num = (Jx(xin2,w,yij)-Jx(xin,w,yij))/eps\n",
    "        assert (np.isclose(analytical[d,i], num, rtol=1e-4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b1ec14e9796838b004ed5ff88b1c00d8",
     "grade": false,
     "grade_id": "cell-95bc336d9f0a2634",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Define the function `dJ_dvij` which is the derivative of the loss function with respect to the weights between the hidden layer and the output layer. It should be a `ni+1 x nh` matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dJ_dvij(iin,v,w,yij):    \n",
    "    nh, _ = v.shape\n",
    "    xin = forwardPass(iin,v)\n",
    "    nd, ni = iin.shape\n",
    "\n",
    "    Xaug = np.empty( (nd, ni+1)) # nd x (nf+1)\n",
    "    Xaug[:,1:] = iin\n",
    "    Xaug[:,0] = np.ones(nd)\n",
    "    z = np.dot(Xaug,v.T)\n",
    "    y = sigmoid(z)\n",
    "\n",
    "    res = np.zeros((ni+1,nh))\n",
    "\n",
    "    y1my = (y*(1-y))  # nd x no\n",
    "    \n",
    "    dx = dJ_dx(xin, w, yij)\n",
    "\n",
    "    for d in range(nd):\n",
    "        for i in range(ni+1):\n",
    "            for j in range(nh):\n",
    "                res[i,j] += dx[d,j]*y1my[d,j]*Xaug[d,i]\n",
    "    return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a7c4b6d70d98efb2d5dcfdad2c9cc82d",
     "grade": true,
     "grade_id": "cell-11cdd9f9cca18aef",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "analytic = dJ_dvij(iin,v,w,yij)\n",
    "assert analytic.shape == (ni+1, nh)\n",
    "\n",
    "eps = 1e-7\n",
    "for i in range(ni+1):\n",
    "    for j in range(nh):\n",
    "        v2 = np.array(v)\n",
    "        v2[j,i] += eps \n",
    "        ndiff = (J(iin,v2,w,yij)-J(iin,v,w,yij))/eps\n",
    "        assert np.isclose(analytic[i,j], ndiff,rtol=1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "10b03a1128f8fb649c1c4417d39884fc",
     "grade": false,
     "grade_id": "cell-bbb63e338d047152",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Application\n",
    "\n",
    "Here is another set of Iris data. We will use our new implementation of a neural network to fit classifier to this data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "data_dic = datasets.load_iris()\n",
    "features = data_dic['data']\n",
    "targets = data_dic['target']\n",
    "\n",
    "c1 = features[targets==0]\n",
    "c2 = features[targets==1]\n",
    "c3 = features[targets==2]\n",
    "\n",
    "ind1, ind2 = 0,2\n",
    "plt.scatter(c1[:,ind1],c1[:,ind2], color='red', marker='s', alpha=0.5, label=\"Setosa\")\n",
    "plt.scatter(c2[:,ind1],c2[:,ind2], color='blue', marker='x', alpha=0.5, label=\"Versicolour\")\n",
    "plt.scatter(c3[:,ind1],c3[:,ind2], color='green', marker='o', alpha=0.5, label=\"Versicolour\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"sepal length [cm]\")\n",
    "plt.ylabel(\"sepal width [cm]\");\n",
    "plt.savefig('iris_3class.png')\n",
    "def subSample(nData):\n",
    "    X = np.empty((3*nData,2))\n",
    "    X[:nData] = c1[:nData,(ind1, ind2)]\n",
    "    X[nData:2*nData] = c2[:nData,(ind1, ind2)]\n",
    "    X[2*nData:] = c3[:nData,(ind1, ind2)]\n",
    "    Y = np.empty(3*nData)\n",
    "    Y[:nData] = np.zeros(nData)\n",
    "    Y[nData:2*nData] = np.ones(nData)\n",
    "    Y[2*nData:] = 2*np.ones(nData)\n",
    "    return X,Y\n",
    "\n",
    "X, Y = subSample(50)\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "35cddc0958d2778e5d5f549eaae9ab99",
     "grade": false,
     "grade_id": "cell-aefc88fa3d2c1adb",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "We will use a neural network with 1 hidden layer with 3 units to classify the three classes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ni = 2\n",
    "nh = 3\n",
    "no = 3\n",
    "nd = len(X)\n",
    "\n",
    "np.random.seed(1211)\n",
    "v = np.random.random(size=(nh,ni+1))\n",
    "w = np.random.random(size=(no,nh+1))\n",
    "\n",
    "enc = OneHotEncoder(sparse=False, categories='auto')\n",
    "yij=enc.fit_transform(Y[:,np.newaxis])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement gradient descent to train this network, i.e. update the weights according to the derivative of the loss function. Use 10000 steps with learning rate 0.05. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eta = 0.05\n",
    "\n",
    "for i in range(100):\n",
    "    x = forwardPass(X,v)\n",
    "    v -= dJ_dvij(X,v,w,yij).T*eta\n",
    "    w -= dJ_dwij(x,w,yij).T*eta\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code will display the result of the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X):\n",
    "    xs = forwardPass(X,v)\n",
    "    ys = forwardPass(xs,w)\n",
    "    probs = softmax(ys)\n",
    "    return np.argmax(probs,axis=1)\n",
    "\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "def regions3(X, y,colors,resolution=0.02, savename=None):\n",
    "    # setup marker generator and color map\n",
    "    markers = ('s', 'x', 'o', '^', 'v')\n",
    "    cmap = ListedColormap(colors[:len(np.unique(y))])\n",
    "    # plot the decision surface\n",
    "    x1_min, x1_max = X[:, 0].min() - 0.1, X[:, 0].max() + 0.1\n",
    "    x2_min, x2_max = X[:, 1].min() - 0.1, X[:, 1].max() + 0.1\n",
    "    xx1, xx2 = np.meshgrid(np.arange(x1_min, x1_max, resolution),\n",
    "                            np.arange(x2_min, x2_max, resolution))\n",
    "    Z = predict(np.array([xx1.ravel(), xx2.ravel()]).T)\n",
    "    Z = Z.reshape(xx1.shape)\n",
    "    plt.contourf(xx1, xx2, Z, alpha=0.1, cmap=cmap)\n",
    "    plt.xlim(xx1.min(), xx1.max())\n",
    "    plt.ylim(xx2.min(), xx2.max())\n",
    "    \n",
    "    c2 = X[y==2]\n",
    "    c1 = X[y==1]\n",
    "    c0 = X[y==0]\n",
    "    \n",
    "    xb,yb=c0[:,0],c0[:,1]\n",
    "    plt.scatter(xb,yb,color=colors[0],alpha=0.4)\n",
    "    xb,yb=c1[:,0],c1[:,1]\n",
    "    plt.scatter(xb,yb,color=colors[1],alpha=0.4)\n",
    "    xb,yb=c2[:,0],c2[:,1]\n",
    "    plt.scatter(xb,yb,color=colors[2],alpha=0.4)\n",
    "    if savename:\n",
    "        plt.savefig(savename)\n",
    "        \n",
    "regions3(X, Y,['red','blue','green'],resolution=0.01)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
