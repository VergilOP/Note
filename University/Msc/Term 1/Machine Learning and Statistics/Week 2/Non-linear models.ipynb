{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Non linear models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This creates the data we will play with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noisy_circles = datasets.make_circles(n_samples=100, factor=.5,\n",
    "                                      noise=.05, random_state=1)\n",
    "X = np.array(noisy_circles[0])\n",
    "y = noisy_circles[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X[:,0][y==1],X[:,1][y==1], color='r')\n",
    "plt.scatter(X[:,0][y==0],X[:,-1][y==0], color='b')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "40a6f9e4dcbf7e2c37b837caa8251cff",
     "grade": false,
     "grade_id": "cell-20813a8b79b5714e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Implement the function `augment` that takes as argument `X` a `nd`x`nf` dataset of `nd` data points with `nf` features. It should return an augmented dataset the contains a constant term, the original features and all second order combinations of features.\n",
    "The augmented matrix should look like this\n",
    "\n",
    "$$ \n",
    "\\left(\n",
    "\\begin{matrix}\n",
    "X_{11} & X_{12}  & \\cdots & X_{1n_f}\\\\\n",
    "X_{21} & \\\\\n",
    "\\vdots \\\\\n",
    "X_{n_d 1} & & & X_{n_d n_f}\n",
    "\\end{matrix}\n",
    "\\right)\n",
    "\\rightarrow\n",
    "\\left(\n",
    "\\begin{array}{c|cccc| cccc|ccc}\n",
    "%\\begin{matrix}\n",
    "1& X_{11} & X_{12}  & \\cdots & X_{1n_f} & X_{11}^2 & X_{12}^2 & \\cdots & X_{1n_f}^2  \n",
    "& X_{11}\\cdot X_{12} & \\cdots & X_{1 (n_f-1)}\\cdot X_{1 n_f} \\\\\n",
    "1& X_{21} & \\\\\n",
    "\\vdots \\\\\n",
    "1& X_{n_d 1} & & & X_{n_d n_f}\n",
    "& & & & & & \n",
    "& X_{n_d (n_f-1)}\\cdot X_{n_d n_f} \\\\\n",
    "%\\end{matrix}\n",
    "\\end{array}\n",
    "\\right)\n",
    "$$\n",
    "\n",
    "I.e. your new matrix should contain a vector of ones, then the orginal matrix, then the squared terms of the features, then products of the features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment(X):\n",
    "    nd, nf = X.shape\n",
    "\n",
    "    nnf = 1+ nf+ nf + (nf*(nf-1))//2\n",
    "    print(nnf,3*nf)\n",
    "    Xaug = np.empty( (nd, nnf ))\n",
    "    # fill first row with ones\n",
    "    index = 0\n",
    "    Xaug[:,index] = np.ones(nd) \n",
    "    index = index +1\n",
    "    # fill next nf elements (i.e the rows 1 to nf+1) with the old matrix\n",
    "    Xaug[:,index:nf+index] = X\n",
    "    index = index + nf\n",
    "    # fill next nf elements (i.e the rows nf+2 to 2nf+2) with the squares of the features of the old matrix\n",
    "    for ii in range(0,nf):\n",
    "        Xaug[:,index] = X[:,ii]*X[:,ii]\n",
    "        index = index + 1\n",
    "    # fill next nf*(nf-1)/2 elements with products of different features of the old matrix\n",
    "    for ii in range(0,nf-1):\n",
    "        for jj in range(ii+1,nf):\n",
    "            Xaug[:,index] = X[:,ii]*X[:,jj]\n",
    "            index = index + 1 \n",
    "    \n",
    "    return Xaug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xaug = augment(X)\n",
    "assert Xaug.shape == (100, 6)\n",
    "assert set(Xaug[0,:]) == set([-0.39910416635565776,\n",
    " -0.24312948020420502,\n",
    " 0.05911194414436692,\n",
    " 0.09703398851338364,\n",
    " 0.15928413560244456,\n",
    " 1.0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "fc7602f50b54341b8ad80065a08966bd",
     "grade": false,
     "grade_id": "cell-6bdc78384f0ffddd",
     "locked": true,
     "points": 4,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "Use a logistic regresssion model to fit to the augmented data. Create a plot to show that with the additional features we can separate the data. To make a contour plot, you can create a grid in the input feature space and calculate the prediction of each of the grid points which you can plot using the `contourf` function.\n",
    "\n",
    "To plot the selection boundary of the model, you can follow these steps:\n",
    "\n",
    "(i) Create a grid of the (two) input features in X, $x_1$ and $x_2$.  To do so, you can for instance combine two arrays using the `np.meshgrid` function.\n",
    "\n",
    "(ii) Use the `ravel` function to turn the arrays of the grid coordinates into two one-dimensional, i.e. a one-dimensional array for the $x_1$ coordinates and a one-dimensional array for the $x_2$ coordinate. Then, combine the `x1` and `x2` arrays to a matrix of $[x_1, x_2]$ pairs. \n",
    "\n",
    "(iii) Calculate the prediction $Z$ for the (augmented) grid matrix using the `predict` function of Logistic Regression. The function will return a one-dimensional array of the prediction for the data points. You can regroup this array into a two-dimensional one using the `reshape` function.\n",
    "\n",
    "(iv) Make a contour plot of the grid data using the `contourf` function.\n",
    "\n",
    "You might also find it useful, to look at the following example of a similar problem which goes through the steps of creating a grid:\n",
    "\n",
    "https://scikit-learn.org/stable/auto_examples/ensemble/plot_voting_decision_regions.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "lr = LogisticRegression(solver='lbfgs')\n",
    "\n",
    "# plot the data (copied from above)\n",
    "plt.scatter(X[:,0][y==1],X[:,1][y==1], color='r',alpha=0.4)\n",
    "plt.scatter(X[:,0][y==0],X[:,-1][y==0], color='b',alpha=0.4)\n",
    "\n",
    "# you can use this color map for your contour plot to use the same colors as for the data points\n",
    "# you need to put it in as an argument like this:\n",
    "# plt.contourf(... , alpha=0.1, cmap=cmap)\n",
    "from matplotlib.colors import ListedColormap\n",
    "colors = ['b','r']\n",
    "cmap = ListedColormap(colors)\n",
    "\n",
    "\n",
    "Xaug = augment(X)\n",
    "lr.fit(Xaug,y)\n",
    "\n",
    "# plot the decision boundary\n",
    "\n",
    "# create the grid\n",
    "x1_min, x1_max = X[:, 0].min() - 0.15, X[:, 0].max() + 0.15\n",
    "x2_min, x2_max = X[:, 1].min() - 0.15, X[:, 1].max() + 0.15\n",
    "x1, x2 = np.meshgrid(np.arange(x1_min, x1_max, 0.01),\n",
    "                     np.arange(x2_min, x2_max, 0.01))\n",
    "\n",
    "# Use ravel to turn the two-dimensional arrays into long one-dimensional ones\n",
    "xx1 = x1.ravel()\n",
    "xx2 = x2.ravel()\n",
    "# create a matrix of x1-x2 pairs\n",
    "Xtest = np.array([xx1, xx2]).T\n",
    "\n",
    "# Use the predict function on the augmented Xtest to get the prediction for the grid points\n",
    "Z = lr.predict(augment(Xtest))\n",
    "# reshape the prediction output; make 2-D arrays \n",
    "Z = Z.reshape(x1.shape)\n",
    "\n",
    "# plot the contour plot\n",
    "plt.contourf(x1, x2, Z, alpha=0.1, cmap=cmap)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a1b91baeb85b464a950e94bc5af2f6d9",
     "grade": false,
     "grade_id": "cell-decccfbec7e8a436",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Regularisation\n",
    "Use the training and validation set below to find the best values of $\\alpha$ to use in the ridge regression loss for a eighth-order polynomial model. Train your model on the training sample and use the score (or loss) of the test sample as a measure for the quality of the fit. \n",
    "\n",
    "You may find it useful to include the modules commented out below (Ridge, PolynomialFeatures, LinearRegression). To find out more about these modules and their functionality, please check out the sklearn website. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "def fn(x):\n",
    "    return 7-8*x-0.5*x**2+0.5*x**3\n",
    "  \n",
    "n_train = 100\n",
    "np.random.seed(1122)\n",
    "xs = np.linspace(0,5)\n",
    "rxs = 5* np.random.random(n_train)\n",
    "X1D = np.array([rxs]).T\n",
    "noise = np.random.normal(size = (n_train) )\n",
    "ys1D = fn(rxs)+noise\n",
    "\n",
    "# split into training and test sample\n",
    "X_train, X_test, y_train, y_test = train_test_split(X1D, ys1D, test_size=0.4, random_state=0)\n",
    "\n",
    "# this plots the full data sample and the function fn\n",
    "plt.plot(xs, fn(xs),'b--')\n",
    "plt.plot(rxs, ys1D,'ok')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y');\n",
    "plt.show()\n",
    "\n",
    "# polynomial features \n",
    "polynomial_features= PolynomialFeatures(degree=8)\n",
    "X_train = polynomial_features.fit_transform(X_train)\n",
    "\n",
    "# prediction for fine x value grid\n",
    "xval = np.arange(0,5.1,0.1)\n",
    "xval_res = polynomial_features.fit_transform(xval.reshape(-1,1))\n",
    "Xtest_res = polynomial_features.fit_transform(X_test)\n",
    "\n",
    "y_ridge_alpha = []\n",
    "#alphas=np.arange(0.,100.,0.01) # fine scan\n",
    "alphas=np.logspace(-4,4) # broad scan\n",
    "scores = []\n",
    "for al in alphas:\n",
    "    rr = Ridge(alpha=al)\n",
    "    rr.fit(X_train,y_train)\n",
    "    y_ridge_alpha.append(rr.predict(xval_res))\n",
    "    score = rr.score(Xtest_res, y_test)\n",
    "    #print(score)\n",
    "    scores.append([al,score])\n",
    "\n",
    "scores = np.array(scores)\n",
    "\n",
    "#plt.plot(scores[:,0],scores[:,1])\n",
    "#plt.xscale('log')\n",
    "#plt.show()\n",
    "max_pos = np.argmax(scores[:,1])\n",
    "print('Best result for alpha is', scores[max_pos,0], ' with R^2 ', scores[max_pos,1])\n",
    "\n",
    "# plotting\n",
    "plt.plot(rxs, ys1D,'ok')  \n",
    "\n",
    "mylabel = 'alpha = '+ str(scores[max_pos,0])\n",
    "plt.plot(xval, y_ridge_alpha[max_pos], color='r', label=mylabel)\n",
    "\n",
    "plt.legend()\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.show()\n",
    "\n",
    "def bestAlpha():\n",
    "    best_alpha = 0 # enter your best value for alpha to one signifcant digit here\n",
    "    best_alpha = scores[max_pos,0]\n",
    "    return best_alpha\n",
    "\n",
    "print('The best value of alpha is ', bestAlpha())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell is used for automatic grading. Please do not delete it. \n",
    "assert bestAlpha() > 0.03\n",
    "assert bestAlpha() < 0.05\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "158e9a0ad36a4bf73262c716cc758849",
     "grade": false,
     "grade_id": "cell-150ea6096286c89f",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "Make a plot of the score as a funtion of $\\alpha$. Use a logarithmic scale for the x-axis and display the range $\\alpha = [10^{-4}, \\, 10^{4}]$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot alpha vs the score here\n",
    "plt.plot(scores[:,0],scores[:,1],label='score')\n",
    "plt.xscale('log')\n",
    "plt.xlabel('alpha')\n",
    "plt.ylabel('score')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
