# Summary - Machine Learning and Statistics

## Lecture 1

Errors in the physical sciences, Random errors in measurements, Uncertainties and probabilities   
ç‰©ç†ç§‘å­¦ä¸­çš„è¯¯å·®ã€æµ‹é‡ä¸­çš„éšæœºè¯¯å·®ã€ä¸ç¡®å®šæ€§å’Œæ¦‚ç‡

### precision & Accurate

precision: the spread of results is "**small**"  
ç²¾ç¡®æ€§: æµ‹é‡ç»“æœçš„åˆ†æ•£ç¨‹åº¦è¾ƒå°ï¼Œæ•°æ®ç‚¹é›†ä¸­åœ¨ä¸€èµ·

Accurate: result is in agreement with "**accepted**" value  
å‡†ç¡®æ€§: æµ‹é‡ç»“æœä¸â€œå…¬è®¤â€å€¼çš„æ¥è¿‘ç¨‹åº¦

![](./imgs/Picture1.jpg)

### Mean, Standard deviation and Standard error

- **Mean**: where is the measurement centred  
  å‡å€¼: æµ‹é‡ç»“æœçš„ä¸­å¿ƒä½ç½®
    $$
        \overline{x} = \frac{1}{N}(x_1 + x_2 + \dots + x_n) = \frac{1}{N}\sum\limits^N_{i=1}x_i
    $$
- **Standard deviation**: width of the distribution  
  æ ‡å‡†å·®: åˆ†å¸ƒçš„å®½åº¦ï¼Œè¡¨ç¤ºæ•°æ®çš„ç¦»æ•£ç¨‹åº¦
    $$
        \sigma_{n-1} = \sqrt{\frac{(d^2_1 + d^2_2 + \dots + d^2_N)}{N - 1}} = \sqrt{\frac{1}{N - 1}\sum\limits^N_{i=1}d^2_i}
    $$
    where $d_i = x_i - \overline{x}$
    > - the standard deviation is independent of N  
    >   æ ‡å‡†å·®ä¸å—æ ·æœ¬é‡ ğ‘ çš„å½±å“, å› ä¸ºå®ƒåªå…³å¿ƒæ•°æ®çš„åˆ†æ•£ç¨‹åº¦
- **Standard error**: uncertainty in the location of the centre, $\alpha$  
  æ ‡å‡†è¯¯å·®: å‡å€¼ä½ç½®çš„ä¸ç¡®å®šæ€§ã€‚
    $$
        \alpha = \frac{\sigma_{N-1}}{\sqrt{N}}
    $$
    We should quote our finding as $\overline{x} \plusmn \alpha$
    > - the standard error improves with N  
    >   æ ‡å‡†è¯¯å·®éšæ ·æœ¬é‡ ğ‘ çš„å¢åŠ è€Œå‡å°, æ„å‘³ç€éšç€æ”¶é›†æ›´å¤šçš„æ•°æ®, å‡å€¼ä¼°è®¡ä¼šæ›´åŠ ç²¾ç¡®

- **Gaussian or Normal Distribution**  
  æ­£æ€åˆ†å¸ƒ
    $$
        f(x) = \frac{1}{\sigma\sqrt{2\pi}}\exp[-\frac{(x-\overline{x})^2}{2\sigma^2}]
    $$
    > Facts: peak centred around mean, symmetric about mean,area under curve equals 1(normalised)  
    > äº‹å®ï¼šå³°å€¼ä»¥å¹³å‡å€¼ä¸ºä¸­å¿ƒï¼Œå…³äºå¹³å‡å€¼å¯¹ç§°ï¼Œæ›²çº¿ä¸‹é¢ç§¯ç­‰äº 1ï¼ˆæ ‡å‡†åŒ–ï¼‰

- The error in the error  
  è¯¯å·®ä¸­çš„è¯¯å·®
    $$
        \alpha^{\plusmn} = \frac{1}{\sqrt{2N-2}}
    $$

- The five golden rules for reporting results  
  æŠ¥å‘Šç»“æœçš„äº”æ¡é»„é‡‘æ³•åˆ™
  1. The best estimate for a parameter is the mean  
    å‚æ•°çš„æœ€ä½³ä¼°è®¡æ˜¯å‡å€¼
  2. The error is the standard error in the mean   
    è¯¯å·®æ˜¯å‡å€¼çš„æ ‡å‡†è¯¯å·®
  3. Round up the error to the appropriate number of significant figures  
    å°†è¯¯å·®å››èˆäº”å…¥åˆ°é€‚å½“çš„æœ‰æ•ˆæ•°å­—
  4. Match the number of decimal places in the mean to the standard error  
    å°†å‡å€¼çš„å°æ•°ä½æ•°ä¸æ ‡å‡†è¯¯å·®åŒ¹é…
  5. Include units  
    åŒ…å«å•ä½

### Confidence level

- We need to evaluate the **error function** of the Gaussian distribution(G)
    $$
        Erf(x_1;\overline{x}, \sigma) = \int^{x_1}_{-\infty}G(x;\overline{x},\sigma)
    $$
    $$
       f(x) = \frac{1}{\sqrt{2\pi}} e^{-x^2 / 2}
    $$
    $$
        \text{CDF}(x) = \frac{1}{2} \left[ 1 + \text{erf}\left( \frac{x}{\sqrt{2}} \right) \right]
    $$
    $$
         P(-\sigma \leq X \leq \sigma) = \text{erf}\left( \frac{\sigma}{\sqrt{2}} \right)
    $$
    The standard deviation is thus used to define a confidence level on the data.  
    å› æ­¤ï¼Œæ ‡å‡†å·®ç”¨äºå®šä¹‰æ•°æ®çš„ç½®ä¿¡åº¦ã€‚
    $$
        \overline{x} \pm ? \frac{\sigma}{\sqrt{N}}
    $$

- The standard deviation is thus used to define a **confidence level** on the data  
  ä½¿ç”¨æ ‡å‡†å·®å®šä¹‰æ•°æ®çš„ç½®ä¿¡æ°´å¹³
  - Up to 1 standard error it is in **excellent agreement**
  - Between 1 and 2: **reasonable agreement**
  - More than 3 standard errors: **disagreement**

### Poisson distribution

- Events are rare  
  äº‹ä»¶ç¨€å°‘ã€‚
- Events are independent  
  äº‹ä»¶ç‹¬ç«‹ã€‚
- Tha average rate does not change with time  
  å¹³å‡å‘ç”Ÿç‡ä¸éšæ—¶é—´å˜åŒ–ã€‚
  $$
    P(N;\overline{N}) = \frac{\exp(-\overline{N})\overline{N}^N}{N!}
  $$
  > Mean = $\overline{N}$  
  > Standard deviation = $\sqrt{\overline{N}}$

### Chauvenet's Criterion 

**Chauvenet's Criterion** æ˜¯ä¸€ç§ç»Ÿè®¡å­¦æ–¹æ³•, ç”¨äºæ£€æµ‹å’Œåˆ¤æ–­æ•°æ®é›†ä¸­æ˜¯å¦å­˜åœ¨ç¦»ç¾¤å€¼(outliers)ã€‚ç¦»ç¾¤å€¼æ˜¯æŒ‡ä¸å…¶ä»–æ•°æ®ç‚¹åå·®è¾ƒå¤§çš„æ•°æ®ç‚¹ã€‚Chauvenet çš„å‡†åˆ™åŸºäºæ ‡å‡†æ­£æ€åˆ†å¸ƒ, æä¾›äº†ä¸€ç§æ–¹æ³•æ¥ç¡®å®šä¸€ä¸ªæ•°æ®ç‚¹æ˜¯å¦ä¸æ•°æ®é›†çš„å…¶ä»–éƒ¨åˆ†æ˜¾è‘—ä¸åŒã€‚

1. **è®¡ç®—æ•°æ®çš„å‡å€¼(mean, $\mu$)å’Œæ ‡å‡†å·®(standard deviation, $\sigma$)**ã€‚

2. **è®¡ç®—æ¯ä¸ªæ•°æ®ç‚¹ä¸å‡å€¼çš„åå·®**ï¼šä½¿ç”¨ä»¥ä¸‹å…¬å¼è®¡ç®—æ¯ä¸ªæ•°æ®ç‚¹ä¸å‡å€¼çš„æ ‡å‡†åŒ–å·®å€¼(å³ Z å€¼)ï¼š
   $$
   Z = \frac{|x_i - \mu|}{\sigma}
   $$
   å…¶ä¸­ $x_i$ æ˜¯ç¬¬ $i$ ä¸ªæ•°æ®ç‚¹, $\mu$ æ˜¯å‡å€¼, $\sigma$ æ˜¯æ ‡å‡†å·®ã€‚

3. **è®¡ç®—ç¦»ç¾¤å€¼çš„æ¦‚ç‡**ï¼šåˆ©ç”¨æ­£æ€åˆ†å¸ƒ, Z å€¼ä»£è¡¨çš„æ˜¯æ•°æ®ç‚¹ä¸å‡å€¼çš„åå·®ç¨‹åº¦ã€‚å¯¹äºæ­£æ€åˆ†å¸ƒ, è®¡ç®—è¯¥æ•°æ®ç‚¹çš„ç´¯è®¡æ¦‚ç‡ã€‚è¿™ä¸ªæ¦‚ç‡è¡¨ç¤ºæ•°æ®ç‚¹åœ¨å¤šå¤§ç¨‹åº¦ä¸Šå¯ä»¥è¢«è§†ä¸ºå¼‚å¸¸å€¼ã€‚

4. **åˆ¤æ–­æ•°æ®ç‚¹æ˜¯å¦ä¸ºç¦»ç¾¤å€¼**ï¼šæ ¹æ®æ•°æ®ç‚¹çš„æ•°é‡ $N$, Chauvenet's Criterion æä¾›äº†ä¸€ä¸ªé—¨æ§›ã€‚å¦‚æœä¸€ä¸ªæ•°æ®ç‚¹çš„æ¦‚ç‡ä½äºï¼š
   $$
   P = \frac{1}{2N}
   $$
   åˆ™è¯¥æ•°æ®ç‚¹å¯ä»¥è¢«è§†ä¸ºç¦»ç¾¤å€¼å¹¶è¢«æ‹’ç»(REJECT), å¦åˆ™æ¥å—(ACCEPT)ã€‚

## Lecture 2

Error propagation, Data visualization and reduction  
è¯¯å·®ä¼ æ’­ã€æ•°æ®å¯è§†åŒ–å’Œç¼©å‡

### Single-variable functions å•å˜é‡è¯¯å·®ä¼ æ’­

- Functional approach å‡½æ•°æ³•
    $$
        \alpha_Z = |f(\overline{A} + \alpha_A) - f(\overline{A})|
    $$

- Calculus-based approach å¾®ç§¯åˆ†æ³•
    $$
        \alpha_Z = |\frac{dZ}{dA}|\alpha_A
    $$

### Multi-variable functions å¤šå˜é‡å‡½æ•°çš„è¯¯å·®ä¼ æ’­

- Functional approach å‡½æ•°æ³•
  $$
    (\alpha_Z)^2 = (\alpha^A_Z)^2 + (\alpha^B_Z)^2 + (\alpha^C_Z)^2 + ...
  $$
  - Change in Z when A is varied and B is constant  
      $$
        \alpha^A_Z = f(\overline{A} + \alpha_A, \overline{B}) - f(\overline{A}, \overline{B})
      $$
  - Change in Z when B is varied and A is constant 
      $$
        \alpha^B_Z = f(\overline{A}, \overline{B} + \alpha_B) - f(\overline{A}, \overline{B})
      $$

- Calculus-based approach å¾®ç§¯åˆ†æ³•
  $$
    (\alpha_Z)^2 = (\frac{\partial Z}{\partial A})^2(\alpha_A)^2 + (\frac{\partial Z}{\partial B})^2(\alpha_B)^2 + (\frac{\partial Z}{\partial C})^2(\alpha_C)^2 + ...
  $$

### Least Squares Method æœ€å°äºŒä¹˜æ³•

- æ®‹å·®(residuals) = å®é™…å€¼âˆ’æ¨¡å‹é¢„æµ‹å€¼
$$
    R_i = y_i - y(x_i)
$$

- Goodness-of-fit Parameter æ‹Ÿåˆä¼˜åº¦å‚æ•°
  $$
      \chi^2 = \sum\limits_i\frac{(y_i - y(x_i))^2}{\alpha^2_i}
  $$
  > The probability of obtaining our measurement values from the best-fit line is maximised when Ï‡2 is minimised  
  > å½“ Ï‡2 æœ€å°åŒ–æ—¶ï¼Œä»æœ€ä½³æ‹Ÿåˆçº¿è·å¾—æµ‹é‡å€¼çš„æ¦‚ç‡æœ€å¤§åŒ–
  - $\chi^2$ for data with Poisson errors
    $$
        X^2 = \sum\limits_i\frac{(O_i-E_i)^2}{E_i}
    $$
    > i: number of counts(bins)  
    > $O_i$: observed number of occurrences(in the i-th bin)  
    > $E_j$: expected number of occurrences(in the i-th bin)

    > If we have a goot fit, $\alpha_i = \sqrt{E_i} \approx \sqrt{O_i}$

### Minimisation æœ€å°åŒ–

The best values for **slope** and **intercept** are those that minimise the squares of the differences summed for all data points  
æ–œç‡å’Œæˆªè·çš„æœ€ä½³å€¼æ˜¯æœ€å°åŒ–æ‰€æœ‰æ•°æ®ç‚¹ä¹‹å·®çš„å¹³æ–¹å’Œ

$$
    S = \sum\limits_i(y_i - mx_i - c)^2 
$$

æˆªè· ğ‘ çš„å…¬å¼
$$
    c = \frac{\sum_ix^2_i\sum_iy_i - \sum_ix_i\sum_ix_iy_i}{\Delta}, \alpha_c = \alpha_{CU}\sqrt{\frac{\sum_ix^2_i}{\Delta}}
$$

æ–œç‡ ğ‘š çš„å…¬å¼ï¼š
$$
    m = \frac{N\sum_ix_iy_i - \sum_ix_i\sum_iy_i}{\Delta}, \alpha_m = \alpha_{CU}\sqrt{\frac{N}{\Delta}}
$$

> å…±åŒä¸ç¡®å®šæ€§:    
> Common uncertainty: $\alpha_{CU} = \sqrt{\frac{1}{N - 2}\sum_i(y_i - mx_i - c)^2}$

æ€»ä¸ç¡®å®šæ€§å‚æ•° Î” 
$$
\Delta = N\sum_ix^2_i - (\sum_i x_i)^2
$$

Need to perform a **weighted** least-squares fit to take them into account  
éœ€è¦æ‰§è¡Œ**åŠ æƒ**æœ€å°äºŒä¹˜æ³•æ‰èƒ½å°†å®ƒä»¬è€ƒè™‘åœ¨å†…
$$
    R_i = \frac{y_i - y(x_i)}{\alpha_i}
$$

## Lecture 3

Least-squares fitting, Computer minimization and error matrix  
æœ€å°äºŒä¹˜æ‹Ÿåˆã€è®¡ç®—æœºæœ€å°åŒ–å’Œè¯¯å·®çŸ©é˜µ

### Least-squares fit to an arbitrary function

- Procedure:
  1. for each value of the independent variable, $x_i$, calculate $y(x_i)$ using an estimated set of values for the parameters  
    è®¡ç®—ä¼°è®¡å‚æ•°é›†ä¸‹çš„å‡½æ•°å€¼
  2. for each value of the independent variables, calculate the square of the normalised residual, $[^{(y_i-y(x_i))}/_{\alpha i}]^2$  
    è®¡ç®—å½’ä¸€åŒ–æ®‹å·®çš„å¹³æ–¹
  3. calculate $\chi^2$(sum the square of the normalised residuals)  
    è®¡ç®—$\chi^2$
  4. minimise $\chi^2$ by optimising the fit parameters  
    ä¼˜åŒ–æ‹Ÿåˆå‚æ•°

- Fit data with a double-peak model:  
  ![](./imgs/double_peak%20model.png)
  - residuals randomly distributed -> good fit  
    åŒå³°æ¨¡å‹æ‹Ÿåˆ X å°„çº¿è¡å°„æ•°æ®ï¼šæ®‹å·®éšæœºåˆ†å¸ƒï¼Œè¡¨ç¤ºæ‹Ÿåˆè‰¯å¥½ã€‚
- Fit data with a single-peak model:  
  ![](./imgs/single_peak%20model.png)
  - residuals show structure -> bad fit  
    å•å³°æ¨¡å‹ï¼šæ®‹å·®æœ‰ç»“æ„æ€§ï¼Œè¡¨ç¤ºæ‹Ÿåˆè¾ƒå·®ã€‚

- To better visualise structure in residuals: make a lag plot -> normalised residuals $R_i$ vs lagged residuals $R_{i-k}$(k is usually 1)
- Good fit  
  ![](./imgs/good_fit%20lag_plot.png)
  - Random parttern
  - at least 91% of data points in a 2D box of $\pm2$ limits
  - éšæœºæ¨¡å¼ -> æ‹Ÿåˆè‰¯å¥½ã€‚
- Bad fit:  
  ![](./imgs/bad_fit%20lag_plot.png)
  - non-random pattern
  - < 91% of data points in a 2D box of $\pm2$ limits
  - ééšæœºæ¨¡å¼ -> æ‹Ÿåˆä¸è‰¯ã€‚

- Durbin-Watson
  $$
    D=\frac{\sum_{i=2}^N(R_i-R_{i-1})^2}{\sum_{i=1}^NR_i^2}
  $$
  > $R_i$è¡¨ç¤ºç¬¬$i$ä¸ªæ ‡å‡†å·®æ®‹å·®  
  > åˆ†å­éƒ¨åˆ†æ˜¯æ»åæ®‹å·®çš„å¹³æ–¹å’Œ, è¡¨ç¤ºç›¸é‚»æ®‹å·®ä¹‹é—´çš„å˜åŒ–  
  > åˆ†æ¯éƒ¨åˆ†æ˜¯æ‰€æœ‰æ®‹å·®çš„å¹³æ–¹å’Œï¼Œè¡¨ç¤ºæ®‹å·®çš„æ€»å˜å¼‚

What does it mean?
- 0 < D < 4
- D = 0: systematically correlated residuals ç³»ç»Ÿæ€§ç›¸å…³æ®‹å·®
- D = 2: randomly distributed residuals with Gaussian distribution æ®‹å·®éšæœºåˆ†å¸ƒï¼Œç¬¦åˆé«˜æ–¯åˆ†å¸ƒ
- D = 4: systematically anticorrelated residuals ç³»ç»Ÿæ€§åç›¸å…³æ®‹å·®

### the error surface
- å¯¹äºç›´çº¿æ‹Ÿåˆ, $\chi^2$çš„å˜åŒ–éšæ–œç‡çš„å˜åŒ–å‘ˆç°å‡ºç®€å•çš„æ›²çº¿
- å¯¹æ›´å¤æ‚çš„å‡½æ•°, $\chi^2$éšå‚æ•°å˜åŒ–å½¢æˆçš„è¯¯å·®æ›²é¢å¯èƒ½å…·æœ‰å¤šä¸ªå±€éƒ¨æœ€å°å€¼
- é€šè¿‡æ³°å‹’å±•å¼€æ¥è¿‘ä¼¼$\chi^2$åœ¨æœ€ä¼˜è§£é™„è¿‘çš„è¡Œä¸º, ä»è€Œè¡¨è¾¾æ ‡å‡†è¯¯å·®:
    $$
        \alpha_j = \sqrt{\frac{2}{(\frac{\partial^2\chi^2}{\partial \alpha^2_j})}}
    $$

### Curvature Matrix and Error Matrix

- For a straight line fit, the $\chi^2$ surface is perfectly parabolic with respect to both variabels, such that:
  - There is only 1 minimum
  - Finding the minimum is easy
  - The curvature matrix has analytic results that let you calculate errors easily$A=\begin{bmatrix}A_{cc}&&A_{cm}\\A_{mc}&&A_{mm}\end{bmatrix}$
    $$
        A_{cc}=\sum_{i}\frac{1}{\alpha_{i}^{2}}\quad A_{cm}=A_{mc}=\sum_{i}\frac{x_{i}}{\alpha_{i}^{2}}\quad  A_{mm}=\sum_{i}\frac{x_{i}^{2}}{\alpha_{i}^{2}}
    $$
    $$
        [C] = [A]^{-1}
    $$
    And finally: the uncertainty $a_j$ of the parameter $a_i$ is $\alpha_j = \sqrt{C_{jj}}$, so $a_j + a_j = a_j \pm \sqrt{C_{jj}}$

  - Without correlation: $\alpha_{V}^{2}=f^{2}\alpha_{m}^{2} + \alpha_{c}^{2}=f^{2}C_{22}+C_{11}$
  - With correlation: $\alpha_{V}^{2}=f^{2}C_{22}+C_{11}+2fC_{12}$

## Lecture 4

### Degree of freedom

- è‹¥æˆ‘ä»¬æµ‹é‡äº† $N$ ä¸ªç‹¬ç«‹æ•°æ®ç‚¹å¹¶ç”¨ $\mathcal{N}$ ä¸ªå‚æ•°æ‹Ÿåˆæ¨¡å‹ï¼Œè‡ªç”±åº¦ $v$ å®šä¹‰ä¸º
  $$
    v = N - \mathcal{N}
  $$
- è‡ªç”±åº¦çš„æ¦‚å¿µæ„å‘³ç€æˆ‘ä»¬åœ¨è®¡ç®—æ ‡å‡†åå·®æ—¶éœ€è¦å‡å°‘ä¸€ä¸ªè‡ªç”±å˜é‡ï¼Œå› ä¸ºæœ€åä¸€ä¸ªåå·®æ˜¯ç”±ä¹‹å‰çš„åå·®å†³å®šçš„

### The $\chi^2$ probability distribution function

$\chi^2$ is a random variable(it depends on a variety of input parameters, e.g., the input data, the chosen model, the uncertainties, etc)  
$\chi^2$ æ˜¯ä¸€ä¸ªéšæœºå˜é‡ï¼Œå®ƒä¾èµ–äºè®¸å¤šè¾“å…¥å‚æ•°ï¼Œå¦‚è¾“å…¥æ•°æ®ã€é€‰å®šæ¨¡å‹å’Œä¸ç¡®å®šæ€§ç­‰  
-> it has a normalised probability distribution function(PDF)  
å…¶å…·æœ‰ä¸€ä¸ªå½’ä¸€åŒ–çš„æ¦‚ç‡åˆ†å¸ƒå‡½æ•° (PDF)
$$
  X(\chi^2; v) = \frac{(\chi^2)^{(\frac{v}{2}-1)}\exp[-\chi^2 / 2]}{2^{v/2}\Gamma(v/2)}
$$

Facts:
- $X(\chi^2, v)$ is asymmetric(median $\neq$ mode) æ˜¯éå¯¹ç§°çš„
- It has a mean(expectation value) of $v$, and a standard deviation, $\sigma_{\chi^2} = \sqrt{2v}$
- P of obtaining a value of $\chi^2$ between $\chi^2_{min}$ and $\infin$ is the cumulative probability function
  $$
    P\left(\chi_{\min}^2\leq\chi^2\leq\infty;v\right)=\int\limits_{\chi_{\min}^2}^\infty X\left(\chi^2;v\right) \mathrm{d}\chi^2
  $$

![](./imgs/46%25%20P.png)

If $\chi^2_{\min} \gg v$ $\rarr$ probability is small
- is null hypothesis wrong?
- are uncertainties incorrect?

![](./imgs/4%25%20P.png)

If $\chi^2_{\min} < v$ $\rarr$ probability tends towards 1
- Not an indication of improved fit
- Likely, uncertainties are overestimated, which results in unrealistically small $\chi^2$ values

There's a fast way of telling if a null hypothesis should be rejected: reduced $\chi^2$  
ä¸€ä¸ªå¿«é€Ÿåˆ¤æ–­é›¶å‡è®¾æ˜¯å¦åº”è¢«æ‹’ç»çš„æ–¹æ³•æ˜¯ ç®€åŒ–çš„ $\chi^2$
$$
  \chi^2_v = \chi^2_{\min} / v
$$

### What makes a good fit?

- Two-thirds of the data points should be within one standard error of the theoretical model.  
  çº¦ 2/3 çš„æ•°æ®ç‚¹åœ¨ç†è®ºæ¨¡å‹çš„ä¸€å€æ ‡å‡†è¯¯å·®å†…ã€‚
- $\chi^2_{\nu} \approx 1$.
- $P(\chi^2_{\text{min}}; \nu) \approx 0.5$.
- A visual inspection of the residuals shows no structure.  
  æ®‹å·®çš„å¯è§†åŒ–æ£€æŸ¥æ— ç»“æ„æ€§ã€‚
- A test of the autocorrelation of the normalized residuals yields $D \approx 2$.  
  å½’ä¸€åŒ–æ®‹å·®çš„è‡ªç›¸å…³æµ‹è¯• $D \approx 2$ã€‚
- The histogram of the normalized residuals should be Gaussian, centered on zero, with a standard deviation of 1.  
  å½’ä¸€åŒ–æ®‹å·®çš„ç›´æ–¹å›¾å‘ˆç°ä¸ºä»¥ 0 ä¸ºä¸­å¿ƒçš„é«˜æ–¯åˆ†å¸ƒï¼Œæ ‡å‡†å·®ä¸º 1ã€‚

### Anscombe's quartet

4 data sets:
- Described by same statistic  
  ä½¿ç”¨ç›¸åŒçš„ç»Ÿè®¡æè¿°ã€‚
- Very different distributions  
  ä½†å…·æœ‰éå¸¸ä¸åŒçš„åˆ†å¸ƒå½¢æ€ã€‚
- Illustrate the danger of not inspecting your plots  
  å±•ç¤ºäº†å¦‚æœåªä¾èµ–ç»Ÿè®¡æè¿°è€Œä¸æŸ¥çœ‹å›¾å½¢å¯èƒ½ä¼šå¸¦æ¥çš„å±é™©ã€‚

### Benford's Law(aka the first digit law)

The "first digit phenomenon" can be described by the following probability:

$$
P(d) = \log_{10}\left(1 + \frac{1}{d}\right)
$$

"In many real-life numerical sets of data, the leading digit is likely to be small" (wiki).

- 1 appears about 30% of the time.
- 9 occurs about 5% of the time.
- Works best if data spans many orders of magnitude.

è¿™è¡¨æ˜ï¼Œæ•°æ®ä¸­çš„å‰å¯¼æ•°å­—é€šå¸¸æ˜¯å°æ•°å­—ï¼Œå°¤å…¶æ˜¯å½“æ•°æ®æ¶‰åŠä¸åŒæ•°é‡çº§æ—¶