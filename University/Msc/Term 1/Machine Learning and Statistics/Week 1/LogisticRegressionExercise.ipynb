{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3c1064fd6a8779b8cc95e8a28c1a0be3",
     "grade": false,
     "grade_id": "cell-f1c48667aa374cfa",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement a function that returns the linear prediction for the log of the odds ratio. It should include the bias term. `X` is a $n_d\\times n_f$ array of data: $n_d$ samples with each $n_f$ features. `w` is a $n_f+1$ vector of parameters. We want the function to return a $n_d$ dimanesional vector with each component $z^{(i)}$ satisfying \n",
    "\n",
    "$$ z^{(i)} = w_0 +\\sum_{j=1}^{n_f} w_j x_j^{(i)}\\;.$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def z(X,w):\n",
    "    nd, nf = X.shape\n",
    "    Xaug = np.empty((nd, nf + 1))\n",
    "    Xaug[:,0] = np.ones( (nd))\n",
    "    Xaug[:,1:] = X\n",
    "                \n",
    "    return np.dot(Xaug,w)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "fab888a69f76013b9b96f392664a1193",
     "grade": false,
     "grade_id": "cell-e7047163c4ea090b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "This tests your code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtest = np.array([[1,2],[2,4],[3,6]])\n",
    "ytest = np.array([0,0,1])\n",
    "wtest = np.array([7,0,0])\n",
    "assert isinstance(z(Xtest,wtest), (list, tuple, np.ndarray)), 'Return value should be an array/list/tuple'\n",
    "assert (z(Xtest,wtest) == np.array([7,7,7])).all()\n",
    "wtest = np.array([7,-2,1])\n",
    "assert (z(Xtest,wtest) == np.array([7,7,7])).all()\n",
    "wtest = np.array([7,10,1])\n",
    "assert (z(Xtest,wtest) == np.array([19,31,43])).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e4d893ef878564c1140d3842c2cd20d3",
     "grade": false,
     "grade_id": "cell-7613c07d333d033c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Implement the sigmoid function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def phi(z):\n",
    "    return 1.0/(1+np.exp(-z))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7df53d89933a8cc2c12ce569808a58a6",
     "grade": false,
     "grade_id": "cell-2f6f03c177a90f4f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Test your answer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert phi(0)==0.5\n",
    "assert np.isclose(phi(np.linspace(-2,2,10)),np.array([0.11920292, 0.17428532, 0.2476638 , 0.33924363, 0.44467194,\n",
    "       0.55532806, 0.66075637, 0.7523362 , 0.82571468, 0.88079708])).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "606f0bc0d53bbf8164c8940168efa1e8",
     "grade": false,
     "grade_id": "cell-269415e0db8d4794",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Implement a function that returns the prediction for data $X$ and parameter vector $w$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X,w):\n",
    "    Z = z(X,w)\n",
    "    return phi(Z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "bfb65dd3c93c9a95e9883bf1d081b286",
     "grade": false,
     "grade_id": "cell-32fb7a786bc5c62f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Test your implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtest = np.array([[1,-3],[2,-4],[3,6]])\n",
    "wtest = np.array([0.01,0.01,0.01])\n",
    "assert isinstance(predict(Xtest,wtest), (list, tuple, np.ndarray)), 'Return value for this test should be an array/list/tuple'\n",
    "assert isinstance(predict(Xtest,wtest)[0], float), 'Return value should be a list of floats.'\n",
    "assert np.isclose(predict(Xtest,wtest),np.array([0.49750002, 0.49750002, 0.52497919])).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the cross entropy loss function for data $X$, label $y$ and parameter $w$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def J(X,y,w):\n",
    "    pred = predict(X, w)\n",
    "    eps = 1e-10\n",
    "    pred = np.clip(pred,eps, 1-eps)\n",
    "    return - np.sum( y*np.log(pred) + (1-y)*np.log(1-pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "25bd42bea87b50c77434f43e8e9ef649",
     "grade": false,
     "grade_id": "cell-acd14ddd6cfdf2f6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Test your implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtest = np.array([[1,-3],[2,-4],[3,6]])\n",
    "ytest = np.array([0,0,1])\n",
    "wtest = np.array([0.01,0.01,0.01])\n",
    "assert isinstance(J(Xtest, ytest, wtest), float), 'Return value should be a float.'\n",
    "assert np.isclose(J(Xtest, ytest, wtest), 2.020716021089296)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9263f8cec7f1bb667d583e48197e8789",
     "grade": false,
     "grade_id": "cell-c5029c4b5f763683",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "For fitting the model we need gradient of the loss function. Implement it here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradJ(X,y,w):\n",
    "    nd, nf = X.shape\n",
    "    Xaug = np.empty((nd, nf + 1))\n",
    "    Xaug[:,0] = np.ones( (nd))\n",
    "    Xaug[:,1:] = X\n",
    "    return -  np.dot( (y-predict(X,w)),  Xaug)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f5200ba69b284b43f80ad2b04f9b211f",
     "grade": false,
     "grade_id": "cell-31deb0463572775a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Test your code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtest = np.array([[1,-3],[2,-4],[3,6]])\n",
    "ytest = np.array([0,0,1])\n",
    "wtest = np.array([0.01,0.01,0.01])\n",
    "\n",
    "assert isinstance(gradJ(Xtest,ytest,wtest), (list, tuple, np.ndarray)), 'Return value for this test should be an array/list/tuple'\n",
    "assert isinstance(gradJ(Xtest,ytest,wtest)[0], float), 'Return value should be a list of floats.'\n",
    "\n",
    "assert np.isclose(gradJ(Xtest,ytest,wtest), np.array([ 0.51997923,  0.06743762, -6.33262502])).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "bce941a29bc44d7316a4a70974b7b8bb",
     "grade": false,
     "grade_id": "cell-7a15b0691437c274",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "We can also test numerically that we got the gradient right:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delta = 0.00001\n",
    "Jp0 = J(Xtest,ytest,wtest+[delta,0,0])\n",
    "Jp1 = J(Xtest,ytest,wtest+[0,delta,0])\n",
    "Jp2 = J(Xtest,ytest,wtest+[0,0,delta])\n",
    "J0 = J(Xtest,ytest,wtest)\n",
    "g0 = (Jp0-J0)/delta\n",
    "g1 = (Jp1-J0)/delta\n",
    "g2 = (Jp2-J0)/delta\n",
    "assert np.isclose( gradJ(Xtest,ytest,wtest), [g0, g1, g2], rtol=1e-3).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "00c122d376c338da5c0638dd73765f48",
     "grade": false,
     "grade_id": "cell-70164198740c27a1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "We also need a function to calculate the score, that is the ratio of correctly predicted values to the total number of values predicted. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getScore(X,y,w):\n",
    "    p = (predict(X,w) > 0.5)\n",
    "    success = (p.astype(int) == y)\n",
    "    return np.count_nonzero( success )/X.shape[0]   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtest = np.array([[1,-3],[2,-4],[3,6]])\n",
    "ytest = np.array([0,0,1])\n",
    "wtest = np.array([0.01,0.01,0.01])\n",
    "assert getScore(Xtest, ytest, wtest) == 1.0\n",
    "ytest = np.array([0,1,1])\n",
    "assert getScore(Xtest, ytest, wtest) == 2.0/3.0\n",
    "ytest = np.array([1,1,1])\n",
    "assert getScore(Xtest, ytest, wtest) == 1.0/3.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning code\n",
    "\n",
    "Define the function `learn` that takes the data `X`, the target `y`, the learning rate `eta` and the number of steps `nsteps` as arguments and returns the three vectors:\n",
    " - the value of the parameters after each step (including before the first step)\n",
    " - the value of the loss at each step\n",
    " - the value of the score at each step\n",
    "\n",
    "Start from randomly initialized parameters in the range $[0, \\, 1]$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learn(X,y,eta, nsteps):\n",
    "    random.seed(1243)\n",
    "    w0 = [random.random(),random.random(),random.random()]\n",
    "    ws = np.empty( (nsteps+1,3) )\n",
    "    ws[0] = np.array(w0)\n",
    "    js = np.empty( nsteps+1 )\n",
    "    js[0] = J(X,y,ws[0])\n",
    "    scores = np.empty( nsteps+1 )\n",
    "    scores[0] = getScore(X,y,w0)\n",
    "    for i in range(nsteps):\n",
    "        g = gradJ(X,y,ws[i])\n",
    "        ws[i+1] = ws[i] - eta * g\n",
    "        js[i+1] = J(X,y,ws[i+1])\n",
    "        scores[i+1] = getScore(X,y,ws[i+1])  \n",
    "    return ws, js, scores\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function will show you plots for the learning of your algorithm if you provide the above mentionned vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makePlots(ws, js, scores):\n",
    "    plt.figure(figsize=(15,10))\n",
    "    plt.subplot(221)\n",
    "    plt.plot(ws[:,0])\n",
    "    plt.plot(ws[:,1])\n",
    "    plt.plot(ws[:,2])\n",
    "    plt.title(\"Parameters\")\n",
    "    \n",
    "    plt.subplot(222)\n",
    "    \n",
    "    plt.plot(js)\n",
    "    plt.yscale('log')\n",
    "    plt.title('loss')\n",
    "    \n",
    "    plt.subplot(223)\n",
    "    plt.plot(scores)\n",
    "    plt.title('scores')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cancer data\n",
    "\n",
    "We start we a dataset of benign and malignent cancer cases. We only look at two features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "cancer = datasets.load_breast_cancer()\n",
    "#only take two features\n",
    "X = cancer.data[:,:2]\n",
    "y = cancer.target\n",
    "malignent = cancer.data[y==1]\n",
    "benign = cancer.data[y==0]\n",
    "xb, yb = benign[:,0],benign[:,1]\n",
    "xm, ym = malignent[:,0],malignent[:,1]\n",
    "plt.scatter(xb,yb,color='g',alpha=0.2,label='benign')\n",
    "plt.scatter(xm,ym,color='r',alpha=0.2,label='malignent')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we normalise the features before learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xunscaled = np.array(X)\n",
    "av = np.average(X,axis=0)\n",
    "sc = np.std(X,axis=0)\n",
    "X = (X-av)/sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experiment with different learning rates and number of steps! Try without normalising the features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With a reasonable learning rate the parameter converge to a fixed value and the loss and score stablize. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ws, js, scores = learn(X, y, eta= 0.01, nsteps = 50)\n",
    "makePlots(ws, js, scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With a too large learning rate the solution is unstable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ws, js, scores = learn(X, y, eta= 0.1, nsteps = 50)\n",
    "makePlots(ws, js, scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With a too small learning rate it takes longer to get to the best value of the parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ws, js, scores = learn(X, y, eta= 0.001, nsteps = 50)\n",
    "makePlots(ws, js, scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Without normalising the inputs we don't get a stable result with the reasonable learning rate we used before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ws, js, scores = learn(Xunscaled, y, eta= 0.01, nsteps = 50)\n",
    "makePlots(ws, js, scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can get a reasonable convergence for a much smaller learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ws, js, scores = learn(Xunscaled, y, eta= 0.00001, nsteps = 500)\n",
    "makePlots(ws, js, scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem is that the initial weights are too big, they are of order 1 and our features of order ~20. So the arguments of the sigmoid function become very large. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iris dataset\n",
    "\n",
    "Here we use a different data set. This time the data is linearly separable. What is different? Is the decision boundary better than with the perceptron algorithm?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "data_dic = datasets.load_iris()\n",
    "features = data_dic['data']\n",
    "targets = data_dic['target']\n",
    "\n",
    "c1 = features[targets==0]\n",
    "c2 = features[targets==1]\n",
    "c3 = features[targets==2]\n",
    "\n",
    "\n",
    "def subSample(nData):\n",
    "    X = np.empty((2*nData,2))\n",
    "    X[:nData] = c1[:nData,:2]\n",
    "    X[nData:] = c2[:nData,:2]\n",
    "    Y = np.empty(2*nData)\n",
    "    Y[:nData] = np.ones(nData)\n",
    "    Y[nData:] = np.zeros(nData)\n",
    "    return X,Y\n",
    "\n",
    "nData = 30\n",
    "X, Y = subSample(nData)\n",
    "\n",
    "# plot\n",
    "\n",
    "X[Y==0]\n",
    "c1 = X[Y==1]\n",
    "c2 = X[Y==0]\n",
    "plt.scatter(c1[:,0],c1[:,1], color='red', marker='s', alpha=0.5, label=\"Setosa\")\n",
    "plt.scatter(c2[:,0],c2[:,1], color='blue', marker='x', alpha=0.5, label=\"Versicolour\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"sepal length [cm]\")\n",
    "plt.ylabel(\"sepal width [cm]\");\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experiment with different step numbers and learning rates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ws, js, scores = learn(X, Y, eta= 0.01, nsteps = 1000)\n",
    "makePlots(ws, js, scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Does it make a difference to normalise the features? Do the parameters stabilize to a given value? Can you explain why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the data is linearly separable and you have parameter $w$ that give a valid decision line, then scaling all parameters by a common factor will improve the loss. This means making harsher and harsher decision around the boundary. This means the model is less likely to generalise well. Adding a penalisation term to the loss would limit the size of the parameters to preven overfitting.    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
