# Note - Machine Learning and Statistics

## Lecture 1

### Precise & accuracy

- **Precise** measurement: the spread of results is "**small**"
- **Accurate** measurement: result is in agreement with "**accepted**" value

### Mean, standard deviation, standard error

- **Mean**: where is the measurement centred
    $$
        \overline{x} = \frac{1}{N}(x_1 + x_2 + \dots + x_n) = \frac{1}{N}\sum\limits^N_{i=1}x_i
    $$
- **Standard deviation**: width of the distribution
    $$
        \sigma_{n-1} = \sqrt{\frac{(d^2_1 + d^2_2 + \dots + d^2_N)}{N - 1}} = \sqrt{\frac{1}{N - 1}\sum\limits^N_{i=1}d^2_i}
    $$
    where $d_i = x_i - \overline{x}$
- **Standard error**: uncertainty in the location of the centre, $\alpha$
    $$
        \alpha = \frac{\sigma_{N-1}}{\sqrt{N}}
    $$
    We should quote our finding as $\overline{x} \plusmn \alpha$

> Consider an experiment with N number of data points collected:
> - the standard deviation is independent of N  
>   æ ‡å‡†å·®ä¸å—æ ·æœ¬é‡ ð‘ çš„å½±å“, å› ä¸ºå®ƒåªå…³å¿ƒæ•°æ®çš„åˆ†æ•£ç¨‹åº¦
> - the standard error improves with N  
>   æ ‡å‡†è¯¯å·®éšæ ·æœ¬é‡ ð‘ çš„å¢žåŠ è€Œå‡å°, æ„å‘³ç€éšç€æ”¶é›†æ›´å¤šçš„æ•°æ®, å‡å€¼ä¼°è®¡ä¼šæ›´åŠ ç²¾ç¡®

### Random errors, the normal distribution

- **Gaussian or Normal Distribution**
    $$
        f(x) = \frac{1}{\sigma\sqrt{2\pi}}\exp[-\frac{(x-\overline{x})^2}{2\sigma^2}]
    $$
    > Facts: peak centred around mean, symmetric about mean,area under curve equals 1(normalised)

- The error in the error
    $$
        \alpha^{\plusmn} = \text{error in the error} = \frac{1}{\sqrt{2N-2}}
    $$

> Bigger sample size:
> - lower error in the error
> - more confidence
> - can quote more significant figures
>
> Need $N=50$ for the error to be known to 10%
> Need $N > 10k$ for the error to be known below 1%

### Reporting results, Confidence limits and error bars

- The five golden rules for reporting results
  1. The best estimate for a parameter is the mean
  2. The error is the standard error in the mean
  3. Round up the error to the appropriate number of significant figures
  4. Match the number of decimal places in the mean to the standard error
  5. Include units

- What is the probability of the data to lie within some multiple of $\sigma$?â€
    - We need to evaluate the **error function** of the Gaussian distribution(G)
        $$
            Erf(x_1;\overline{x}, \sigma) = \int^{x_1}_{-\infty}G(x;\overline{x},\sigma)
        $$

        $$
           f(x) = \frac{1}{\sqrt{2\pi}} e^{-x^2 / 2}
        $$
        $$
            \text{CDF}(x) = \frac{1}{2} \left[ 1 + \text{erf}\left( \frac{x}{\sqrt{2}} \right) \right]
        $$
        $$
             P(-\sigma \leq X \leq \sigma) = \text{erf}\left( \frac{\sigma}{\sqrt{2}} \right)
        $$

- The standard deviation is thus used to define a **confidence level** on the data
- If your result and the accepted value differ by:
  - Up to 1 standard error it is in **excellent agreement**
  - Between 1 and 2: **reasonable agreement**
  - More than 3 standard errors: **disagreement**

### Poisson distribution

- The conditions under which a Poisson distribution holds are:
  - Events are rare
  - Events are independent
  - Tha average rate does not change with time
  $$
    P(N;\overline{N}) = \frac{\exp(-\overline{N})\overline{N}^N}{N!}
  $$
  > Mean = $\overline{N}$
  > Standard deviation = $\sqrt{\overline{N}}$

### Chauvenet's Criterion 

**Chauvenet's Criterion** æ˜¯ä¸€ç§ç»Ÿè®¡å­¦æ–¹æ³•, ç”¨äºŽæ£€æµ‹å’Œåˆ¤æ–­æ•°æ®é›†ä¸­æ˜¯å¦å­˜åœ¨ç¦»ç¾¤å€¼(outliers)ã€‚ç¦»ç¾¤å€¼æ˜¯æŒ‡ä¸Žå…¶ä»–æ•°æ®ç‚¹åå·®è¾ƒå¤§çš„æ•°æ®ç‚¹ã€‚Chauvenet çš„å‡†åˆ™åŸºäºŽæ ‡å‡†æ­£æ€åˆ†å¸ƒ, æä¾›äº†ä¸€ç§æ–¹æ³•æ¥ç¡®å®šä¸€ä¸ªæ•°æ®ç‚¹æ˜¯å¦ä¸Žæ•°æ®é›†çš„å…¶ä»–éƒ¨åˆ†æ˜¾è‘—ä¸åŒã€‚

### ä½¿ç”¨ Chauvenet's Criterion çš„æ­¥éª¤ï¼š

1. **è®¡ç®—æ•°æ®çš„å‡å€¼(mean, $\mu$)å’Œæ ‡å‡†å·®(standard deviation, $\sigma$)**ã€‚

2. **è®¡ç®—æ¯ä¸ªæ•°æ®ç‚¹ä¸Žå‡å€¼çš„åå·®**ï¼šä½¿ç”¨ä»¥ä¸‹å…¬å¼è®¡ç®—æ¯ä¸ªæ•°æ®ç‚¹ä¸Žå‡å€¼çš„æ ‡å‡†åŒ–å·®å€¼(å³ Z å€¼)ï¼š
   $$
   Z = \frac{|x_i - \mu|}{\sigma}
   $$
   å…¶ä¸­ $x_i$ æ˜¯ç¬¬ $i$ ä¸ªæ•°æ®ç‚¹, $\mu$ æ˜¯å‡å€¼, $\sigma$ æ˜¯æ ‡å‡†å·®ã€‚
   
3. **è®¡ç®—ç¦»ç¾¤å€¼çš„æ¦‚çŽ‡**ï¼šåˆ©ç”¨æ­£æ€åˆ†å¸ƒ, Z å€¼ä»£è¡¨çš„æ˜¯æ•°æ®ç‚¹ä¸Žå‡å€¼çš„åå·®ç¨‹åº¦ã€‚å¯¹äºŽæ­£æ€åˆ†å¸ƒ, è®¡ç®—è¯¥æ•°æ®ç‚¹çš„ç´¯è®¡æ¦‚çŽ‡ã€‚è¿™ä¸ªæ¦‚çŽ‡è¡¨ç¤ºæ•°æ®ç‚¹åœ¨å¤šå¤§ç¨‹åº¦ä¸Šå¯ä»¥è¢«è§†ä¸ºå¼‚å¸¸å€¼ã€‚

4. **åˆ¤æ–­æ•°æ®ç‚¹æ˜¯å¦ä¸ºç¦»ç¾¤å€¼**ï¼šæ ¹æ®æ•°æ®ç‚¹çš„æ•°é‡ $N$, Chauvenet's Criterion æä¾›äº†ä¸€ä¸ªé—¨æ§›ã€‚å¦‚æžœä¸€ä¸ªæ•°æ®ç‚¹çš„æ¦‚çŽ‡ä½ŽäºŽï¼š
   $$
   P = \frac{1}{2N}
   $$
   åˆ™è¯¥æ•°æ®ç‚¹å¯ä»¥è¢«è§†ä¸ºç¦»ç¾¤å€¼å¹¶è¢«æ‹’ç»(REJECT), å¦åˆ™æŽ¥å—(ACCEPT)ã€‚

## Lecture 2

### Error propagation

**Objective**:
1. Understanding how to propagate the error is **a vital part of data analysis and reduction**.  
    äº†è§£å¦‚ä½•ä¼ æ’­é”™è¯¯æ˜¯**æ•°æ®åˆ†æžå’Œå‡å°‘é”™è¯¯çš„é‡è¦éƒ¨åˆ†**ã€‚
2. Understanding which factors contribute to the limiting error is **a vital part of experimental design**.  
    äº†è§£å“ªäº›å› ç´ å¯¼è‡´äº†é™åˆ¶è¯¯å·®æ˜¯**å®žéªŒè®¾è®¡çš„é‡è¦éƒ¨åˆ†**ã€‚

$$
    \alpha_\text{speed} \neq \alpha_\text{distance} + \alpha_{time}
$$
$$
    \alpha_\text{speed} \approx \text{speed}\sqrt{(\frac{\alpha_d}{d})^2 + (\frac{\alpha_t}{t})^2}
$$

#### Single-variable functions å•å˜é‡è¯¯å·®ä¼ æ’­

##### Functional approach
$$
    \overline{Z} \plusmn \alpha_Z = f(\overline{A} + \alpha_A) \\
    \overline{Z} = f(\overline{A}) \\
    \overline{Z} \mp \alpha_Z = f(\overline{A} - \alpha_A)
$$

- Valid for every single-varible function
    $$
        \alpha_Z = |f(\overline{A} + \alpha_A) - f(\overline{A})|
    $$

##### Calculus-based approach

For $Z = f(A)$:

$$
    P = (\overline{A}, f(A))\\
    Q = (\overline{A} + \alpha_A, f(\overline{A}))\\
    R = (\overline{A} + \alpha_A, f(\overline{A}) + \frac{df}{dA} \times \alpha_A)\\
    S = (\overline{A} + \alpha_A, f(\overline{A} + \alpha_A))\\
    f(\overline{A}) + \frac{df(A)}{dA}\alpha_A = f(\overline{A} + \alpha_A)
$$

![](./images/Single-variable%20functions.png)

$$
    \alpha_Z = |\frac{dZ}{dA}|\alpha_A
$$

#### Multi-variable functions å¤šå˜é‡å‡½æ•°çš„è¯¯å·®ä¼ æ’­

##### Functional approach

Consider a function of two variables, $Z = f(A, B)$
The error of Z has 2 components:
- Change in Z when A is varied and B is constant  
  $$
    \alpha^A_Z = f(\overline{A} + \alpha_A, \overline{B}) - f(\overline{A}, \overline{B})
  $$
- Change in Z when B is varied and A is constant 
  $$
    \alpha^B_Z = f(\overline{A}, \overline{B} + \alpha_B) - f(\overline{A}, \overline{B})
  $$

The total error on Z is obtained via Pythagoras, adding in quadrature:
$$
    (\alpha_Z)^2 = (\alpha^A_Z)^2 + (\alpha^B_Z)^2 + (\alpha^C_Z)^2 + ...
$$

> $$
>     (\alpha Z)^2 = 
>   \left[ f(\bar{A} + \alpha_A, \bar{B}, \bar{C}, \dots) - f(\bar{A}, \bar{B}, \bar{C}, \dots) \right]^2 \\
>   + \left[ f(\bar{A}, \bar{\bar{B}} + \alpha_B, \bar{C}, \dots) - f(\bar{A}, \bar{B}, \bar{C}, \dots) \right]^2 \\
>   + \left[ f(\bar{A}, \bar{B}, \bar{C} + \alpha_C, \dots) - f(\bar{A}, \bar{B}, \bar{C}, \dots) \right]^2 \\
>   + \dots
> $$

- Calculus approximation
$$
    (\alpha_Z)^2 = (\frac{\partial Z}{\partial A})^2(\alpha_A)^2 + (\frac{\partial Z}{\partial B})^2(\alpha_B)^2 + (\frac{\partial Z}{\partial C})^2(\alpha_C)^2 + ...
$$

### Least Squares Method æœ€å°äºŒä¹˜æ³•

#### The importance of residuals æ®‹å·®çš„é‡è¦æ€§

æ®‹å·®=å®žé™…å€¼âˆ’æ¨¡åž‹é¢„æµ‹å€¼
$$
    R_i = y_i - y(x_i)
$$
> æœ€ä½³æ‹Ÿåˆç›´çº¿åº”è¯¥ä½¿æ‰€æœ‰æ®‹å·®å°½å¯èƒ½å°ã€‚

#### The goodness-of-fit parameter æ‹Ÿåˆä¼˜åº¦å‚æ•°

Determining the optimal values of parameters for a function is called **regression analysis**.  
ç¡®å®šå‡½æ•°å‚æ•°çš„æœ€ä¼˜å€¼ç§°ä¸ºå›žå½’åˆ†æžã€‚

The **best-fit straight line** is the one that is close to as many data points as possible -> residuals will be small  
**æœ€ä½³æ‹Ÿåˆç›´çº¿**æ˜¯å°½å¯èƒ½æŽ¥è¿‘æ›´å¤šæ•°æ®ç‚¹çš„ç›´çº¿ -> æ®‹å·®ä¼šå¾ˆå°

This is quantified by the **goodness-of-fit parameter**, $X^2$:
- When $X^2$ is minimised, the probability that we obtain our original set of measurements from the best-fit straight line, is maximised.  
    å½“ $X^2$ æœ€å°åŒ–æ—¶ï¼Œæˆ‘ä»¬ä»Žæœ€ä½³æ‹Ÿåˆç›´çº¿èŽ·å¾—åŽŸå§‹æµ‹é‡å€¼é›†åˆçš„æ¦‚çŽ‡æœ€å¤§åŒ–ã€‚
$$
    X^2 = \sum\limits_i\frac{(y_i - y(x_i))^2}{\alpha^2_i}
$$

> What we have just done is called **method of least squares** (= minimising the sum of the squares of the residuals)  
> æˆ‘ä»¬åˆšåˆšåšçš„å«åšæœ€å°äºŒä¹˜æ³•ï¼ˆ=æœ€å°åŒ–æ®‹å·®å¹³æ–¹å’Œï¼‰  
> In the case of the straight line, the best values of slope and intercept are those that minimise the summed differences squared  
> å¯¹äºŽç›´çº¿ï¼Œæ–œçŽ‡å’Œæˆªè·çš„æœ€ä½³å€¼æ˜¯æœ€å°åŒ–å¹³æ–¹å’Œçš„å€¼  
> This is derived from what is called **maximum likelihood** together with the **central limit theorem** (assumption: the parent distribution from which we draw the yi values is Gaussian width width given by the standard error, âºi)  
> è¿™æ˜¯ä»Žæ‰€è°“çš„æœ€å¤§ä¼¼ç„¶å’Œä¸­å¿ƒæžé™å®šç†ä¸­å¾—å‡ºçš„ï¼ˆå‡è®¾ï¼šæˆ‘ä»¬ä»Žä¸­å¾—å‡º yi å€¼çš„çˆ¶åˆ†å¸ƒæ˜¯é«˜æ–¯å®½åº¦ï¼Œç”±æ ‡å‡†è¯¯å·® âºi ç»™å‡ºï¼‰
> The y-coordinate we get from the best-fit line equation is the **most probable** value of the parent distribution  
> æˆ‘ä»¬ä»Žæœ€ä½³æ‹Ÿåˆçº¿æ–¹ç¨‹ä¸­å¾—åˆ°çš„ y åæ ‡æ˜¯çˆ¶åˆ†å¸ƒçš„æœ€å¯èƒ½å€¼  
> The probability of obtaining our measurement values from the best-fit line is maximised when Ï‡2 is minimised  
> å½“ Ï‡2 æœ€å°åŒ–æ—¶ï¼Œä»Žæœ€ä½³æ‹Ÿåˆçº¿èŽ·å¾—æµ‹é‡å€¼çš„æ¦‚çŽ‡æœ€å¤§åŒ–

- $X^2$ for data with Poisson errors
$$
    X^2 = \sum\limits_i\frac{(O_i-E_i)^2}{E_i}
$$

> i: number of counts(bins)
> $O_i$: observed number of occurrences(in the i-th bin)
> $E_j$: expected number of occurrences(in the i-th bin)

> If we have a goot fit, $\alpha_i = \sqrt{E_i} \approx \sqrt{O_i}$

#### Minimisation æœ€å°åŒ–

The best values for **slope** and **intercept** are those that minimise the squares of the differences summed for all data points  
æ–œçŽ‡å’Œæˆªè·çš„æœ€ä½³å€¼æ˜¯æœ€å°åŒ–æ‰€æœ‰æ•°æ®ç‚¹ä¹‹å·®çš„å¹³æ–¹å’Œ

$$
    S = \sum\limits_i(y_i - y(x_i))^2 = \sum\limits_i R^2_i\\
    S = \sum\limits_i(y_i - mx_i - c)^2 
$$

$$
    \frac{\partial S}{\partial m} = -2\sum_i(x_i[y_i - mx_i - c]) = 0\\
    \frac{\partial S}{\partial c} = -2\sum_i(y_i - mx_i - c) = 0
$$

$S$ is a minimum when $\frac{\partial S}{\partial m} = \frac{\partial S}{\partial c} = 0$

The required values of the slope (m) and the intercept (c) are obtained from the two simultaneous equations  
æ–œçŽ‡ (m) å’Œæˆªè· (c) çš„æ‰€éœ€å€¼å¯é€šè¿‡ä¸¤ä¸ªè”ç«‹æ–¹ç¨‹å¾—å‡º

$$
    m\sum_i x^2_i + c\sum_i x_i = \sum_i x_i y_i
$$
$$
    m\sum_i x_i + cN = \sum_i y_i
$$

- The solutions for gradient, intercept, and their uncertainties reduce to simple analytic expressions  
æˆªè· ð‘ çš„å…¬å¼
$$
    c = \frac{\sum_ix^2_i\sum_iy_i - \sum_ix_i\sum_ix_iy_i}{\Delta}, \alpha_c = \alpha_{CU}\sqrt{\frac{\sum_ix^2_i}{\Delta}}
$$

æ–œçŽ‡ ð‘š çš„å…¬å¼ï¼š
$$
    m = \frac{N\sum_ix_iy_i - \sum_ix_i\sum_iy_i}{\Delta}, \alpha_m = \alpha_{CU}\sqrt{\frac{N}{\Delta}}
$$

> å…±åŒä¸ç¡®å®šæ€§:    
> Common uncertainty: $\alpha_{CU} = \sqrt{\frac{1}{N - 2}\sum_i(y_i - mx_i - c)^2}$

æ€»ä¸ç¡®å®šæ€§å‚æ•° Î” 
$$
\Delta = N\sum_ix^2_i - (\sum_i x_i)^2
$$

#### non-uniform error bars

Need to perform a **weighted** least-squares fit to take them into account  
éœ€è¦æ‰§è¡Œ**åŠ æƒ**æœ€å°äºŒä¹˜æ³•æ‰èƒ½å°†å®ƒä»¬è€ƒè™‘åœ¨å†…
$$
    R_i = \frac{y_i - y(x_i)}{\alpha_i}
$$
The sum of the squares of the normalised residuals is called $X^2$

This is now a weighted fit, and we need to take this into account in the analytic expressions for $m, c, \alpha_c$, with $w_i = \alpha^{-2}_i$

> Points with small errors are more important!

## Lecture 3

### Least-squares fit to an arbitrary function

- Arbitrary non-linear function with N parameters
  $$
    y(x) = f(x; a_1, a_2, ..., a_N)
  $$
- Procedure:
  1. for each value of the independent variable, $x_i$, calculate $y(x_i)$ using an estimated set of values for the parameters
  2. for each value of the independent variables, calculate the square of the normalised residual, $[^{(y_i-y(x_i))}/_{\alpha i}]^2$
  3. calculate $\chi^2$(sum the square of the normalised residuals)
  4. minimise $\chi^2$ by optimising the fit parameters

### Residuals for a least-squares fit to an arbitrary function

- Voltage across an inductor as a function of time - $V(t; a_1, a_2, ..., a_N)$ is a non-linear function
  - $V(t;V_{bat}, V_0, T, \phi, \tau)$
  - Use model and reasonable estimates of parameters to calculate values of the voltage for a range of times
  - Calculate $\chi^2$
  - Minimise $\chi^2$ by varying all 5 parameters to give best fit

$$
    V(t) = V_{bgt} + V_0\cos(2\pi\frac{t}{T}+\phi)\exp(\frac{-t}{\tau})
$$

Data and weighted least-squares-fit for X-ray diffraction of copper

- Fit data with a double-peak model:  
  ![](./images/double_peak%20model.png)
  - residuals randomly distributed -> good fit
- Fit data with a single-peak model:  
  ![](./images/single_peak%20model.png)
  - residuals show structure -> bad fit

- To better visualise structure in residuals: make a lag plot -> normalised residuals $R_i$ vs lagged residuals $R_{i-k}$(k is usually 1)
- Good fit  
  ![](./images/good_fit%20lag_plot.png)
  - Random parttern
  - at least 91% of data points in a 2D box of $\pm2$ limits
- Bad fit:  
  ![](./images/bad_fit%20lag_plot.png)
  - non-random pattern
  - < 91% of data points in a 2D box of $\pm2$ limits

### Durbin-Watson statistic

The degree of correlation in the lag plot can be reduced to a single numerical value by evaluating the **Durbin-Waston statistic, D**

$$
    D = \frac{\sum\limits^N_{i=2}[R_i - R_{i-1}]^2}{\sum\limits^2_{i=1}[R_i]^2}
$$
> What does it mean?
> - 0 < D < 4
> - D = 0: systematically correlated residuals
> - D = 2: randomly distributed residuals with Gaussian distribution
> - D = 4: systematically anticorrelated residuals

### Calculating the error in a least-squares-fit

#### the error surface

- Remember how $\chi^2$ evolved with the gradient m of a straight line?
- More generic:
  - Function is more complex
  - Non-uniform error bars
  - Goodness-of-fit remains $\chi^2$, but now it evolves over **surface** defined by the fit parameters

- Shape of error surface is important:
  - Many(few) contours on axis of a parameter = high(low) sensitivity of fit to that parameter
  - Tilted ellipses $\rarr$ correlation between uncertainties of parameter
  - No tilt $\rarr$ no correlation between $\alpha_A, \alpha_B$

- Investigate shape of error surface via Taylor expansion of $\chi^2$
- Taylor expansion: behavior in close vicinty to a value
  $$
    f(x, a + \Delta a) = f(x, a) + \Delta a\frac{\partial f}{\partial a} + \frac{1}{2}\frac{\partial^2 f}{\partial a^2}(\Delta a)^2 + \dots
  $$
  $$
    \chi^2(\overline{a}_j + \Delta a_j) = \chi^2(\overline{a}_j) + \frac{1}{2}\frac{\partial^2\chi^2}{\partial a^2_j}
  $$

If $\Delta a_j$ (the deviation from the best-fit value) is similar to uncertainty
$$
    \chi^2 \rarr \chi^2_{\min} + 1
$$
$\chi^2$ increases by 1  
This effectively corresponds to the $1\sigma$ contour

Can now express the standard error in terms of the curvature of the error surface
$$
    \alpha_j = \sqrt{\frac{2}{(\frac{\partial^2\chi^2}{\partial a^2_j})}}
$$

### Curvature matrix for straight line fit

- Let's stay with straight line fit and introduce the concept of curvature matrix
- We will see later that the uncertainties for N fit parameters can be calculated from the inverse of the curvature matrix, i.e. the error matrix 
- For a straight line fit, the $\chi^2$ surface is perfectly parabolic with respect to both variabels, such that:
  - There is only 1 minimum
  - Finding the minimum is easy
  - The curvature matrix has analytic results that let you calculate errors easily$A=\begin{bmatrix}A_{cc}&&A_{cm}\\A_{mc}&&A_{mm}\end{bmatrix}$
    $$
        A_{cc}=\sum_{i}\frac{1}{\alpha_{i}^{2}}\quad A_{cm}=A_{mc}=\sum_{i}\frac{x_{i}}{\alpha_{i}^{2}}\quad  A_{mm}=\sum_{i}\frac{x_{i}^{2}}{\alpha_{i}^{2}}
    $$

### The $\chi^2$ surface for an arbitrary function

Arbitrary function: the $\chi^2$ surface can be very complicated!
- Multiple local minima
- Need an initial guess for the best-fit parameters(to avoid being trapped in a local minimum)
- The $\Delta \chi^2$ contours can be asymmetric(might not be able limits)
- The elements of the curvature matrix might not have analytic results $\rarr$ need numerical techniques to calculate them

- Newton-Raphson
- Technique that numerically solves $f(x) = 0$
  1. First (approximate) solution is $x_1$
  2. Let $f(x)$ crosses zero at $x_1 + h, f(x_1 + h) = 0$
  3. If h is small, can use Taylor expansion to find second approximation for zero crossing point $x_2$, $x_2 = x_1 - \frac{f(x_1)}{f'(x_1)}$
  4. Repeat this to get successively closer approximation,
    $$
        x_{s+1} = x_s - \frac{f(x_s)}{f'(x_s)}
    $$
  5. Procedure ends when the zero-crossing point is found(within given tolerance)

- Grid search
- The goodness-of-fit parameter is minimized by changing each parameter in turn, based on input step sizes
  1. Gradient is keptt the same, intercept is increased. If $\chi^2$ increases, the direction of motion across surface is reversed
  2. Parameters are changed in turn until convergence  
  Inefficient method, because parameters are changed sequentially

### How do fitting programs minimise?

Iterative approaches

- Gradient descent
  - Change all parameters simultaneously, with vector directed towards minimum
    1. Vector $\triangledown \chi^2$ is along direction along which $\chi^2$ varies most rapidly
    2. Take steps along this steepest descent util convergence, with the gradient being
        $$
            \left(\nabla\chi^{2}\right)_{j}=\frac{\partial\chi^{2}}{\partial a_{j}}\approx\frac{\chi^{2}\left(a_{j}+\delta a_{j}\right)-\chi^{2}\left(a_{j}\right)}{\delta a_{j}}
        $$
    3. Update rule: $a_{s+1}=a_{s}-B \nabla\chi^{2} (a_{s})$
        > B is scaling factor

- Second-order expansion
  - An excellent approximation of the local minimum is a second-order expansion in the parameters about the minimum, i.e. perform a Taylor expansion of $\chi^2$ about the set of parameters $a_s$
    $$
        \chi^{2}\left(\mathrm{a}_{s}+\mathrm{h}\right)\approx\chi^{2}\left(\mathrm{a}_{s}\right)+\mathrm{g}_{s}^{\mathrm{T}}\mathrm{h}+\frac{1}{2}\mathrm{h}^{\mathrm{T}}\mathrm{H}_{s}\mathrm{h}
    $$
    $$
        \mathfrak{g}_s=\nabla\chi^2\left(\mathfrak{a}_s\right)=\left[\frac{\partial\chi^2}{\partial a_1},\cdots,\frac{\partial\chi^2}{\partial a_{\mathcal{N}}}\right]^\mathrm{T}
    $$

- Marquardt-Levenburg method
- Combines best features of gradient and expansion approaches
  - Uses method of steepest descent to progress towards minimum when initial guess was far from optimum value
  - Smooth transition to expansion method when goodness-of-fit parameter reduces, and surface becomes parabolic(no multiple local minima)
  $$
    \mathrm{a}_{s+1}=\mathrm{a}_{s}-\left(\mathrm{H}_{s}+\lambda\mathrm{diag}\left[\mathrm{H}_{s}\right]\right)^{-\mathrm{1}}\mathrm{g}_{s}
  $$

### Covariance(error) matrix and uncertainties in fit parameters

- Simply: the curvature matrix, A, is one half of the Hessian matrix
  - It is also an NxN matrix
  - It has components $A_{jk} = \frac{1}{2}\frac{\partial x^2}{\partial a_j \partial a_k}$
  - Off-diagonal terms are related to degree of correlation of parameter uncertainties(they describe the curvature of the surface, remember!)

- Inverse of curvature matrix = covariance(error) matrix
  $$
    [C] = [A]^{-1}
  $$
  And finally: the uncertainty $a_j$ of the parameter $a_i$ is $\alpha_j = \sqrt{C_{jj}}$, so $a_j + a_j = a_j \pm \sqrt{C_{jj}}$

### Correlation between uncertainties of fit parameters

- The off-diagonal elements of the covariance matrix are the correlation coefficients
- It's easier to use a dimensionless measure for the correlation matrix:
  - Diagonal elements are all 1
  - Correlation coefficients $\rho_{AB} = \frac{c_{AB}}{\sqrt{c_{AA}c_{BB}}}$
    $$
        -1 \leq \rho \leq 1 \\
        \text{0 when A, B uncorrelated}
    $$

- What is the voltage and its error for $f = 75Hz$?
  - Without correlation: $\alpha_{V}^{2}=f^{2}\alpha_{m}^{2} + \alpha_{c}^{2}=f^{2}C_{22}+C_{11}$
  - With correlation: $\alpha_{V}^{2}=f^{2}C_{22}+C_{11}+2fC_{12}$
    
