# Note - Machine Learning and Statistics

## Lecture 1

### Precise & accuracy

- **Precise** measurement: the spread of results is "**small**"
- **Accurate** measurement: result is in agreement with "**accepted**" value

### Mean, standard deviation, standard error

- **Mean**: where is the measurement centred
    $$
        \overline{x} = \frac{1}{N}(x_1 + x_2 + \dots + x_n) = \frac{1}{N}\sum\limits^N_{i=1}x_i
    $$
- **Standard deviation**: width of the distribution
    $$
        \sigma_{n-1} = \sqrt{\frac{(d^2_1 + d^2_2 + \dots + d^2_N)}{N - 1}} = \sqrt{\frac{1}{N - 1}\sum\limits^N_{i=1}d^2_i}
    $$
    where $d_i = x_i - \overline{x}$
- **Standard error**: uncertainty in the location of the centre, $\alpha$
    $$
        \alpha = \frac{\sigma_{N-1}}{\sqrt{N}}
    $$
    We should quote our finding as $\overline{x} \plusmn \alpha$

> Consider an experiment with N number of data points collected:
> - the standard deviation is independent of N  
>   æ ‡å‡†å·®ä¸å—æ ·æœ¬é‡ ð‘ çš„å½±å“, å› ä¸ºå®ƒåªå…³å¿ƒæ•°æ®çš„åˆ†æ•£ç¨‹åº¦
> - the standard error improves with N  
>   æ ‡å‡†è¯¯å·®éšæ ·æœ¬é‡ ð‘ çš„å¢žåŠ è€Œå‡å°, æ„å‘³ç€éšç€æ”¶é›†æ›´å¤šçš„æ•°æ®, å‡å€¼ä¼°è®¡ä¼šæ›´åŠ ç²¾ç¡®

### Random errors, the normal distribution

- **Gaussian or Normal Distribution**
    $$
        f(x) = \frac{1}{\sigma\sqrt{2\pi}}\exp[-\frac{(x-\overline{x})^2}{2\sigma^2}]
    $$
    > Facts: peak centred around mean, symmetric about mean,area under curve equals 1(normalised)

- The error in the error
    $$
        \alpha^{\plusmn} = \text{error in the error} = \frac{1}{\sqrt{2N-2}}
    $$

> Bigger sample size:
> - lower error in the error
> - more confidence
> - can quote more significant figures
>
> Need $N=50$ for the error to be known to 10%
> Need $N > 10k$ for the error to be known below 1%

### Reporting results, Confidence limits and error bars

- The five golden rules for reporting results
  1. The best estimate for a parameter is the mean
  2. The error is the standard error in the mean
  3. Round up the error to the appropriate number of significant figures
  4. Match the number of decimal places in the mean to the standard error
  5. Include units

- What is the probability of the data to lie within some multiple of $\sigma$?â€
    - We need to evaluate the **error function** of the Gaussian distribution(G)
        $$
            Erf(x_1;\overline{x}, \sigma) = \int^{x_1}_{-\infty}G(x;\overline{x},\sigma)
        $$

        $$
           f(x) = \frac{1}{\sqrt{2\pi}} e^{-x^2 / 2}
        $$
        $$
            \text{CDF}(x) = \frac{1}{2} \left[ 1 + \text{erf}\left( \frac{x}{\sqrt{2}} \right) \right]
        $$
        $$
             P(-\sigma \leq X \leq \sigma) = \text{erf}\left( \frac{\sigma}{\sqrt{2}} \right)
        $$

- The standard deviation is thus used to define a **confidence level** on the data
- If your result and the accepted value differ by:
  - Up to 1 standard error it is in **excellent agreement**
  - Between 1 and 2: **reasonable agreement**
  - More than 3 standard errors: **disagreement**

### Poisson distribution

- The conditions under which a Poisson distribution holds are:
  - Events are rare
  - Events are independent
  - Tha average rate does not change with time
  $$
    P(N;\overline{N}) = \frac{\exp(-\overline{N})\overline{N}^N}{N!}
  $$
  > Mean = $\overline{N}$
  > Standard deviation = $\sqrt{\overline{N}}$

### Chauvenet's Criterion 

**Chauvenet's Criterion** æ˜¯ä¸€ç§ç»Ÿè®¡å­¦æ–¹æ³•, ç”¨äºŽæ£€æµ‹å’Œåˆ¤æ–­æ•°æ®é›†ä¸­æ˜¯å¦å­˜åœ¨ç¦»ç¾¤å€¼(outliers)ã€‚ç¦»ç¾¤å€¼æ˜¯æŒ‡ä¸Žå…¶ä»–æ•°æ®ç‚¹åå·®è¾ƒå¤§çš„æ•°æ®ç‚¹ã€‚Chauvenet çš„å‡†åˆ™åŸºäºŽæ ‡å‡†æ­£æ€åˆ†å¸ƒ, æä¾›äº†ä¸€ç§æ–¹æ³•æ¥ç¡®å®šä¸€ä¸ªæ•°æ®ç‚¹æ˜¯å¦ä¸Žæ•°æ®é›†çš„å…¶ä»–éƒ¨åˆ†æ˜¾è‘—ä¸åŒã€‚

### ä½¿ç”¨ Chauvenet's Criterion çš„æ­¥éª¤ï¼š

1. **è®¡ç®—æ•°æ®çš„å‡å€¼(mean, $\mu$)å’Œæ ‡å‡†å·®(standard deviation, $\sigma$)**ã€‚

2. **è®¡ç®—æ¯ä¸ªæ•°æ®ç‚¹ä¸Žå‡å€¼çš„åå·®**ï¼šä½¿ç”¨ä»¥ä¸‹å…¬å¼è®¡ç®—æ¯ä¸ªæ•°æ®ç‚¹ä¸Žå‡å€¼çš„æ ‡å‡†åŒ–å·®å€¼(å³ Z å€¼)ï¼š
   $$
   Z = \frac{|x_i - \mu|}{\sigma}
   $$
   å…¶ä¸­ $x_i$ æ˜¯ç¬¬ $i$ ä¸ªæ•°æ®ç‚¹, $\mu$ æ˜¯å‡å€¼, $\sigma$ æ˜¯æ ‡å‡†å·®ã€‚
   
3. **è®¡ç®—ç¦»ç¾¤å€¼çš„æ¦‚çŽ‡**ï¼šåˆ©ç”¨æ­£æ€åˆ†å¸ƒ, Z å€¼ä»£è¡¨çš„æ˜¯æ•°æ®ç‚¹ä¸Žå‡å€¼çš„åå·®ç¨‹åº¦ã€‚å¯¹äºŽæ­£æ€åˆ†å¸ƒ, è®¡ç®—è¯¥æ•°æ®ç‚¹çš„ç´¯è®¡æ¦‚çŽ‡ã€‚è¿™ä¸ªæ¦‚çŽ‡è¡¨ç¤ºæ•°æ®ç‚¹åœ¨å¤šå¤§ç¨‹åº¦ä¸Šå¯ä»¥è¢«è§†ä¸ºå¼‚å¸¸å€¼ã€‚

4. **åˆ¤æ–­æ•°æ®ç‚¹æ˜¯å¦ä¸ºç¦»ç¾¤å€¼**ï¼šæ ¹æ®æ•°æ®ç‚¹çš„æ•°é‡ $N$, Chauvenet's Criterion æä¾›äº†ä¸€ä¸ªé—¨æ§›ã€‚å¦‚æžœä¸€ä¸ªæ•°æ®ç‚¹çš„æ¦‚çŽ‡ä½ŽäºŽï¼š
   $$
   P = \frac{1}{2N}
   $$
   åˆ™è¯¥æ•°æ®ç‚¹å¯ä»¥è¢«è§†ä¸ºç¦»ç¾¤å€¼å¹¶è¢«æ‹’ç»(REJECT), å¦åˆ™æŽ¥å—(ACCEPT)ã€‚

## Lecture 2

### Error propagation

**Objective**:
1. Understanding how to propagate the error is **a vital part of data analysis and reduction**.  
    äº†è§£å¦‚ä½•ä¼ æ’­é”™è¯¯æ˜¯**æ•°æ®åˆ†æžå’Œå‡å°‘é”™è¯¯çš„é‡è¦éƒ¨åˆ†**ã€‚
2. Understanding which factors contribute to the limiting error is **a vital part of experimental design**.  
    äº†è§£å“ªäº›å› ç´ å¯¼è‡´äº†é™åˆ¶è¯¯å·®æ˜¯**å®žéªŒè®¾è®¡çš„é‡è¦éƒ¨åˆ†**ã€‚

$$
    \alpha_\text{speed} \neq \alpha_\text{distance} + \alpha_{time}
$$
$$
    \alpha_\text{speed} \approx \text{speed}\sqrt{(\frac{\alpha_d}{d})^2 + (\frac{\alpha_t}{t})^2}
$$

#### Single-variable functions å•å˜é‡è¯¯å·®ä¼ æ’­

##### Functional approach
$$
    \overline{Z} \plusmn \alpha_Z = f(\overline{A} + \alpha_A) \\
    \overline{Z} = f(\overline{A}) \\
    \overline{Z} \mp \alpha_Z = f(\overline{A} - \alpha_A)
$$

- Valid for every single-varible function
    $$
        \alpha_Z = |f(\overline{A} + \alpha_A) - f(\overline{A})|
    $$

##### Calculus-based approach

For $Z = f(A)$:

$$
    P = (\overline{A}, f(A))\\
    Q = (\overline{A} + \alpha_A, f(\overline{A}))\\
    R = (\overline{A} + \alpha_A, f(\overline{A}) + \frac{df}{dA} \times \alpha_A)\\
    S = (\overline{A} + \alpha_A, f(\overline{A} + \alpha_A))\\
    f(\overline{A}) + \frac{df(A)}{dA}\alpha_A = f(\overline{A} + \alpha_A)
$$

![](./images/Single-variable%20functions.png)

$$
    \alpha_Z = |\frac{dZ}{dA}|\alpha_A
$$

#### Multi-variable functions å¤šå˜é‡å‡½æ•°çš„è¯¯å·®ä¼ æ’­

##### Functional approach

Consider a function of two variables, $Z = f(A, B)$
The error of Z has 2 components:
- Change in Z when A is varied and B is constant  
  $$
    \alpha^A_Z = f(\overline{A} + \alpha_A, \overline{B}) - f(\overline{A}, \overline{B})
  $$
- Change in Z when B is varied and A is constant 
  $$
    \alpha^B_Z = f(\overline{A}, \overline{B} + \alpha_B) - f(\overline{A}, \overline{B})
  $$

The total error on Z is obtained via Pythagoras, adding in quadrature:
$$
    (\alpha_Z)^2 = (\alpha^A_Z)^2 + (\alpha^B_Z)^2 + (\alpha^C_Z)^2 + ...
$$

> $$
>     (\alpha Z)^2 = 
>   \left[ f(\bar{A} + \alpha_A, \bar{B}, \bar{C}, \dots) - f(\bar{A}, \bar{B}, \bar{C}, \dots) \right]^2 \\
>   + \left[ f(\bar{A}, \bar{\bar{B}} + \alpha_B, \bar{C}, \dots) - f(\bar{A}, \bar{B}, \bar{C}, \dots) \right]^2 \\
>   + \left[ f(\bar{A}, \bar{B}, \bar{C} + \alpha_C, \dots) - f(\bar{A}, \bar{B}, \bar{C}, \dots) \right]^2 \\
>   + \dots
> $$

- Calculus approximation
$$
    (\alpha_Z)^2 = (\frac{\partial Z}{\partial A})^2(\alpha_A)^2 + (\frac{\partial Z}{\partial B})^2(\alpha_B)^2 + (\frac{\partial Z}{\partial C})^2(\alpha_C)^2 + ...
$$

### Least Squares Method æœ€å°äºŒä¹˜æ³•

#### The importance of residuals æ®‹å·®çš„é‡è¦æ€§

æ®‹å·®=å®žé™…å€¼âˆ’æ¨¡åž‹é¢„æµ‹å€¼
$$
    R_i = y_i - y(x_i)
$$
> æœ€ä½³æ‹Ÿåˆç›´çº¿åº”è¯¥ä½¿æ‰€æœ‰æ®‹å·®å°½å¯èƒ½å°ã€‚

#### The goodness-of-fit parameter æ‹Ÿåˆä¼˜åº¦å‚æ•°

Determining the optimal values of parameters for a function is called **regression analysis**.  
ç¡®å®šå‡½æ•°å‚æ•°çš„æœ€ä¼˜å€¼ç§°ä¸ºå›žå½’åˆ†æžã€‚

The **best-fit straight line** is the one that is close to as many data points as possible -> residuals will be small  
**æœ€ä½³æ‹Ÿåˆç›´çº¿**æ˜¯å°½å¯èƒ½æŽ¥è¿‘æ›´å¤šæ•°æ®ç‚¹çš„ç›´çº¿ -> æ®‹å·®ä¼šå¾ˆå°

This is quantified by the **goodness-of-fit parameter**, $X^2$:
- When $X^2$ is minimised, the probability that we obtain our original set of measurements from the best-fit straight line, is maximised.  
    å½“ $X^2$ æœ€å°åŒ–æ—¶ï¼Œæˆ‘ä»¬ä»Žæœ€ä½³æ‹Ÿåˆç›´çº¿èŽ·å¾—åŽŸå§‹æµ‹é‡å€¼é›†åˆçš„æ¦‚çŽ‡æœ€å¤§åŒ–ã€‚
$$
    X^2 = \sum\limits_i\frac{(y_i - y(x_i))^2}{\alpha^2_i}
$$

> What we have just done is called **method of least squares** (= minimising the sum of the squares of the residuals)  
> æˆ‘ä»¬åˆšåˆšåšçš„å«åšæœ€å°äºŒä¹˜æ³•ï¼ˆ=æœ€å°åŒ–æ®‹å·®å¹³æ–¹å’Œï¼‰  
> In the case of the straight line, the best values of slope and intercept are those that minimise the summed differences squared  
> å¯¹äºŽç›´çº¿ï¼Œæ–œçŽ‡å’Œæˆªè·çš„æœ€ä½³å€¼æ˜¯æœ€å°åŒ–å¹³æ–¹å’Œçš„å€¼  
> This is derived from what is called **maximum likelihood** together with the **central limit theorem** (assumption: the parent distribution from which we draw the yi values is Gaussian width width given by the standard error, âºi)  
> è¿™æ˜¯ä»Žæ‰€è°“çš„æœ€å¤§ä¼¼ç„¶å’Œä¸­å¿ƒæžé™å®šç†ä¸­å¾—å‡ºçš„ï¼ˆå‡è®¾ï¼šæˆ‘ä»¬ä»Žä¸­å¾—å‡º yi å€¼çš„çˆ¶åˆ†å¸ƒæ˜¯é«˜æ–¯å®½åº¦ï¼Œç”±æ ‡å‡†è¯¯å·® âºi ç»™å‡ºï¼‰
> The y-coordinate we get from the best-fit line equation is the **most probable** value of the parent distribution  
> æˆ‘ä»¬ä»Žæœ€ä½³æ‹Ÿåˆçº¿æ–¹ç¨‹ä¸­å¾—åˆ°çš„ y åæ ‡æ˜¯çˆ¶åˆ†å¸ƒçš„æœ€å¯èƒ½å€¼  
> The probability of obtaining our measurement values from the best-fit line is maximised when Ï‡2 is minimised  
> å½“ Ï‡2 æœ€å°åŒ–æ—¶ï¼Œä»Žæœ€ä½³æ‹Ÿåˆçº¿èŽ·å¾—æµ‹é‡å€¼çš„æ¦‚çŽ‡æœ€å¤§åŒ–

- $X^2$ for data with Poisson errors
$$
    X^2 = \sum\limits_i\frac{(O_i-E_i)^2}{E_i}
$$

> i: number of counts(bins)
> $O_i$: observed number of occurrences(in the i-th bin)
> $E_j$: expected number of occurrences(in the i-th bin)

> If we have a goot fit, $\alpha_i = \sqrt{E_i} \approx \sqrt{O_i}$

#### Minimisation æœ€å°åŒ–

The best values for **slope** and **intercept** are those that minimise the squares of the differences summed for all data points  
æ–œçŽ‡å’Œæˆªè·çš„æœ€ä½³å€¼æ˜¯æœ€å°åŒ–æ‰€æœ‰æ•°æ®ç‚¹ä¹‹å·®çš„å¹³æ–¹å’Œ

$$
    S = \sum\limits_i(y_i - y(x_i))^2 = \sum\limits_i R^2_i\\
    S = \sum\limits_i(y_i - mx_i - c)^2 
$$

$$
    \frac{\partial S}{\partial m} = -2\sum_i(x_i[y_i - mx_i - c]) = 0\\
    \frac{\partial S}{\partial c} = -2\sum_i(y_i - mx_i - c) = 0
$$

$S$ is a minimum when $\frac{\partial S}{\partial m} = \frac{\partial S}{\partial c} = 0$

The required values of the slope (m) and the intercept (c) are obtained from the two simultaneous equations  
æ–œçŽ‡ (m) å’Œæˆªè· (c) çš„æ‰€éœ€å€¼å¯é€šè¿‡ä¸¤ä¸ªè”ç«‹æ–¹ç¨‹å¾—å‡º

$$
    m\sum_i x^2_i + c\sum_i x_i = \sum_i x_i y_i
$$
$$
    m\sum_i x_i + cN = \sum_i y_i
$$

- The solutions for gradient, intercept, and their uncertainties reduce to simple analytic expressions  
æˆªè· ð‘ çš„å…¬å¼
$$
    c = \frac{\sum_ix^2_i\sum_iy_i - \sum_ix_i\sum_ix_iy_i}{\Delta}, \alpha_c = \alpha_{CU}\sqrt{\frac{\sum_ix^2_i}{\Delta}}
$$

æ–œçŽ‡ ð‘š çš„å…¬å¼ï¼š
$$
    m = \frac{N\sum_ix_iy_i - \sum_ix_i\sum_iy_i}{\Delta}, \alpha_m = \alpha_{CU}\sqrt{\frac{N}{\Delta}}
$$

> å…±åŒä¸ç¡®å®šæ€§:    
> Common uncertainty: $\alpha_{CU} = \sqrt{\frac{1}{N - 2}\sum_i(y_i - mx_i - c)^2}$

æ€»ä¸ç¡®å®šæ€§å‚æ•° Î” 
$$
\Delta = N\sum_ix^2_i - (\sum_i x_i)^2
$$

#### non-uniform error bars

Need to perform a **weighted** least-squares fit to take them into account
$$
    R_i = \frac{y_i - y(x_i)}{\alpha_i}
$$
The sum of the squares of the normalised residuals is called $X^2$

This is now a weighted fit, and we need to take this into account in the analytic expressions for $m, c, \alpha_c$, with $w_i = \alpha^{-2}_i$

> Points with small errors are more important!

