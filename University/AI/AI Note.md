<!-- TOC start -->
- [AI Note](#ai-note)
  * [1. Machine Learning Basics](#1-machine-learning-basics)
    + [1.1 Categories of machine learning](#11-categories-of-machine-learning)
    + [1.2 Supervised learning workflow](#12-supervised-learning-workflow)
    + [1.3 Model evaluation](#13-model-evaluation)
  * [2. Hierarchical Clustering](#2-hierarchical-clustering)
    + [2.1 Clustering concepts](#21-clustering-concepts)
    + [2.2 Hierarchical clustering](#22-hierarchical-clustering)
  * [3. K-means](#3-k-means)
    + [3.1 K-means](#31-k-means)
  * [4. GMM/EM](#4-gmmem)
    + [4.1 Gaussian mixture models(GMMs)](#41-gaussian-mixture-modelsgmms)
    + [4.2 Expectation-Maximization(EM Algorithm)](#42-expectation-maximizationem-algorithm)
    + [4.3 Summary](#43-summary)
  * [5. DBSCAN](#5-dbscan)
    + [5.1 Density-based Clustering - DBSCAN](#51-density-based-clustering---dbscan)
    + [5.2 The algorithm](#52-the-algorithm)
  * [6. Supervised Learning](#6-supervised-learning)
    + [6.1 Supervised Learning](#61-supervised-learning)
    + [6.2 Training data](#62-training-data)
    + [6.3 Terminology in Supervised Learning](#63-terminology-in-supervised-learning)
    + [6.4 Applications of supervised learning](#64-applications-of-supervised-learning)
  * [7. Linear Regression](#7-linear-regression)
    + [7.1 Regression](#71-regression)
    + [7.2 Univariate linear regression](#72-univariate-linear-regression)
    + [7.3 Loss functions (or cost functions)](#73-loss-functions-or-cost-functions)
    + [7.4 what we want to do](#74-what-we-want-to-do)
    + [7.5 Gradient Descent](#75-gradient-descent)
    + [7.6 Gradient](#76-gradient)
  * [8. Logistic Regression](#8-logistic-regression)
    + [8.1 Logistic regression](#81-logistic-regression)
    + [8.2 Model formulation](#82-model-formulation)
    + [8.3 Cost function](#83-cost-function)
    + [8.4 Learning algorithm by gradient descent](#84-learning-algorithm-by-gradient-descent)
  * [9. Neural Networks](#9-neural-networks)
    + [9.1 Neural Networks](#91-neural-networks)
    + [9.2 Overfitting](#92-overfitting)

<!-- TOC end -->

# AI Note

## 1. Machine Learning Basics

### 1.1 Categories of machine learning

- Supervised learning  
  æœ‰ç›‘ç£çš„å­¦ä¹ 

  > Labeled data  
  > æ ‡è®°æ•°æ®  
  > Predict outcome/future  
  > é¢„æµ‹ç»“æœ/æœªæ¥

  - Classification: predict categorical class labels  
    åˆ†ç±»:é¢„æµ‹åˆ†ç±»ç±»åˆ«æ ‡ç­¾
    - e.g. the handwritten digit (multi-class)  
      ä¾‹å¦‚ï¼Œæ‰‹å†™çš„æ•°å­—ï¼ˆå¤šç±»ï¼‰

  ![Classification_1.png](Images/Classification_1.png)

  - Regression: Prediction of continuous outcomes  
    å›å½’: å¯¹è¿ç»­ç»“æœçš„é¢„æµ‹
    - e.g. studentsâ€™ grade scores ä¾‹å¦‚å­¦ç”Ÿçš„æˆç»©

  ![Regression_1.png](Images/Regression_1.png)

- Unsupervised learning  
  æ— ç›‘ç£çš„å­¦ä¹ 

  > No labels/targets  
  > æ— æ ‡ç­¾/ç›®æ ‡  
  > Find hidden structure/insights in data  
  > åœ¨æ•°æ®ä¸­æ‰¾åˆ°éšè—çš„ç»“æ„/è§è§£

  - Clustering: Objectives within a cluster share a degree of
    similarity.  
    èšç±»: é›†ç¾¤å†…çš„ç›®æ ‡æœ‰ä¸€å®šç¨‹åº¦çš„ç›¸ä¼¼æ€§
    - e.g. product recommendation  
      ä¾‹å¦‚äº§å“æ¨è

  ![Clustering_1.png](Images/Clustering_1.png)

  - Dimensionality Reduction:  
    é™ç»´
    - reduce data sparsity  
      é™ä½æ•°æ®ç¨€ç–æ€§
    - reduce computational cost  
      é™ä½è®¡ç®—æˆæœ¬

  ![Dimensionality Reduction_1 .png](Images/Dimensionality%20Reduction_1%20.png)

- Reinforcement learning  
  å¼ºåŒ–å­¦ä¹ 
  - Decision process  
    åˆ¤å®šè¿‡ç¨‹
  - Reward system  
    åé¦ˆç³»ç»Ÿ
  - Learn series of actions  
    å­¦ä¹ ä¸€ç³»åˆ—åŠ¨ä½œ
  - Applications: chess, video games, some robots, self-driving cars  
    åº”ç”¨ç¨‹åºï¼šå›½é™…è±¡æ£‹ï¼Œç”µå­æ¸¸æˆï¼Œä¸€äº›æœºå™¨äººï¼Œè‡ªåŠ¨é©¾é©¶æ±½è½¦

![Reinforcement learning_1.png](Images/Reinforcement%20learning_1.png)

### 1.2 Supervised learning workflow

![Supervised learning workflow_1.png](Images/Supervised%20learning%20workflow_1.png)  
![Supervised learning workflow_2.png](Images/Supervised%20learning%20workflow_2.png)

!Note: Training Data is used to build up the model and Test DAta is used
to test the model

![Some algorithms_1.png](Images/Some%20algorithms_1.png)

### 1.3 Model evaluation

- misclassification error  
  é”™è¯¯åˆ†ç±»é”™è¯¯

![Model evaluation_1.png](Images/Model%20evaluation_1.png)

- other metrics  
  å…¶ä»–æŒ‡æ ‡
  - Accuracy (1-Error)
  - ROC, AUC
  - Precision, Recall
  - F-measure, G-mean
  - (Cross) Entropy
  - Likelihood
  - Squared Error/MSE
  - R2

## 2. Hierarchical Clustering

### 2.1 Clustering concepts

- Segment data into clusters, such that there is  
  å°†æ•°æ®åˆ†å‰²æˆé›†ç¾¤ï¼Œè¿™æ ·æœ‰
  - high intra-cluster similarity  
    é«˜èšç±»å†…ç›¸ä¼¼æ€§
  - low inter-cluster similarity  
    ä½èšç±»é—´ç›¸ä¼¼æ€§
- informally, finding natural groupings among objects  
  éæ­£å¼åœ°ï¼Œåœ¨ç‰©ä½“ä¹‹é—´å¯»æ‰¾è‡ªç„¶çš„åˆ†ç»„ã€‚

- Clustering set-up  
  èšç±»è®¾ç½®
  - Our data are: D = {x<sub>1</sub>, . . . , x<sub>N</sub>}.
  - Each data point is m-dimensional, i.e.  
    x<sub>i</sub> = <x<sub>i,1</sub>, . . . , x<sub>i,m</sub>>
  - Define a distance function (i.e. similarity measures) between data,  
    d(x<sub>i</sub>, x<sub>j</sub>)
  - Goal: segment xn into k groups  
    {z<sub>1</sub>, . . . , z<sub>N</sub>} where z<sub>i</sub> âˆˆ {1, . .
    . ,K}

- Similarity Measures
  - Between any two data samples p and q, we can calculate their
    distance d(p,q) using a number of measurements:  
    ![Similarity Measures_1.png](Images/Similarity%20Measures_1.png)

- Types of Clustering Algorithms  
  èšç±»ç®—æ³•çš„ç±»å‹
  - Partitional clustering, e.g. K-means, K-medoids  
    åˆ†åŒºèšç±»ï¼Œä¾‹å¦‚K-meansï¼ŒK-medoids
  - Hierarchical clustering  
    åˆ†å±‚èšç±»
    - Bottom-up(agglomerative)  
      è‡ªä¸‹è€Œä¸Šï¼ˆå‡èšï¼‰
    - Top-down  
      è‡ªä¸Šè€Œä¸‹
  - Density-based clustering, e.g. DBScan  
    åŸºäºå¯†åº¦çš„èšç±»ï¼Œä¾‹å¦‚DBScan
  - Mixture density based clustering åŸºäºæ··åˆå¯†åº¦çš„èšç±»
  - Fuzzy theory based, graph theory based, grid based, etc.  
    åŸºäºæ¨¡ç³Šç†è®ºã€åŸºäºå›¾è®ºç†è®ºã€åŸºäºç½‘æ ¼ç†è®ºç­‰

### 2.2 Hierarchical clustering

- Create a hierarchical decomposition of the set of objects using some
  criterion  
  ä½¿ç”¨æŸç§æ ‡å‡†åˆ›å»ºå¯¹è±¡é›†çš„åˆ†å±‚åˆ†è§£
- Produce a dendrogram  
  ç”Ÿæˆæ ‘çŠ¶å›¾

![Hierarchical clustering_1.png](Images/Hierarchical%20clustering_1.png)

- Agglomerative clustering illustration  
  å‡èšæ€§çš„èšç±»è¯´æ˜
  - Place each data point into its own singleton group  
    å°†æ¯ä¸ªæ•°æ®ç‚¹æ”¾åˆ°å®ƒè‡ªå·±çš„å•ä¾‹ç»„ä¸­
  - Repeat: iteratively merge the two closest groups  
    é‡å¤ï¼šè¿­ä»£åœ°åˆå¹¶ä¸¤ä¸ªæœ€è¿‘çš„ç»„
  - Until: all the data are merged into a single cluster  
    ç›´åˆ°ï¼šå°†æ‰€æœ‰æ•°æ®éƒ½åˆå¹¶ä¸ºå•ä¸ªé›†ç¾¤

- Output: a dendrogram  
  è¾“å‡ºï¼šæ ‘çŠ¶å›¾
- Reply on: a distance metric between clusters  
  å›å¤ï¼šé›†ç¾¤ä¹‹é—´çš„è·ç¦»åº¦é‡

![Agglomerative clustering illustration_1.png](Images/Agglomerative%20clustering%20illustration_1.png)

- Measuring Distance between clusters
  - Single linkage  
    å•è¿é”
    - the similarity of the closest pair  
      æœ€è¿‘çš„ä¸€å¯¹ä¹‹é—´çš„ç›¸ä¼¼æ€§

  ![Measuring Distance between clusters_1.png](Images/Measuring%20Distance%20between%20clusters_1.png)

  - Complete linkage  
    å®Œå…¨è¿é”
    - the similarity of the furthest pair æœ€è¿œçš„ä¸€å¯¹ä¹‹é—´çš„ç›¸ä¼¼æ€§

  ![Measuring Distance between clusters_2.png](Images/Measuring%20Distance%20between%20clusters_2.png)

  - Group average  
    ç»„å¹³å‡å€¼
    - the average similarity of all pairs  
      æ‰€æœ‰æˆå¯¹çš„å¹³å‡ç›¸ä¼¼åº¦
    - more widely used  
      æ›´å¹¿æ³›åœ°ä½¿ç”¨
    - robust against noise æŠ—å™ªå£°å¼º

  ![Measuring Distance between clusters_3.png](Images/Measuring%20Distance%20between%20clusters_3.png)

- Strengths, weaknesses, caveats  
  ä¼˜åŠ¿ã€å¼±ç‚¹å’Œæ³¨æ„äº‹é¡¹
  - Strengths  
    ä¼˜åŠ¿
    - provides deterministic results  
      æä¾›ç¡®å®šæ€§ç»“æœ
    - no need to specify number of clusters beforehand  
      ä¸éœ€è¦é¢„å…ˆæŒ‡å®šé›†ç¾¤çš„æ•°é‡
    - can create clusters of arbitrary shapes  
      å¯ä»¥åˆ›å»ºä»»æ„å½¢çŠ¶çš„é›†ç¾¤å—
  - Weakness  
    ç¼ºç‚¹
    - does not scale up for large datasets, time complexity at least
      O(n<sup>2</sup>)  
      ä¸å¯ä»¥æ‰©å±•åˆ°å¤§å‹æ•°æ®é›†ï¼Œæ—¶é—´å¤æ‚åº¦è‡³å°‘ä¸ºO(n<sup>2</sup>)
  - Caveats  
    æ³¨æ„äº‹é¡¹
    - Different decisions about group similarities can lead to vastly
      different dendrograms.  
      å…³äºç¾¤ä½“ç›¸ä¼¼æ€§çš„ä¸åŒå†³å®šå¯èƒ½ä¼šå¯¼è‡´æˆªç„¶ä¸åŒçš„æ ‘çŠ¶å›¾
    - The algorithm imposes a hierarchical structure on the data, even
      data for which such structure is not appropriate.
      è¯¥ç®—æ³•å¯¹æ•°æ®å¼ºåŠ äº†ä¸€ä¸ªåˆ†å±‚ç»“æ„ï¼Œå³ä½¿æ˜¯è¿™ç§ç»“æ„ä¸åˆé€‚çš„æ•°æ®

## 3. K-means

### 3.1 K-means

- Centroid-based: describe each cluster by its mean  
  åŸºäºè´¨å¿ƒçš„ï¼šç”¨å®ƒçš„å¹³å‡å€¼æ¥æè¿°æ¯ä¸ªèšç±»
- Goal: assign data to K.  
  ç›®æ ‡ï¼šå°†æ•°æ®åˆ†é…ç»™K
- Algorithm objective: minimize the within-cluster variances of all
  clusters.  
  ç®—æ³•ç›®æ ‡ï¼šæœ€å°åŒ–æ‰€æœ‰èšç±»çš„ç°‡å†…æ–¹å·®

- A non-deterministic method  
  éç¡®å®šæ€§æ–¹æ³•
- Finds a local optimal result (multiple restarts are often necessary)  
  æ‰¾åˆ°å±€éƒ¨æœ€ä¼˜ç»“æœï¼ˆé€šå¸¸éœ€è¦å¤šæ¬¡é‡å¯ï¼‰

- Algorithm description  
  ![Algorithm description_1.png](Images/Algorithm%20description_1.png)

## 4. GMM/EM

### 4.1 Gaussian mixture models(GMMs)

- Assume data was generated by a set of Gaussian distributions  
  å‡è®¾æ•°æ®æ˜¯ç”±ä¸€ç»„é«˜æ–¯åˆ†å¸ƒç”Ÿæˆçš„
- The probability density is a mixture of them æ¦‚ç‡å¯†åº¦æ˜¯å®ƒä»¬çš„æ··åˆç‰©
- Find the parameters of the Gaussian distributions and how much each
  distribution contributes to the data  
  æ‰¾å‡ºé«˜æ–¯åˆ†å¸ƒçš„å‚æ•°ä»¥åŠæ¯ä¸ªåˆ†å¸ƒå¯¹æ•°æ®çš„è´¡çŒ®ç¨‹åº¦
- This is a mixture model of Gaussian  
  è¿™æ˜¯ä¸€ä¸ªé«˜æ–¯åˆ†å¸ƒçš„æ··åˆæ¨¡å‹

- Generative Models  
  ç”Ÿæˆæ¨¡å‹
  - In supervised learning, we model the joint distribution  
    åœ¨ç›‘ç£å­¦ä¹ ä¸­ï¼Œæˆ‘ä»¬å»ºç«‹äº†è”åˆåˆ†å¸ƒçš„æ¨¡å‹  
    ![Generative Models_1.png](Images/Generative%20Models_1.png)
  - In unsupervised learning, we do not have labels z, we model  
    åœ¨æ— ç›‘ç£å­¦ä¹ ä¸­ï¼Œæˆ‘ä»¬æ²¡æœ‰æ ‡ç­¾zï¼Œæˆ‘ä»¬å»ºæ¨¡  
    ![Generative Models_2.png](Images/Generative%20Models_2.png)

- A GMM represents a distributions as  
  ä¸€ä¸ªGMMè¡¨ç¤ºä¸€ä¸ªåˆ†å¸ƒä¸º  
  ![GMMs_1.png](Images/GMMs_1.png)
- with Ï€<sub>k</sub> the mixing coefficients, where  
  ä¸Ï€<sub>k</sub>çš„æ··åˆç³»æ•°ï¼Œå…¶ä¸­  
  ![GMMs_2.png](Images/GMMs_2.png)
- GMM is a density estimator  
  GMMæ˜¯ä¸€ä¸ªå¯†åº¦ä¼°è®¡å™¨
- GMM is universal approximators of densities (if you have enough
  Gaussians)  
  GMMæ˜¯å¯†åº¦çš„é€šç”¨è¿‘ä¼¼å™¨ï¼ˆå¦‚æœä½ æœ‰è¶³å¤Ÿçš„é«˜æ–¯åˆ†å¸ƒï¼‰

- To have a model best fit data, we need to maximize the (log)
  likelihood  
  ä¸ºäº†å¾—åˆ°ä¸€ä¸ªæ¨¡å‹çš„æœ€ä½³æ‹Ÿåˆæ•°æ®ï¼Œæˆ‘ä»¬éœ€è¦æœ€å¤§åŒ–ï¼ˆå¯¹æ•°ï¼‰çš„å¯èƒ½æ€§  
  ![GMMs_3.png](Images/GMMs_3.png)
- Expectation: if we knew Ï€<sub>k</sub>, Âµ and âˆ‘ , we can get â€œsoftâ€
  Z<sub>k</sub> P(z<sup>(n)</sup>|x) - responsibility
- Maximization: if we know Z<sub>k</sub>, we can get Ï€<sub>k</sub>, Âµ
  and âˆ‘

- GMM model has 3 parameters in total to optimise:  
  GMMæ¨¡å‹å…±æœ‰3ä¸ªå‚æ•°è¿›è¡Œä¼˜åŒ–ï¼š
  - the mean vectors of each component(mu)  
    æ¯ä¸ªåˆ†é‡çš„å¹³å‡å‘é‡(mu)
  - the covariances matrices of each component(sigma)  
    æ¯ä¸ªåˆ†é‡çš„åæ–¹å·®çŸ©é˜µ(sigma)
  - the weights associated with each component(pi)  
    ä¸æ¯ä¸ªç»„ä»¶å…³è”çš„æƒé‡(pi)

  - Each iteration of the EM algorithm increases to likelihood of the
    data, unless you happen to be exactly at a local optimum.  
    EMç®—æ³•çš„æ¯æ¬¡è¿­ä»£éƒ½ä¼šå¢åŠ åˆ°æ•°æ®çš„å¯èƒ½æ€§ï¼Œé™¤éä½ ç¢°å·§æ°å¥½å¤„äºå±€éƒ¨æœ€ä¼˜çŠ¶æ€ã€‚

### 4.2 Expectation-Maximization(EM Algorithm)

- An optimization process that alternates between 2 steps:  
  åœ¨ä»¥ä¸‹ä¸¤ä¸ªæ­¥éª¤ä¹‹é—´äº¤æ›¿è¿›è¡Œçš„ä¼˜åŒ–è¿‡ç¨‹ï¼š
  - E-step: compute the posterior probability over z given the current
    model.  
    Eæ­¥ï¼šè®¡ç®—ç»™å®šå½“å‰æ¨¡å‹å¯¹zçš„åéªŒæ¦‚ç‡  
    ![EM_1.png](Images/EM_1.png)
  - M-step: Assuming data was really generated this way, change the
    parameters of each Gaussian to maximize the probability that it
    would generate the data it is currently responsible for.  
    mæ­¥ï¼šå‡è®¾æ•°æ®çœŸçš„æ˜¯ä»¥è¿™ç§æ–¹å¼ç”Ÿæˆçš„ï¼Œé‚£ä¹ˆå°±æ”¹å˜æ¯ä¸ªé«˜æ–¯åˆ†å¸ƒçš„å‚æ•°ï¼Œä»¥æœ€å¤§é™åº¦åœ°æé«˜å®ƒäº§ç”Ÿå®ƒç›®å‰è´Ÿè´£çš„æ•°æ®çš„æ¦‚ç‡
    ![EM_2.png](Images/EM_2.png)

- A general algorithm for optimizing many latent variable models (not
  just for GMMs).  
  ä¸€ç§ç”¨äºä¼˜åŒ–è®¸å¤šæ½œåœ¨å˜é‡æ¨¡å‹çš„é€šç”¨ç®—æ³•(ä¸ä»…ä»…æ˜¯ç”¨äºgmm)
- Iteratively computes a lower bound then optimizes it  
  è¿­ä»£åœ°è®¡ç®—ä¸€ä¸ªä¸‹ç•Œï¼Œç„¶åä¼˜åŒ–å®ƒ
- Converges but maybe to a local minima  
  æ”¶æ•›ï¼Œä½†å¯èƒ½ä¼šæ”¶æ•›åˆ°ä¸€ä¸ªå±€éƒ¨æœ€å°å€¼
- Can use multiple restarts  
  å¯ä»¥ä½¿ç”¨å¤šä¸ªé‡æ–°å¯åŠ¨

### 4.3 Summary

- Clustering  
  èšç±»
  - group similar data points  
    ç»„ç›¸ä¼¼çš„æ•°æ®ç‚¹
  - need a distance measure  
    éœ€è¦ä¸€ä¸ªè·ç¦»æµ‹é‡
- Agglomerative hierarchical clustering  
  å‡èšçš„å±‚æ¬¡èšç±»
  - successively merges similar groups of points  
    ä¾æ¬¡åˆå¹¶ç›¸ä¼¼çš„ç‚¹ç»„
  - build a dendrogram (binary tree)  
    æ„å»ºä¸€ä¸ªæ ‘çŠ¶å›¾ï¼ˆäºŒå‰æ ‘ï¼‰
  - different ways to measure distance between clusters  
    æµ‹é‡é›†ç¾¤ä¹‹é—´è·ç¦»çš„ä¸åŒæ–¹æ³•
- GMM using EM  
  GMMä½¿ç”¨EM
  - build a generative model based on Gaussian distributions  
    å»ºç«‹ä¸€ä¸ªåŸºäºé«˜æ–¯åˆ†å¸ƒçš„ç”Ÿæˆæ¨¡å‹
  - need to pre-define k (number of clusters)  
    éœ€è¦é¢„å…ˆå®šä¹‰kï¼ˆé›†ç¾¤çš„æ•°é‡ï¼‰
  - Using EM to find the best fit of the model  
    åˆ©ç”¨EMæ‰¾åˆ°æ¨¡å‹çš„æœ€ä½³æ‹Ÿåˆæ€§


## 5. DBSCAN

### 5.1 Density-based Clustering - DBSCAN

- Acronym for: Density-based spatial clustering of applications with
  noise  
  ç¼©å†™ï¼šåŸºäºå¯†åº¦çš„å¸¦æœ‰å™ªå£°çš„åº”ç”¨ç¨‹åºçš„ç©ºé—´èšç±»
- Clusters are dense regions in the data space separated by regions of
  lower sample density  
  èšç±»æ˜¯æ•°æ®ç©ºé—´ä¸­ç”±æ ·æœ¬å¯†åº¦è¾ƒä½çš„åŒºåŸŸåˆ†éš”çš„å¯†é›†åŒºåŸŸ
- A cluster is defined as a maximal set of density connected points  
  ä¸€ä¸ªç°‡è¢«å®šä¹‰ä¸ºå¯†åº¦è¿æ¥ç‚¹çš„æœ€å¤§é›†
- Discover clusters of arbitrary shape  
  å‘ç°ä»»æ„å½¢çŠ¶çš„ç°‡

- Define three exclusive types of points  
  å®šä¹‰ä¸‰ç§æ’ä»–æ€§ç±»å‹çš„ç‚¹
  - Core, Border (or Edge) and Noise (or outlier)  
    æ ¸å¿ƒã€è¾¹ç•Œï¼ˆæˆ–è¾¹ç¼˜ï¼‰å’Œå™ªå£°ï¼ˆæˆ–å¼‚å¸¸å€¼ï¼‰

> Core points -- dense region æ ¸å¿ƒç‚¹ï¼Œå¯†é›†åŒºåŸŸ  
> Noise -- sparse region å™ªå£°ç¨€ç–åŒº

![DBSCAN_1.png](Images/DBSCAN_1.png)

- Need two parameters  
  éœ€è¦ä¸¤ä¸ªå‚æ•°
  - a circle of epsilon radius  
    ä¸€ä¸ªåŠå¾„çš„åœ†
  - a circle containing at least minPts number of points  
    ä¸€ä¸ªè‡³å°‘åŒ…å«åˆ†é’Ÿæ•°ç‚¹çš„åœ†

- Three types of points

| core   | The point has at least minPts number of points within Eps<br>è¯¥ç‚¹åœ¨Epså†…è‡³å°‘æœ‰minPtsæ•°é‡çš„ç‚¹æ•°                                                            |
|:-------|:------------------------------------------------------------------------------------------------------------------------------------------------------|
| border | The point has fewer than minPts within Eps, but is in the neighbourhood (i.e. circle) of a core point<br>è¯¥ç‚¹åœ¨Epså†…æ¯”minptå°‘ï¼Œä½†åœ¨ä¸€ä¸ªæ ¸å¿ƒç‚¹çš„é™„è¿‘ï¼ˆå³åœ†åœˆï¼‰ |
| noise  | Any point that is not a core point or a border point. <br>ä»»ä½•ä¸æ˜¯æ ¸å¿ƒç‚¹æˆ–è¾¹ç•Œç‚¹çš„ä»»ä½•ç‚¹                                                                   |

![DBSCAN_3.png](Images/DBSCAN_3.png)

- Density-reachability  
  å¯†åº¦å¯è¾¾æ€§
  - Directly density-reachable: a point q is directly density-reachable
    from point p if p is a core point and q is in pâ€™s neighbourhood  
    ç›´æ¥å¯†åº¦å¯è¾¾ï¼šå¦‚æœpæ˜¯ä¸€ä¸ªæ ¸å¿ƒç‚¹ï¼Œå¹¶ä¸”qåœ¨pçš„é‚»åŸŸå†…ï¼Œåˆ™ä¸€ä¸ªç‚¹qä»ç‚¹pç›´æ¥å¯†åº¦å¯è¾¾

  ![DBSCAN_4.png](Images/DBSCAN_4.png)
  - q is directly density-reachable from p  
    qå¯ä»¥ä»pç›´æ¥å¾—åˆ°å¯†åº¦
  - p is not necessarily directly density-reachable from q  
    pä¸ä¸€å®šèƒ½ä»qä¸­ç›´æ¥è¾¾åˆ°å¯†åº¦
  - Density-reachability is asymmetric  
    å¯†åº¦-å¯è¾¾æ€§æ˜¯ä¸å¯¹ç§°çš„
  - Density-Reachable (directly and indirectly)  
    å¯†åº¦-å¯è¾¾æ€§ï¼ˆç›´æ¥æˆ–é—´æ¥ï¼‰
    - A point p is directly density-reachable from p2  
      ä¸€ä¸ªç‚¹på¯ä»¥ä»p2ç›´æ¥é€šè¿‡å¯†åº¦åˆ°è¾¾
    - p2 is directly density-reachable from p1  
      p2å¯ä»¥ä»p1ç›´æ¥è¾¾åˆ°å¯†åº¦
    - p1 is directly density-reachable from q  
      p1å¯ä»¥ç›´æ¥ä»qè¾¾åˆ°å¯†åº¦
    - q -> p1 -> p2 -> p form a chain(p is the border)  
      q->p1->p2->på½¢æˆä¸€ä¸ªé“¾(pæ˜¯è¾¹ç•Œ)

  ![DBSCAN_5.png](Images/DBSCAN_5.png)
  - p is indirectly density-reachable from q  
    pæ˜¯ä»qé—´æ¥è¾¾åˆ°å¯†åº¦çš„
  - q is not density-reachable from p  
    qä¸èƒ½ä»på¾—åˆ°å¯†åº¦

### 5.2 The algorithm

1. Label all points as core, border or noise.  
   å°†æ‰€æœ‰ç‚¹æ ‡è®°ä¸ºæ ¸å¿ƒã€è¾¹ç•Œæˆ–å™ªå£°
2. Eliminate noise points. æ¶ˆé™¤å™ªå£°ç‚¹
3. For every core point p that has not been assigned to a cluster:  
   å¯¹äºæ²¡æœ‰åˆ†é…ç»™é›†ç¾¤çš„æ¯ä¸ªæ ¸å¿ƒç‚¹p  
   Create a new cluster with the point p and all the points that are
   density-reachable from p  
   ç”¨ç‚¹på’Œæ‰€æœ‰ä»på¯ä»¥è¾¾åˆ°å¯†åº¦çš„ç‚¹åˆ›å»ºä¸€ä¸ªæ–°çš„é›†ç¾¤
4. For border points belonging to more than 1 cluster, assign it to the
   cluster of the closest core point.  
   å¯¹äºå±äº1ä¸ªé›†ç¾¤çš„è¾¹ç•Œç‚¹ï¼Œå°†å…¶åˆ†é…ç»™æœ€è¿‘æ ¸å¿ƒç‚¹çš„é›†ç¾¤ã€‚

- Some key points
  - DBSCAN can find non-linearly separable clusters. (an advantage over
    K-means and GMM)  
    DBSCANå¯ä»¥æ‰¾åˆ°éçº¿æ€§å¯åˆ†çš„ç°‡ã€‚(ç›¸å¯¹äºK-meanså’ŒGMMçš„ä¼˜åŠ¿)
  - Resistant to noise  
    è€å™ªéŸ³
  - Not entirely deterministic: border points that are reachable from
    more than one cluster can be part of either cluster, depending on
    the implementation  
    ä¸å®Œå…¨ç¡®å®šæ€§çš„ï¼šä»å¤šä¸ªé›†ç¾¤å¯è®¿é—®çš„è¾¹ç•Œç‚¹å¯ä»¥æ˜¯ä»»æ„ä¸€ä¸ªé›†ç¾¤çš„ä¸€éƒ¨åˆ†ï¼Œè¿™å–å†³äºå®ç°

- K-means and EM rely on cluster initialisation, and EM also relies on
  gradient descent. Therefore, they are non-deterministic algorithms and
  may get struck at local optima  
  K-meanså’ŒEMä¾èµ–äºèšç±»åˆå§‹åŒ–ï¼Œè€ŒEMä¹Ÿä¾èµ–äºæ¢¯åº¦ä¸‹é™ã€‚å› æ­¤ï¼Œå®ƒä»¬æ˜¯éç¡®å®šæ€§çš„ç®—æ³•ï¼Œå¯èƒ½ä¼šè¾¾åˆ°å±€éƒ¨æœ€ä¼˜

- Gaussian mixture model trained using EM is a soft version of K-means,
  but these two algorithms do not necessarily produce the same cluster
  centres given the same data set.  
  ä½¿ç”¨EMè®­ç»ƒçš„é«˜æ–¯æ··åˆæ¨¡å‹æ˜¯K-meansçš„è½¯ç‰ˆæœ¬ï¼Œä½†è¿™ä¸¤ç§ç®—æ³•åœ¨ç›¸åŒçš„æ•°æ®é›†ä¸‹ä¸ä¸€å®šäº§ç”Ÿç›¸åŒçš„èšç±»ä¸­å¿ƒã€‚
- DBSCAN is capable to discover clusters of any shapes.  
  DBSCANèƒ½å¤Ÿå‘ç°ä»»ä½•å½¢çŠ¶çš„é›†ç¾¤
- Using Gaussian mixture model with Expectation-maximization
  optimization to cluster a data set, the result is non-deterministic
  and may get stuck in local optima.  
  åˆ©ç”¨é«˜æ–¯æ··åˆæ¨¡å‹å’ŒæœŸæœ›æœ€å¤§åŒ–ä¼˜åŒ–å¯¹æ•°æ®é›†è¿›è¡Œèšç±»ï¼Œç»“æœæ˜¯ä¸ç¡®å®šæ€§çš„ï¼Œå¯èƒ½ä¼šé™·å…¥å±€éƒ¨æœ€ä¼˜ã€‚

## 6. Supervised Learning

### 6.1 Supervised Learning

- One of the most prevalent forms of ML  
  MLæœ€æ™®éçš„å½¢å¼ä¹‹ä¸€
  - Teach a computer to do something, then let it use its knowledge to
    do it  
    æ•™ç”µè„‘å»åšæŸä»¶äº‹ï¼Œç„¶åè®©å®ƒç”¨è‡ªå·±çš„çŸ¥è¯†å»åš
- Other forms of ML  
  å…¶ä»–å½¢å¼çš„ML
  - Unsupervised learning  
    æ— ç›‘ç£å­¦ä¹ 
  - Reinforcement learning  
    å¼ºåŒ–å­¦ä¹ 

- Types of supervised learning  
  ç›‘ç£å­¦ä¹ çš„ç±»å‹
  - Regression  
    å›å½’
  - Classification  
    èšç±»
    - Binary
    - Multi-class

### 6.2 Training data

- Supervised learning needs annotated data for training:  
  ç›‘ç£å­¦ä¹ éœ€è¦åŸ¹è®­çš„æ³¨é‡Šæ•°æ®ï¼š  
  in the form of examples of (Input, Output) pairs  
  ä»¥ï¼ˆè¾“å…¥ã€è¾“å‡ºï¼‰å¯¹çš„ç¤ºä¾‹çš„å½¢å¼å‡ºç°
- After training completed  
  åŸ¹è®­å®Œæˆå
  - you present it with new Input that it hasnâ€™t seen before  
    ä½ ç”¨å®ƒä»¥å‰ä»æœªè§è¿‡çš„æ–°è¾“å…¥æ¥å‘ˆç°å®ƒ
  - It needs to predict the appropriate Output  
    å®ƒéœ€è¦é¢„æµ‹é€‚å½“çš„è¾“å‡º

### 6.3 Terminology in Supervised Learning

- Input = attribute(s) = feature(s) = independent variable
- Output = target = response = dependent variable
- function = hypothesis = predictor

### 6.4 Applications of supervised learning

- Handwriting recognition  
  æ‰‹å†™è¯†åˆ«
  - When you write an envelope, algorithms can automatically route
    envelopes through the post  
    å½“ä½ å†™ä¸€ä¸ªä¿¡å°æ—¶ï¼Œç®—æ³•å¯ä»¥è‡ªåŠ¨é€šè¿‡é‚®ä»¶å‘é€ä¿¡å°
- Computer vision & graphics  
  è®¡ç®—æœºè§†è§‰å’Œå›¾å½¢
  - When you go out during lockdown, object detection & visual tracking
    algorithms can automatically detect compliance with the rules  
    å½“ä½ åœ¨é”å®šæœŸé—´å¤–å‡ºæ—¶ï¼Œç›®æ ‡æ£€æµ‹å’Œè§†è§‰è·Ÿè¸ªç®—æ³•å¯ä»¥è‡ªåŠ¨æ£€æµ‹åˆ°æ˜¯å¦ç¬¦åˆè§„åˆ™
- Bioinformatics  
  ç”Ÿç‰©
  - Algorithms can predict protein function from sequence  
    ç®—æ³•å¯ä»¥ä»åºåˆ—ä¸­é¢„æµ‹è›‹ç™½è´¨çš„åŠŸèƒ½
- Human-computer interaction  
  äººæœºäº’åŠ¨
  - Intrusion detection algorithms can recognise speech, gestures,
    intention  
    å…¥ä¾µæ£€æµ‹ç®—æ³•å¯ä»¥è¯†åˆ«è¯­éŸ³ã€æ‰‹åŠ¿ã€æ„å›¾

## 7. Linear Regression

### 7.1 Regression

- Regression means learning a function that captures the â€œtrendâ€ between
  input and output  
  å›å½’æ„å‘³ç€å­¦ä¹ ä¸€ä¸ªæ•è·è¾“å…¥å’Œè¾“å‡ºä¹‹é—´çš„â€œè¶‹åŠ¿â€çš„å‡½æ•°
- We then use this function to predict target values for new inputs  
  ç„¶åï¼Œæˆ‘ä»¬ä½¿ç”¨è¿™ä¸ªå‡½æ•°æ¥é¢„æµ‹æ–°è¾“å…¥çš„ç›®æ ‡å€¼

### 7.2 Univariate linear regression

- Visually, there appears to be a trend  
  ä»è§†è§‰ä¸Šçœ‹ï¼Œä¼¼ä¹æœ‰ä¸€ç§è¶‹åŠ¿
- A reasonable **model** seems to be the **class of linear functions
  (lines)**  
  ä¸€ä¸ªåˆç†çš„æ¨¡å‹ä¼¼ä¹æ˜¯ä¸€ç±»çº¿æ€§å‡½æ•°ï¼ˆçº¿ï¼‰
- We have one input attribute (year) - hence the name **univariate**  
  æˆ‘ä»¬æœ‰ä¸€ä¸ªè¾“å…¥å±æ€§ï¼ˆå¹´ä»½ï¼‰ï¼Œå› æ­¤å®ƒè¢«å‘½åä¸ºå•å˜é‡

![LinearRegression_1.png](Images/LinearRegression_1.png)

- Any line is described by this equation by specifying values for ğ‘¤1,
  ğ‘¤0.

### 7.3 Loss functions (or cost functions)

- We need a criterion that, given the data, for any given line will tell
  us how bad is that line.  
  æˆ‘ä»¬éœ€è¦ä¸€ä¸ªæ ‡å‡†ï¼Œç»™å®šæ•°æ®ï¼Œå¯¹äºä»»ä½•ç»™å®šçš„çº¿éƒ½ä¼šå‘Šè¯‰æˆ‘ä»¬è¿™æ¡çº¿æœ‰å¤šç³Ÿç³•
- Such criterion is called a loss function. It is a function of the free
  parameters!  
  è¿™ç§å‡†åˆ™è¢«ç§°ä¸ºæŸå¤±å‡½æ•°ã€‚å®ƒæ˜¯ä¸€ä¸ªè‡ªç”±å‚æ•°çš„å‡½æ•°
- Loss function = cost function = loss = cost = error function

![Loss functions_1.png](Images/Loss%20functions_1.png)

- Square loss(L2 loss)
  - The loss expresses an error, so it must be always non-negative  
    æŸå¤±è¡¨ç¤ºä¸€ä¸ªé”™è¯¯ï¼Œæ‰€ä»¥å®ƒå¿…é¡»æ€»æ˜¯æ˜¯éè´Ÿçš„
  - Square loss is a sensible choice to measure mismatch for regression  
    å¹³æ–¹æŸå¤±æ˜¯è¡¡é‡å›å½’ä¸åŒ¹é…çš„åˆç†é€‰æ‹©
  - Mean Square Error (MSE)å¹³å‡å¹³æ–¹è¯¯å·®(MSE)  
    ![Loss functions_2.png](Images/Loss%20functions_2.png)

![Loss functions_3.png](Images/Loss%20functions_3.png)

### 7.4 what we want to do

- Given training data  
  ![Loss functions_4.png](Images/Loss%20functions_4.png)
- Fit the model  
  ![Loss functions_5.png](Images/Loss%20functions_5.png)
- By minimising the cost function  
  ![Loss functions_6.png](Images/Loss%20functions_6.png)

- Every combination of w0 and w1 has an associated cost  
  w0å’Œw1çš„æ¯ä¸ªç»„åˆéƒ½æœ‰ä¸€ä¸ªç›¸å…³çš„æˆæœ¬
- To find the â€˜best fitâ€™ we need to find values for w0 and w1 such that
  the cost is minimum.  
  ä¸ºäº†æ‰¾åˆ°â€œæœ€ä½³æ‹Ÿåˆâ€ï¼Œæˆ‘ä»¬éœ€è¦æ‰¾åˆ°w0å’Œw1çš„å€¼ï¼Œä»è€Œä½¿æˆæœ¬æœ€å°

### 7.5 Gradient Descent

- A general strategy to minimise cost functions  
  ä¸€ç§æœ€å°åŒ–æˆæœ¬å‡½æ•°çš„ä¸€èˆ¬ç­–ç•¥
- Goal: Minimise cost function ğ‘”(ğ‘¤), where ğ’˜ =(ğ‘¤0, ğ‘¤1, , â€¦)  
  ç›®æ ‡ï¼šæœ€å°åŒ–æˆæœ¬å‡½æ•°ï¼Œğ‘”(ğ‘¤)  
  ![Gradient Descent_1.png](Images/Gradient%20Descent_1.png)
- Î± is called â€œlearning rateâ€= â€œstep sizeâ€

- If the value of alpha is too high, Gradient Descent will never reach
  the minimum  
  å¦‚æœalphaçš„å€¼è¿‡é«˜ï¼Œæ¢¯åº¦ä¸‹é™å°†æ°¸è¿œä¸ä¼šè¾¾åˆ°æœ€å°å€¼

### 7.6 Gradient

- Partial derivative with respect to ğ‘¤0
  is![Gradient_1.png](Images/Gradient_1.png)It means the derivative
  function of ğ‘”(ğ‘¤0, ğ‘¤1) when ğ‘¤1 is treated as constant.
- Partial derivative with respect to ğ‘¤1
  is![Gradient_2.png](Images/Gradient_2.png)It means the derivative
  function of ğ‘”(ğ‘¤0, ğ‘¤1) when ğ‘¤0 is treated as constant.
- The vector of partial derivatives is called the gradient  
  åå¯¼æ•°çš„å‘é‡ç§°ä¸ºæ¢¯åº¦ ![Gradient_3.png](Images/Gradient_3.png)
- The negative of the gradient evaluated at a location (ğ‘¤0, ğ‘¤1) gives
  us the direction of the steepest descent from that location.  
  åœ¨ä¸€ä¸ªä½ç½®ï¼ˆğ‘¤0ï¼Œğ‘¤1ï¼‰ä¸Šè®¡ç®—çš„æ¢¯åº¦çš„è´Ÿå€¼ç»™å‡ºäº†ä»è¯¥ä½ç½®æœ€é™¡ä¸‹é™çš„æ–¹å‘

- Computing the gradient for our L2 loss  
  è®¡ç®—L2æŸå¤±çš„æ¢¯åº¦  
  ![Gradient_4.png](Images/Gradient_4.png)
- Algorithm for univariate linear regression using GD  
  åŸºäºGDçš„å•å˜é‡çº¿æ€§å›å½’ç®—æ³•  
  ![Gradient_5.png](Images/Gradient_5.png)

- Multivariate linear regression  
  å¤šå…ƒçº¿æ€§å›å½’  
  ![Gradient_6.png](Images/Gradient_6.png)

- Univariate nonlinear regression  
  å•å˜é‡éçº¿æ€§å›å½’  
  ![Gradient_7.png](Images/Gradient_7.png)

- Advantages of vector notation  
  çŸ¢é‡ç¬¦å·çš„ä¼˜ç‚¹
  - Vector notation in concise  
    å‘é‡ç¬¦å·ç®€æ˜
  - With the vectors ğ’˜ and ğ± populated appropriately (and differently
    in each case, as on the previous 2 slides), these models are still
    linear in the parameter vector.  
    ç”±äºå‘é‡ğ’˜å’Œğ±é€‚å½“å¡«å……ï¼ˆæ¯ç§æƒ…å†µä¸‹éƒ½ä¸åŒï¼Œå¦‚å‰ä¸¤ä¸ªå¹»ç¯ç‰‡ï¼‰ï¼Œè¿™äº›æ¨¡å‹åœ¨å‚æ•°å‘é‡ä¸­ä»ç„¶æ˜¯çº¿æ€§çš„ã€‚
  - The cost function is the L2 as before  
    æˆæœ¬å‡½æ•°å’Œå‰é¢ä¸€æ ·æ˜¯l2
  - So the gradient in both cases
    is:![Gradient_8.png](Images/Gradient_8.png)
  - Ready to be plugged into the general gradient descent algorithm  
    å‡†å¤‡å¥½è¢«æ’å…¥åˆ°ä¸€èˆ¬çš„æ¢¯åº¦ä¸‹é™ç®—æ³•ä¸­

- x is independent variables
- w is free parameters(weights)

- Note: The choice of learning rate alpha depends upon dataset and
  hypothesis function. Thus, without any further known details and given
  an arbitrary choice of alpha, it cannot be estimated whether gradient
  descent will converge or not.  
  æ³¨ï¼šå­¦ä¹ ç‡alphaçš„é€‰æ‹©å–å†³äºæ•°æ®é›†å’Œå‡è®¾å‡½æ•°ã€‚å› æ­¤ï¼Œå¦‚æœæ²¡æœ‰ä»»ä½•å·²çŸ¥ç»†èŠ‚å’Œç»™å®šä»»æ„é€‰æ‹©ï¼Œå°±ä¸èƒ½ä¼°è®¡æ¢¯åº¦ä¸‹é™æ˜¯å¦ä¼šæ”¶æ•›ã€‚

## 8. Logistic Regression

### 8.1 Logistic regression

- It is a linear model for classification (contrary to its name!)  
  å®ƒæ˜¯ä¸€ä¸ªåˆ†ç±»çš„çº¿æ€§æ¨¡å‹ï¼ˆä¸å®ƒçš„åå­—ç›¸åï¼ï¼‰

- In regression, the targets are real values  
  åœ¨å›å½’ä¸­ï¼Œç›®æ ‡æ˜¯çœŸå®çš„å€¼
- In classification, the targets are categories, and they are called
  labels  
  åœ¨åˆ†ç±»ä¸­ï¼Œç›®æ ‡æ˜¯ç±»åˆ«ï¼Œå®ƒä»¬è¢«ç§°ä¸ºæ ‡ç­¾

### 8.2 Model formulation

- We want to put a boundary between 2 classes  
  æˆ‘ä»¬æƒ³åœ¨ä¸¤ä¸ªç±»ä¹‹é—´è®¾ç½®ä¸€ä¸ªç•Œé™
- If x has a single attribute, we can do it with a point  
  å¦‚æœxæœ‰ä¸€ä¸ªå•ä¸€çš„å±æ€§ï¼Œæˆ‘ä»¬å¯ä»¥ç”¨ä¸€ä¸ªç‚¹æ¥å®Œæˆå®ƒ  
  ![Logistic Regression_1.png](Images/Logistic%20Regression_1.png)
- If x has 2 attributes, we can do it with a line  
  å¦‚æœxæœ‰ä¸¤ä¸ªå±æ€§ï¼Œæˆ‘ä»¬å¯ä»¥ç”¨ä¸€è¡Œæ¥åš  
  ![Logistic Regression_2.png](Images/Logistic%20Regression_2.png)
- If x has 3 attributes, we can do it with a plane  
  å¦‚æœxæœ‰3ä¸ªå±æ€§ï¼Œæˆ‘ä»¬å¯ä»¥ç”¨ä¸€ä¸ªå¹³é¢æ¥åš
- If x has more than 3 attributes, we can do it with a hyperplane (canâ€™t
  draw it anymore)  
  å¦‚æœxæœ‰è¶…è¿‡3ä¸ªå±æ€§ï¼Œæˆ‘ä»¬å¯ä»¥ç”¨ä¸€ä¸ªè¶…å¹³é¢æ¥å®Œæˆå®ƒï¼ˆä¸èƒ½å†ç»˜åˆ¶å®ƒäº†ï¼‰
- If the classes are linearly separable, the training error will be 0  
  å¦‚æœè¿™äº›ç±»æ˜¯çº¿æ€§å¯åˆ†çš„ï¼Œåˆ™è®­ç»ƒè¯¯å·®å°†ä¸º0

![Model formulation_1.png](Images/Model%20formulation_1.png)

![Model formulation_2.png](Images/Model%20formulation_2.png)

- The sigmoid function takes a single argument (note, ğ’˜<sup>ğ‘‡</sup>ğ’™
  is one number).  
  så‹å‡½æ•°é‡‡ç”¨å•ä¸ªå‚æ•°ï¼ˆæ³¨æ„ï¼Œğ’˜<sup>ğ‘‡</sup>ğ’™æ˜¯ä¸€ä¸ªæ•°å­—ï¼‰
- It always returns a value between 0 and 1. The meaning of this value
  is the probability that the label is 1  
  å®ƒæ€»æ˜¯è¿”å›ä¸€ä¸ªä»‹äº0åˆ°1ä¹‹é—´çš„å€¼ã€‚è¿™ä¸ªå€¼çš„å«ä¹‰æ˜¯æ ‡ç­¾ä¸º1çš„æ¦‚ç‡  
  ![Model formulation_3.png](Images/Model%20formulation_3.png)
  - If this is smaller than 0.5 then we predict label 0  
    å¦‚æœè¿™å°äº0.5ï¼Œé‚£ä¹ˆæˆ‘ä»¬é¢„æµ‹æ ‡ç­¾ä¸º0
  - if this is larger than 0.5 then we predict label 1  
    å¦‚æœè¿™å¤§äº0.5ï¼Œé‚£ä¹ˆæˆ‘ä»¬é¢„æµ‹æ ‡ç­¾1
- There is a slim chance that the sigmoid outputs exactly 0.5. The set
  of all possible inputs for which this happens is called the decision
  boundary.  
  sigmoid è¾“å‡ºæ°å¥½ä¸º 0.5 çš„å¯èƒ½æ€§å¾ˆå°ã€‚æ‰€æœ‰å¯èƒ½çš„é›†åˆå‘ç”Ÿè¿™ç§æƒ…å†µçš„è¾“å…¥ç§°ä¸ºå†³ç­–è¾¹ç•Œ

### 8.3 Cost function

- each data point contributes a cost, and the overall cost function is
  the average of these  
  æ¯ä¸ªæ•°æ®ç‚¹è´¡çŒ®ä¸€ä¸ªæˆæœ¬ï¼Œæ€»ä½“æˆæœ¬å‡½æ•°æ˜¯è¿™äº›æˆæœ¬çš„å¹³å‡å€¼
- the cost is a function of the free parameters of the model  
  ä»£ä»·æ˜¯æ¨¡å‹çš„è‡ªç”±å‚æ•°çš„å‡½æ•°

![Cost function_1.png](Images/Cost%20function_1.png)

- Given training data  
  ![Cost function_2.png](Images/Cost%20function_2.png)
- Fit the model  
  ![Cost function_4.png](Images/Cost%20function_4.png)
- By minimising the cross-entropy cost function  
  ![Cost function_3.png](Images/Cost%20function_3.png)

- When the actual output y=0 and the prediction is 1, the logistic
  regression cost function assigns a cost of âˆ å½“å®é™…è¾“å‡º y=0 ä¸”é¢„æµ‹ä¸º 1
  æ—¶ï¼Œé€»è¾‘å›å½’æˆæœ¬å‡½æ•°åˆ†é…çš„æˆæœ¬ä¸º âˆ

### 8.4 Learning algorithm by gradient descent

- We use gradient descent (again!) to minimise the cost function, i.e.
  to find the best weight values.  
  æˆ‘ä»¬ä½¿ç”¨æ¢¯åº¦ä¸‹é™ï¼ˆå†æ¬¡å¦‚æ­¤ï¼ï¼‰ä½¿æˆæœ¬å‡½æ•°æœ€å°åŒ–ï¼Œå³æ‰¾åˆ°æœ€ä½³çš„æƒé‡å€¼
- The gradient vector is:  
  æ¢¯åº¦å‘é‡ä¸º  
  ![Learning algorithm by gradient descent_1.png](Images/Learning%20algorithm%20by%20gradient%20descent_1.png)  
  ![Learning algorithm by gradient descent_2.png](Images/Learning%20algorithm%20by%20gradient%20descent_2.png)

- Learning algorithm for logistic regression  
  ![Learning algorithm by gradient descent_3.png](Images/Learning%20algorithm%20by%20gradient%20descent_3.png)

- Nonlinear logistic regression: instead of linear function inside the
  exp in the sigmoid, we can use polynomial functions of the input
  attributes  
  éçº¿æ€§é€»è¾‘å›å½’ï¼šæˆ‘ä»¬å¯ä»¥ä½¿ç”¨è¾“å…¥å±æ€§çš„å¤šé¡¹å¼å‡½æ•°ï¼Œè€Œä¸æ˜¯så‹expä¸­çš„çº¿æ€§å‡½æ•°
- Multi-class logistic regression: uses a multi-valued version of
  sigmoid  
  å¤šç±»é€»è¾‘å›å½’ï¼šä½¿ç”¨å¤šå€¼ç‰ˆæœ¬çš„så‹ç®—æ³•

- Examples of application of logistic regression  
  é€»è¾‘å›å½’çš„åº”ç”¨ä¾‹å­
  - Face detection: classes consist of images that contain a face and
    images without a face
  - Sentiment analysis: classes consist of written product-reviews
    expressing a positive or a negative opinion
  - Automatic diagnosis of medical conditions: classes consist of
    medical data of patients who either do or do not have a specific
    disease

## 9. Neural Networks

### 9.1 Neural Networks

- Highly nonlinear models having many free parameters  
  å…·æœ‰è®¸å¤šè‡ªç”±å‚æ•°çš„é«˜åº¦éçº¿æ€§æ¨¡å‹
- Can be used for either regression and classification depending on the
  choice of loss function  
  å¯æ ¹æ®æŸå¤±å‡½æ•°çš„é€‰æ‹©è¿›è¡Œå›å½’å’Œåˆ†ç±»
- Can replace nonlinear regression and nonlinear logistic regression
  which are less practical  
  å¯ä»¥ä»£æ›¿ä¸å¤ªå®ç”¨çš„éçº¿æ€§å›å½’å’Œéçº¿æ€§é€»è¾‘å›å½’

1. Model formulation

- Sometimes called â€œarchitectureâ€  
  æœ‰æ—¶ä¹Ÿè¢«ç§°ä¸ºâ€œå»ºç­‘â€
- Designing this for the problem at hand is the main challenge  
  é’ˆå¯¹å½“å‰çš„é—®é¢˜è®¾è®¡è¿™ä¸ªæ–¹æ¡ˆæ˜¯ä¸»è¦çš„æŒ‘æˆ˜

2. Cost function

- for regression: Mean square error between predictions and observed
  targets  
  å›å½’ï¼šé¢„æµ‹å’Œè§‚æµ‹ç›®æ ‡ä¹‹é—´çš„å‡æ–¹è¯¯å·®
- for classification: Logistic loss (also called cross-entropy)  
  ç”¨äºåˆ†ç±»ï¼šLogisticæŸå¤±ï¼ˆä¹Ÿç§°ä¸ºäº¤å‰ç†µï¼‰

3. Learning algorithm by gradient descent

- The update rules are non-trivial, because the models are much more
  complex  
  æ›´æ–°è§„åˆ™ä¸ç®€å•ï¼Œå› ä¸ºæ¨¡å‹è¦å¤æ‚å¾—å¤š
- It is performed by an algorithm called â€œBackpropagationâ€  
  å®ƒæ˜¯ç”±ä¸€ç§å«åšâ€œåå‘ä¼ æ’­â€çš„ç®—æ³•æ¥æ‰§è¡Œçš„
- Conceptually, each iteration of Backprop takes a gradient descent step  
  ä»æ¦‚å¿µä¸Šè®²ï¼Œæ¯ä¸€æ¬¡çš„åpropè¿­ä»£éƒ½é‡‡å–ä¸€ä¸ªæ¢¯åº¦ä¸‹é™æ­¥éª¤
- Implementations exist that are able to compute the grandient
  automatically  
  å­˜åœ¨ç€èƒ½å¤Ÿè‡ªåŠ¨è®¡ç®—å®ä¼Ÿå»ºç­‘çš„å®ç°
- To update the weights of the Neural Network  
  æ›´æ–°ç¥ç»ç½‘ç»œçš„æƒé‡

- Building blocks of a feedforward neural net  
  å‰é¦ˆç¥ç»ç½‘ç»œçš„æ„ä»¶
  - Each node is one unit or neuron  
    æ¯ä¸ªèŠ‚ç‚¹æ˜¯ä¸€ä¸ªå•ä½æˆ–ç¥ç»å…ƒ
  - Each arrow is a connection with a weight  
    æ¯ä¸ªç®­å¤´éƒ½æ˜¯ä¸€ä¸ªå¸¦æœ‰ä¸€ä¸ªé‡é‡çš„è¿æ¥ç‚¹
  - Nodes are arranged in layers  
    èŠ‚ç‚¹è¢«åˆ†å±‚æ’åˆ—
    - One input layer
    - One output layer
    - Any number of hidden layers (0,1,2,â€¦)
  - Hidden & output nodes typically apply a sigmoid, or other activation
    function  
    éšè—å’Œè¾“å‡ºèŠ‚ç‚¹é€šå¸¸åº”ç”¨så‹èŠ‚ç‚¹æˆ–å…¶ä»–æ¿€æ´»å‡½æ•°

- Simplest neural net  
  æœ€ç®€å•çš„ç¥ç»ç½‘ç»œ
  - A neural net with 0 hidden layers is called a perceptron  
    ä¸€ä¸ªå…·æœ‰0ä¸ªéšè—å±‚çš„ç¥ç»ç½‘ç»œè¢«ç§°ä¸ºæ„ŸçŸ¥å™¨
  - If the activation function is the sigmoid, then this model is
    equivalent to a logistic regression  
    å¦‚æœæ¿€æ´»å‡½æ•°æ˜¯så‹çš„ï¼Œé‚£ä¹ˆè¿™ä¸ªæ¨¡å‹å°±ç­‰ä»·äºä¸€ä¸ªé€»è¾‘å›å½’  
    ![Neural Networks_1.png](Images/Neural%20Networks_1.png)
    - The type of computation performed by each non-input node is the
      same in multi-layer networks too.  
      åœ¨å¤šå±‚ç½‘ç»œä¸­ï¼Œæ¯ä¸ªéè¾“å…¥èŠ‚ç‚¹æ‰€æ‰§è¡Œçš„è®¡ç®—ç±»å‹ä¹ŸåŒæ ·ç›¸åŒ
    - The choice of activation function can be different  
      æ¿€æ´»å‡½æ•°çš„é€‰æ‹©å¯èƒ½ä¼šæœ‰æ‰€ä¸åŒ

- Multi-layer perceptron  
  å¤šå±‚æ„ŸçŸ¥æœº
  - When we have one hidden layer, the model is called multi-layer
    perceptron  
    å½“æˆ‘ä»¬æœ‰ä¸€ä¸ªéšè—å±‚æ—¶ï¼Œè¯¥æ¨¡å‹è¢«ç§°ä¸ºå¤šå±‚æ„ŸçŸ¥å™¨
  - It is a truly non-linear model  
    è¿™æ˜¯ä¸€ä¸ªçœŸæ­£çš„éçº¿æ€§æ¨¡å‹
  - Weights = parameters
  - Number of hidden units, choice of activation function =
    hyperparameters
  - Number of output nodes = number of targets or labels we want to
    predict
  - MLP is more complex, hence it is more flexible  
    MLP æ›´å¤æ‚ï¼Œå› æ­¤æ›´çµæ´»
  - MLP can learn a nonlinear curve  
    MLP å¯ä»¥å­¦ä¹ éçº¿æ€§æ›²çº¿

- Deep neural networks  
  æ·±åº¦ç¥ç»ç½‘ç»œ
  - Very simply, deep learning is machine learning using neural networks
    that have multiple hidden layers  
    å¾ˆç®€å•ï¼Œæ·±åº¦å­¦ä¹ æ˜¯ä¸€ç§ä½¿ç”¨å…·æœ‰å¤šä¸ªéšè—å±‚çš„ç¥ç»ç½‘ç»œçš„æœºå™¨å­¦ä¹ 
  - Number of hidden layers is another hyperparameter  
    éšè—å±‚çš„æ•°é‡æ˜¯å¦ä¸€ä¸ªè¶…å‚æ•°
  - Several hidden layers, several hidden nodes, several hyperparameters  
    å‡ ä¸ªéšè—å±‚ï¼Œå‡ ä¸ªéšè—èŠ‚ç‚¹ï¼Œå‡ ä¸ªè¶…å‚æ•°
  - Mean square error

### 9.2 Overfitting

- learning every irrelevant detail (noise) in a training data set will
  not help  
  åœ¨è®­ç»ƒæ•°æ®é›†ä¸­å­¦ä¹ æ¯ä¸€ä¸ªä¸ç›¸å…³çš„ç»†èŠ‚ï¼ˆå™ªå£°ï¼‰æ˜¯æ²¡æœ‰å¸®åŠ©çš„
- Overfitting happens when the model is more complex than required  
  å½“æ¨¡å‹æ¯”è¦æ±‚çš„æ›´å¤æ‚æ—¶ï¼Œå°±ä¼šå‘ç”Ÿè¿‡æ‹Ÿåˆ
- The error on the test data increases across consecutive epochs whereas
  that on the training data reduces  
  æµ‹è¯•æ•°æ®ä¸Šçš„è¯¯å·®åœ¨è¿ç»­çš„æ—¶æœŸå†…å¢åŠ ï¼Œè€Œè®­ç»ƒæ•°æ®ä¸Šçš„è¯¯å·®å‡å°‘

- Classification  
  ![Overfitting_1.png](Images/Overfitting_1.png)

- Regression  
  ![Overfitting_2.png](Images/Overfitting_2.png)

- Regularisation è§„åˆ™åŒ–
  - One way to guard against overfitting is regularisation  
    é˜²æ­¢è¿‡åº¦æ‹Ÿåˆçš„ä¸€ç§æ–¹æ³•æ˜¯è§„åˆ™åŒ–
  - Add a penalty to the cost function to penalise more complex models  
    åœ¨æˆæœ¬å‡½æ•°ä¸­æ·»åŠ ä¸€ä¸ªæƒ©ç½šï¼Œä»¥æƒ©ç½šæ›´å¤æ‚çš„æ¨¡å‹
  - Prune the model  
    ä¿®å‰ªæ¨¡å‹

- Early stopping æ—©åœ
  - Stopping the training early is another effective way to guard
    against overfitting  
    æå‰åœæ­¢è®­ç»ƒæ˜¯é˜²æ­¢è¿‡åº¦æ‹Ÿåˆçš„å¦ä¸€ç§æœ‰æ•ˆæ–¹æ³•
  - After each gradient update (or Backprop cycle), the training cost
    will decrease until it reaches 0  
    åœ¨æ¯æ¬¡æ¢¯åº¦æ›´æ–°ï¼ˆæˆ–åå‘å¾ªç¯ï¼‰åï¼ŒåŸ¹è®­æˆæœ¬å°†ä¼šä¸‹é™ï¼Œç›´åˆ°è¾¾åˆ°0
  - Set aside a subset of the data (called hold-out set) to use only for
    monitoring the cost on previously unseen data  
    ç•™å‡ºä¸€ä¸ªæ•°æ®å­é›†ï¼ˆç§°ä¸ºä¿ç•™é›†ï¼‰ï¼Œä»…ç”¨äºç›‘è§†ä»¥å‰æœªè§è¿‡çš„æ•°æ®çš„æˆæœ¬
  - The error on hold-out set will decrease at first, but as training
    continues, it can start increasing  
    ä¿ç•™é›†ä¸Šçš„é”™è¯¯ä¸€å¼€å§‹ä¼šå‡å°‘ï¼Œä½†éšç€è®­ç»ƒçš„ç»§ç»­ï¼Œå®ƒå¯èƒ½ä¼šå¼€å§‹å¢åŠ 
  - Stop training when the error on hold-out set starts increasing  
    å½“åœ¨ä¿ç•™é›†ä¸Šçš„é”™è¯¯å¼€å§‹å¢åŠ æ—¶ï¼Œåœæ­¢è®­ç»ƒ


