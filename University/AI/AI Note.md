<!-- TOC start -->
- [AI Note](#ai-note)
  * [1. Machine Learning Basics](#1-machine-learning-basics)
    + [1.1 Categories of machine learning](#11-categories-of-machine-learning)
    + [1.2 Supervised learning workflow](#12-supervised-learning-workflow)
    + [1.3 Model evaluation](#13-model-evaluation)
  * [2. Hierarchical Clustering](#2-hierarchical-clustering)
    + [2.1 Clustering concepts](#21-clustering-concepts)
    + [2.2 Hierarchical clustering](#22-hierarchical-clustering)
  * [3. K-means](#3-k-means)
    + [3.1 K-means](#31-k-means)
  * [4. GMM/EM](#4-gmmem)
    + [4.1 Gaussian mixture models(GMMs)](#41-gaussian-mixture-modelsgmms)
    + [4.2 Expectation-Maximization(EM Algorithm)](#42-expectation-maximizationem-algorithm)
    + [4.3 Summary](#43-summary)
  * [5. DBSCAN](#5-dbscan)
    + [5.1 Density-based Clustering - DBSCAN](#51-density-based-clustering---dbscan)
    + [5.2 The algorithm](#52-the-algorithm)
  * [6. Supervised Learning](#6-supervised-learning)
    + [6.1 Supervised Learning](#61-supervised-learning)
    + [6.2 Training data](#62-training-data)
    + [6.3 Terminology in Supervised Learning](#63-terminology-in-supervised-learning)
    + [6.4 Applications of supervised learning](#64-applications-of-supervised-learning)
  * [7. Linear Regression](#7-linear-regression)
    + [7.1 Regression](#71-regression)
    + [7.2 Univariate linear regression](#72-univariate-linear-regression)
    + [7.3 Loss functions (or cost functions)](#73-loss-functions-or-cost-functions)
    + [7.4 what we want to do](#74-what-we-want-to-do)
    + [7.5 Gradient Descent](#75-gradient-descent)
    + [7.6 Gradient](#76-gradient)
<!-- TOC end -->

# AI Note

## 1. Machine Learning Basics

### 1.1 Categories of machine learning

- Supervised learning  
  æœ‰ç›‘ç£çš„å­¦ä¹ 

  > Labeled data  
  > æ ‡è®°æ•°æ®  
  > Predict outcome/future  
  > é¢„æµ‹ç»“æœ/æœªæ¥

  - Classification: predict categorical class labels  
    åˆ†ç±»:é¢„æµ‹åˆ†ç±»ç±»åˆ«æ ‡ç­¾
    - e.g. the handwritten digit (multi-class)  
      ä¾‹å¦‚ï¼Œæ‰‹å†™çš„æ•°å­—ï¼ˆå¤šç±»ï¼‰

  ![Classification_1.png](Images/Classification_1.png)

  - Regression: Prediction of continuous outcomes  
    å›å½’: å¯¹è¿ç»­ç»“æœçš„é¢„æµ‹
    - e.g. studentsâ€™ grade scores ä¾‹å¦‚å­¦ç”Ÿçš„æˆç»©

  ![Regression_1.png](Images/Regression_1.png)

- Unsupervised learning  
  æ— ç›‘ç£çš„å­¦ä¹ 

  > No labels/targets  
  > æ— æ ‡ç­¾/ç›®æ ‡  
  > Find hidden structure/insights in data  
  > åœ¨æ•°æ®ä¸­æ‰¾åˆ°éšè—çš„ç»“æ„/è§è§£

  - Clustering: Objectives within a cluster share a degree of
    similarity.  
    èšç±»: é›†ç¾¤å†…çš„ç›®æ ‡æœ‰ä¸€å®šç¨‹åº¦çš„ç›¸ä¼¼æ€§
    - e.g. product recommendation  
      ä¾‹å¦‚äº§å“æ¨è

  ![Clustering_1.png](Images/Clustering_1.png)

  - Dimensionality Reduction:  
    é™ç»´
    - reduce data sparsity  
      é™ä½æ•°æ®ç¨€ç–æ€§
    - reduce computational cost  
      é™ä½è®¡ç®—æˆæœ¬

  ![Dimensionality Reduction_1 .png](Images/Dimensionality%20Reduction_1%20.png)

- Reinforcement learning  
  å¼ºåŒ–å­¦ä¹ 
  - Decision process  
    åˆ¤å®šè¿‡ç¨‹
  - Reward system  
    åé¦ˆç³»ç»Ÿ
  - Learn series of actions  
    å­¦ä¹ ä¸€ç³»åˆ—åŠ¨ä½œ
  - Applications: chess, video games, some robots, self-driving cars  
    åº”ç”¨ç¨‹åºï¼šå›½é™…è±¡æ£‹ï¼Œç”µå­æ¸¸æˆï¼Œä¸€äº›æœºå™¨äººï¼Œè‡ªåŠ¨é©¾é©¶æ±½è½¦

![Reinforcement learning_1.png](Images/Reinforcement%20learning_1.png)

### 1.2 Supervised learning workflow

![Supervised learning workflow_1.png](Images/Supervised%20learning%20workflow_1.png)  
![Supervised learning workflow_2.png](Images/Supervised%20learning%20workflow_2.png)

!Note: Training Data is used to build up the model and Test DAta is used
to test the model

![Some algorithms_1.png](Images/Some%20algorithms_1.png)

### 1.3 Model evaluation

- misclassification error  
  é”™è¯¯åˆ†ç±»é”™è¯¯

![Model evaluation_1.png](Images/Model%20evaluation_1.png)

- other metrics  
  å…¶ä»–æŒ‡æ ‡
  - Accuracy (1-Error)
  - ROC, AUC
  - Precision, Recall
  - F-measure, G-mean
  - (Cross) Entropy
  - Likelihood
  - Squared Error/MSE
  - R2

## 2. Hierarchical Clustering

### 2.1 Clustering concepts

- Segment data into clusters, such that there is  
  å°†æ•°æ®åˆ†å‰²æˆé›†ç¾¤ï¼Œè¿™æ ·æœ‰
  - high intra-cluster similarity  
    é«˜èšç±»å†…ç›¸ä¼¼æ€§
  - low inter-cluster similarity  
    ä½èšç±»é—´ç›¸ä¼¼æ€§
- informally, finding natural groupings among objects  
  éæ­£å¼åœ°ï¼Œåœ¨ç‰©ä½“ä¹‹é—´å¯»æ‰¾è‡ªç„¶çš„åˆ†ç»„ã€‚

- Clustering set-up  
  èšç±»è®¾ç½®
  - Our data are: D = {x<sub>1</sub>, . . . , x<sub>N</sub>}.
  - Each data point is m-dimensional, i.e.  
    x<sub>i</sub> = <x<sub>i,1</sub>, . . . , x<sub>i,m</sub>>
  - Define a distance function (i.e. similarity measures) between data,  
    d(x<sub>i</sub>, x<sub>j</sub>)
  - Goal: segment xn into k groups  
    {z<sub>1</sub>, . . . , z<sub>N</sub>} where z<sub>i</sub> âˆˆ {1, . .
    . ,K}

- Similarity Measures
  - Between any two data samples p and q, we can calculate their
    distance d(p,q) using a number of measurements:  
    ![Similarity Measures_1.png](Images/Similarity%20Measures_1.png)

- Types of Clustering Algorithms  
  èšç±»ç®—æ³•çš„ç±»å‹
  - Partitional clustering, e.g. K-means, K-medoids  
    åˆ†åŒºèšç±»ï¼Œä¾‹å¦‚K-meansï¼ŒK-medoids
  - Hierarchical clustering  
    åˆ†å±‚èšç±»
    - Bottom-up(agglomerative)  
      è‡ªä¸‹è€Œä¸Šï¼ˆå‡èšï¼‰
    - Top-down  
      è‡ªä¸Šè€Œä¸‹
  - Density-based clustering, e.g. DBScan  
    åŸºäºå¯†åº¦çš„èšç±»ï¼Œä¾‹å¦‚DBScan
  - Mixture density based clustering åŸºäºæ··åˆå¯†åº¦çš„èšç±»
  - Fuzzy theory based, graph theory based, grid based, etc.  
    åŸºäºæ¨¡ç³Šç†è®ºã€åŸºäºå›¾è®ºç†è®ºã€åŸºäºç½‘æ ¼ç†è®ºç­‰

### 2.2 Hierarchical clustering

- Create a hierarchical decomposition of the set of objects using some
  criterion  
  ä½¿ç”¨æŸç§æ ‡å‡†åˆ›å»ºå¯¹è±¡é›†çš„åˆ†å±‚åˆ†è§£
- Produce a dendrogram  
  ç”Ÿæˆæ ‘çŠ¶å›¾

![Hierarchical clustering_1.png](Images/Hierarchical%20clustering_1.png)

- Agglomerative clustering illustration  
  å‡èšæ€§çš„èšç±»è¯´æ˜
  - Place each data point into its own singleton group  
    å°†æ¯ä¸ªæ•°æ®ç‚¹æ”¾åˆ°å®ƒè‡ªå·±çš„å•ä¾‹ç»„ä¸­
  - Repeat: iteratively merge the two closest groups  
    é‡å¤ï¼šè¿­ä»£åœ°åˆå¹¶ä¸¤ä¸ªæœ€è¿‘çš„ç»„
  - Until: all the data are merged into a single cluster  
    ç›´åˆ°ï¼šå°†æ‰€æœ‰æ•°æ®éƒ½åˆå¹¶ä¸ºå•ä¸ªé›†ç¾¤

- Output: a dendrogram  
  è¾“å‡ºï¼šæ ‘çŠ¶å›¾
- Reply on: a distance metric between clusters  
  å›å¤ï¼šé›†ç¾¤ä¹‹é—´çš„è·ç¦»åº¦é‡

![Agglomerative clustering illustration_1.png](Images/Agglomerative%20clustering%20illustration_1.png)

- Measuring Distance between clusters
  - Single linkage  
    å•è¿é”
    - the similarity of the closest pair  
      æœ€è¿‘çš„ä¸€å¯¹ä¹‹é—´çš„ç›¸ä¼¼æ€§

  ![Measuring Distance between clusters_1.png](Images/Measuring%20Distance%20between%20clusters_1.png)

  - Complete linkage  
    å®Œå…¨è¿é”
    - the similarity of the furthest pair æœ€è¿œçš„ä¸€å¯¹ä¹‹é—´çš„ç›¸ä¼¼æ€§

  ![Measuring Distance between clusters_2.png](Images/Measuring%20Distance%20between%20clusters_2.png)

  - Group average  
    ç»„å¹³å‡å€¼
    - the average similarity of all pairs  
      æ‰€æœ‰æˆå¯¹çš„å¹³å‡ç›¸ä¼¼åº¦
    - more widely used  
      æ›´å¹¿æ³›åœ°ä½¿ç”¨
    - robust against noise æŠ—å™ªå£°å¼º

  ![Measuring Distance between clusters_3.png](Images/Measuring%20Distance%20between%20clusters_3.png)

- Strengths, weaknesses, caveats  
  ä¼˜åŠ¿ã€å¼±ç‚¹å’Œæ³¨æ„äº‹é¡¹
  - Strengths  
    ä¼˜åŠ¿
    - provides deterministic results  
      æä¾›ç¡®å®šæ€§ç»“æœ
    - no need to specify number of clusters beforehand  
      ä¸éœ€è¦é¢„å…ˆæŒ‡å®šé›†ç¾¤çš„æ•°é‡
    - can create clusters of arbitrary shapes  
      å¯ä»¥åˆ›å»ºä»»æ„å½¢çŠ¶çš„é›†ç¾¤å—
  - Weakness  
    ç¼ºç‚¹
    - does not scale up for large datasets, time complexity at least
      O(n<sup>2</sup>)  
      ä¸å¯ä»¥æ‰©å±•åˆ°å¤§å‹æ•°æ®é›†ï¼Œæ—¶é—´å¤æ‚åº¦è‡³å°‘ä¸ºO(n<sup>2</sup>)
  - Caveats  
    æ³¨æ„äº‹é¡¹
    - Different decisions about group similarities can lead to vastly
      different dendrograms.  
      å…³äºç¾¤ä½“ç›¸ä¼¼æ€§çš„ä¸åŒå†³å®šå¯èƒ½ä¼šå¯¼è‡´æˆªç„¶ä¸åŒçš„æ ‘çŠ¶å›¾
    - The algorithm imposes a hierarchical structure on the data, even
      data for which such structure is not appropriate.
      è¯¥ç®—æ³•å¯¹æ•°æ®å¼ºåŠ äº†ä¸€ä¸ªåˆ†å±‚ç»“æ„ï¼Œå³ä½¿æ˜¯è¿™ç§ç»“æ„ä¸åˆé€‚çš„æ•°æ®

## 3. K-means

### 3.1 K-means

- Centroid-based: describe each cluster by its mean  
  åŸºäºè´¨å¿ƒçš„ï¼šç”¨å®ƒçš„å¹³å‡å€¼æ¥æè¿°æ¯ä¸ªèšç±»
- Goal: assign data to K.  
  ç›®æ ‡ï¼šå°†æ•°æ®åˆ†é…ç»™K
- Algorithm objective: minimize the within-cluster variances of all
  clusters.  
  ç®—æ³•ç›®æ ‡ï¼šæœ€å°åŒ–æ‰€æœ‰èšç±»çš„ç°‡å†…æ–¹å·®

- A non-deterministic method  
  éç¡®å®šæ€§æ–¹æ³•
- Finds a local optimal result (multiple restarts are often necessary)  
  æ‰¾åˆ°å±€éƒ¨æœ€ä¼˜ç»“æœï¼ˆé€šå¸¸éœ€è¦å¤šæ¬¡é‡å¯ï¼‰

- Algorithm description  
  ![Algorithm description_1.png](Images/Algorithm%20description_1.png)

## 4. GMM/EM

### 4.1 Gaussian mixture models(GMMs)

- Assume data was generated by a set of Gaussian distributions  
  å‡è®¾æ•°æ®æ˜¯ç”±ä¸€ç»„é«˜æ–¯åˆ†å¸ƒç”Ÿæˆçš„
- The probability density is a mixture of them æ¦‚ç‡å¯†åº¦æ˜¯å®ƒä»¬çš„æ··åˆç‰©
- Find the parameters of the Gaussian distributions and how much each
  distribution contributes to the data  
  æ‰¾å‡ºé«˜æ–¯åˆ†å¸ƒçš„å‚æ•°ä»¥åŠæ¯ä¸ªåˆ†å¸ƒå¯¹æ•°æ®çš„è´¡çŒ®ç¨‹åº¦
- This is a mixture model of Gaussian  
  è¿™æ˜¯ä¸€ä¸ªé«˜æ–¯åˆ†å¸ƒçš„æ··åˆæ¨¡å‹

- Generative Models  
  ç”Ÿæˆæ¨¡å‹
  - In supervised learning, we model the joint distribution  
    åœ¨ç›‘ç£å­¦ä¹ ä¸­ï¼Œæˆ‘ä»¬å»ºç«‹äº†è”åˆåˆ†å¸ƒçš„æ¨¡å‹  
    ![Generative Models_1.png](Images/Generative%20Models_1.png)
  - In unsupervised learning, we do not have labels z, we model  
    åœ¨æ— ç›‘ç£å­¦ä¹ ä¸­ï¼Œæˆ‘ä»¬æ²¡æœ‰æ ‡ç­¾zï¼Œæˆ‘ä»¬å»ºæ¨¡  
    ![Generative Models_2.png](Images/Generative%20Models_2.png)

- A GMM represents a distributions as  
  ä¸€ä¸ªGMMè¡¨ç¤ºä¸€ä¸ªåˆ†å¸ƒä¸º  
  ![GMMs_1.png](Images/GMMs_1.png)
- with Ï€<sub>k</sub> the mixing coefficients, where  
  ä¸Ï€<sub>k</sub>çš„æ··åˆç³»æ•°ï¼Œå…¶ä¸­  
  ![GMMs_2.png](Images/GMMs_2.png)
- GMM is a density estimator  
  GMMæ˜¯ä¸€ä¸ªå¯†åº¦ä¼°è®¡å™¨
- GMM is universal approximators of densities (if you have enough
  Gaussians)  
  GMMæ˜¯å¯†åº¦çš„é€šç”¨è¿‘ä¼¼å™¨ï¼ˆå¦‚æœä½ æœ‰è¶³å¤Ÿçš„é«˜æ–¯åˆ†å¸ƒï¼‰

- To have a model best fit data, we need to maximize the (log)
  likelihood  
  ä¸ºäº†å¾—åˆ°ä¸€ä¸ªæ¨¡å‹çš„æœ€ä½³æ‹Ÿåˆæ•°æ®ï¼Œæˆ‘ä»¬éœ€è¦æœ€å¤§åŒ–ï¼ˆå¯¹æ•°ï¼‰çš„å¯èƒ½æ€§  
  ![GMMs_3.png](Images/GMMs_3.png)
- Expectation: if we knew Ï€<sub>k</sub>, Âµ and âˆ‘ , we can get â€œsoftâ€
  Z<sub>k</sub> P(z<sup>(n)</sup>|x) - responsibility
- Maximization: if we know Z<sub>k</sub>, we can get Ï€<sub>k</sub>, Âµ
  and âˆ‘

- GMM model has 3 parameters in total to optimise:  
  GMMæ¨¡å‹å…±æœ‰3ä¸ªå‚æ•°è¿›è¡Œä¼˜åŒ–ï¼š
  - the mean vectors of each component(mu)  
    æ¯ä¸ªåˆ†é‡çš„å¹³å‡å‘é‡(mu)
  - the covariances matrices of each component(sigma)  
    æ¯ä¸ªåˆ†é‡çš„åæ–¹å·®çŸ©é˜µ(sigma)
  - the weights associated with each component(pi)  
    ä¸æ¯ä¸ªç»„ä»¶å…³è”çš„æƒé‡(pi)

  - Each iteration of the EM algorithm increases to likelihood of the
    data, unless you happen to be exactly at a local optimum.  
    EMç®—æ³•çš„æ¯æ¬¡è¿­ä»£éƒ½ä¼šå¢åŠ åˆ°æ•°æ®çš„å¯èƒ½æ€§ï¼Œé™¤éä½ ç¢°å·§æ°å¥½å¤„äºå±€éƒ¨æœ€ä¼˜çŠ¶æ€ã€‚

### 4.2 Expectation-Maximization(EM Algorithm)

- An optimization process that alternates between 2 steps:  
  åœ¨ä»¥ä¸‹ä¸¤ä¸ªæ­¥éª¤ä¹‹é—´äº¤æ›¿è¿›è¡Œçš„ä¼˜åŒ–è¿‡ç¨‹ï¼š
  - E-step: compute the posterior probability over z given the current
    model.  
    Eæ­¥ï¼šè®¡ç®—ç»™å®šå½“å‰æ¨¡å‹å¯¹zçš„åéªŒæ¦‚ç‡  
    ![EM_1.png](Images/EM_1.png)
  - M-step: Assuming data was really generated this way, change the
    parameters of each Gaussian to maximize the probability that it
    would generate the data it is currently responsible for.  
    mæ­¥ï¼šå‡è®¾æ•°æ®çœŸçš„æ˜¯ä»¥è¿™ç§æ–¹å¼ç”Ÿæˆçš„ï¼Œé‚£ä¹ˆå°±æ”¹å˜æ¯ä¸ªé«˜æ–¯åˆ†å¸ƒçš„å‚æ•°ï¼Œä»¥æœ€å¤§é™åº¦åœ°æé«˜å®ƒäº§ç”Ÿå®ƒç›®å‰è´Ÿè´£çš„æ•°æ®çš„æ¦‚ç‡
    ![EM_2.png](Images/EM_2.png)

- A general algorithm for optimizing many latent variable models (not
  just for GMMs).  
  ä¸€ç§ç”¨äºä¼˜åŒ–è®¸å¤šæ½œåœ¨å˜é‡æ¨¡å‹çš„é€šç”¨ç®—æ³•(ä¸ä»…ä»…æ˜¯ç”¨äºgmm)
- Iteratively computes a lower bound then optimizes it  
  è¿­ä»£åœ°è®¡ç®—ä¸€ä¸ªä¸‹ç•Œï¼Œç„¶åä¼˜åŒ–å®ƒ
- Converges but maybe to a local minima  
  æ”¶æ•›ï¼Œä½†å¯èƒ½ä¼šæ”¶æ•›åˆ°ä¸€ä¸ªå±€éƒ¨æœ€å°å€¼
- Can use multiple restarts  
  å¯ä»¥ä½¿ç”¨å¤šä¸ªé‡æ–°å¯åŠ¨

### 4.3 Summary

- Clustering  
  èšç±»
  - group similar data points  
    ç»„ç›¸ä¼¼çš„æ•°æ®ç‚¹
  - need a distance measure  
    éœ€è¦ä¸€ä¸ªè·ç¦»æµ‹é‡
- Agglomerative hierarchical clustering  
  å‡èšçš„å±‚æ¬¡èšç±»
  - successively merges similar groups of points  
    ä¾æ¬¡åˆå¹¶ç›¸ä¼¼çš„ç‚¹ç»„
  - build a dendrogram (binary tree)  
    æ„å»ºä¸€ä¸ªæ ‘çŠ¶å›¾ï¼ˆäºŒå‰æ ‘ï¼‰
  - different ways to measure distance between clusters  
    æµ‹é‡é›†ç¾¤ä¹‹é—´è·ç¦»çš„ä¸åŒæ–¹æ³•
- GMM using EM  
  GMMä½¿ç”¨EM
  - build a generative model based on Gaussian distributions  
    å»ºç«‹ä¸€ä¸ªåŸºäºé«˜æ–¯åˆ†å¸ƒçš„ç”Ÿæˆæ¨¡å‹
  - need to pre-define k (number of clusters)  
    éœ€è¦é¢„å…ˆå®šä¹‰kï¼ˆé›†ç¾¤çš„æ•°é‡ï¼‰
  - Using EM to find the best fit of the model  
    åˆ©ç”¨EMæ‰¾åˆ°æ¨¡å‹çš„æœ€ä½³æ‹Ÿåˆæ€§


## 5. DBSCAN

### 5.1 Density-based Clustering - DBSCAN

- Acronym for: Density-based spatial clustering of applications with
  noise  
  ç¼©å†™ï¼šåŸºäºå¯†åº¦çš„å¸¦æœ‰å™ªå£°çš„åº”ç”¨ç¨‹åºçš„ç©ºé—´èšç±»
- Clusters are dense regions in the data space separated by regions of
  lower sample density  
  èšç±»æ˜¯æ•°æ®ç©ºé—´ä¸­ç”±æ ·æœ¬å¯†åº¦è¾ƒä½çš„åŒºåŸŸåˆ†éš”çš„å¯†é›†åŒºåŸŸ
- A cluster is defined as a maximal set of density connected points  
  ä¸€ä¸ªç°‡è¢«å®šä¹‰ä¸ºå¯†åº¦è¿æ¥ç‚¹çš„æœ€å¤§é›†
- Discover clusters of arbitrary shape  
  å‘ç°ä»»æ„å½¢çŠ¶çš„ç°‡

- Define three exclusive types of points  
  å®šä¹‰ä¸‰ç§æ’ä»–æ€§ç±»å‹çš„ç‚¹
  - Core, Border (or Edge) and Noise (or outlier)  
    æ ¸å¿ƒã€è¾¹ç•Œï¼ˆæˆ–è¾¹ç¼˜ï¼‰å’Œå™ªå£°ï¼ˆæˆ–å¼‚å¸¸å€¼ï¼‰

> Core points -- dense region æ ¸å¿ƒç‚¹ï¼Œå¯†é›†åŒºåŸŸ  
> Noise -- sparse region å™ªå£°ç¨€ç–åŒº

![DBSCAN_1.png](Images/DBSCAN_1.png)

- Need two parameters  
  éœ€è¦ä¸¤ä¸ªå‚æ•°
  - a circle of epsilon radius  
    ä¸€ä¸ªåŠå¾„çš„åœ†
  - a circle containing at least minPts number of points  
    ä¸€ä¸ªè‡³å°‘åŒ…å«åˆ†é’Ÿæ•°ç‚¹çš„åœ†

- Three types of points

| core   | The point has at least minPts number of points within Eps<br>è¯¥ç‚¹åœ¨Epså†…è‡³å°‘æœ‰minPtsæ•°é‡çš„ç‚¹æ•°                                                            |
|:-------|:------------------------------------------------------------------------------------------------------------------------------------------------------|
| border | The point has fewer than minPts within Eps, but is in the neighbourhood (i.e. circle) of a core point<br>è¯¥ç‚¹åœ¨Epså†…æ¯”minptå°‘ï¼Œä½†åœ¨ä¸€ä¸ªæ ¸å¿ƒç‚¹çš„é™„è¿‘ï¼ˆå³åœ†åœˆï¼‰ |
| noise  | Any point that is not a core point or a border point. <br>ä»»ä½•ä¸æ˜¯æ ¸å¿ƒç‚¹æˆ–è¾¹ç•Œç‚¹çš„ä»»ä½•ç‚¹                                                                   |

![DBSCAN_3.png](Images/DBSCAN_3.png)

- Density-reachability  
  å¯†åº¦å¯è¾¾æ€§
  - Directly density-reachable: a point q is directly density-reachable
    from point p if p is a core point and q is in pâ€™s neighbourhood  
    ç›´æ¥å¯†åº¦å¯è¾¾ï¼šå¦‚æœpæ˜¯ä¸€ä¸ªæ ¸å¿ƒç‚¹ï¼Œå¹¶ä¸”qåœ¨pçš„é‚»åŸŸå†…ï¼Œåˆ™ä¸€ä¸ªç‚¹qä»ç‚¹pç›´æ¥å¯†åº¦å¯è¾¾

  ![DBSCAN_4.png](Images/DBSCAN_4.png)
  - q is directly density-reachable from p  
    qå¯ä»¥ä»pç›´æ¥å¾—åˆ°å¯†åº¦
  - p is not necessarily directly density-reachable from q  
    pä¸ä¸€å®šèƒ½ä»qä¸­ç›´æ¥è¾¾åˆ°å¯†åº¦
  - Density-reachability is asymmetric  
    å¯†åº¦-å¯è¾¾æ€§æ˜¯ä¸å¯¹ç§°çš„
  - Density-Reachable (directly and indirectly)  
    å¯†åº¦-å¯è¾¾æ€§ï¼ˆç›´æ¥æˆ–é—´æ¥ï¼‰
    - A point p is directly density-reachable from p2  
      ä¸€ä¸ªç‚¹på¯ä»¥ä»p2ç›´æ¥é€šè¿‡å¯†åº¦åˆ°è¾¾
    - p2 is directly density-reachable from p1  
      p2å¯ä»¥ä»p1ç›´æ¥è¾¾åˆ°å¯†åº¦
    - p1 is directly density-reachable from q  
      p1å¯ä»¥ç›´æ¥ä»qè¾¾åˆ°å¯†åº¦
    - q -> p1 -> p2 -> p form a chain(p is the border)  
      q->p1->p2->på½¢æˆä¸€ä¸ªé“¾(pæ˜¯è¾¹ç•Œ)

  ![DBSCAN_5.png](Images/DBSCAN_5.png)
  - p is indirectly density-reachable from q  
    pæ˜¯ä»qé—´æ¥è¾¾åˆ°å¯†åº¦çš„
  - q is not density-reachable from p  
    qä¸èƒ½ä»på¾—åˆ°å¯†åº¦

### 5.2 The algorithm

1. Label all points as core, border or noise.  
   å°†æ‰€æœ‰ç‚¹æ ‡è®°ä¸ºæ ¸å¿ƒã€è¾¹ç•Œæˆ–å™ªå£°
2. Eliminate noise points. æ¶ˆé™¤å™ªå£°ç‚¹
3. For every core point p that has not been assigned to a cluster:  
   å¯¹äºæ²¡æœ‰åˆ†é…ç»™é›†ç¾¤çš„æ¯ä¸ªæ ¸å¿ƒç‚¹p  
   Create a new cluster with the point p and all the points that are
   density-reachable from p  
   ç”¨ç‚¹på’Œæ‰€æœ‰ä»på¯ä»¥è¾¾åˆ°å¯†åº¦çš„ç‚¹åˆ›å»ºä¸€ä¸ªæ–°çš„é›†ç¾¤
4. For border points belonging to more than 1 cluster, assign it to the
   cluster of the closest core point.  
   å¯¹äºå±äº1ä¸ªé›†ç¾¤çš„è¾¹ç•Œç‚¹ï¼Œå°†å…¶åˆ†é…ç»™æœ€è¿‘æ ¸å¿ƒç‚¹çš„é›†ç¾¤ã€‚

- Some key points
  - DBSCAN can find non-linearly separable clusters. (an advantage over
    K-means and GMM)  
    DBSCANå¯ä»¥æ‰¾åˆ°éçº¿æ€§å¯åˆ†çš„ç°‡ã€‚(ç›¸å¯¹äºK-meanså’ŒGMMçš„ä¼˜åŠ¿)
  - Resistant to noise  
    è€å™ªéŸ³
  - Not entirely deterministic: border points that are reachable from
    more than one cluster can be part of either cluster, depending on
    the implementation  
    ä¸å®Œå…¨ç¡®å®šæ€§çš„ï¼šä»å¤šä¸ªé›†ç¾¤å¯è®¿é—®çš„è¾¹ç•Œç‚¹å¯ä»¥æ˜¯ä»»æ„ä¸€ä¸ªé›†ç¾¤çš„ä¸€éƒ¨åˆ†ï¼Œè¿™å–å†³äºå®ç°

- K-means and EM rely on cluster initialisation, and EM also relies on
  gradient descent. Therefore, they are non-deterministic algorithms and
  may get struck at local optima  
  K-meanså’ŒEMä¾èµ–äºèšç±»åˆå§‹åŒ–ï¼Œè€ŒEMä¹Ÿä¾èµ–äºæ¢¯åº¦ä¸‹é™ã€‚å› æ­¤ï¼Œå®ƒä»¬æ˜¯éç¡®å®šæ€§çš„ç®—æ³•ï¼Œå¯èƒ½ä¼šè¾¾åˆ°å±€éƒ¨æœ€ä¼˜

- Gaussian mixture model trained using EM is a soft version of K-means,
  but these two algorithms do not necessarily produce the same cluster
  centres given the same data set.  
  ä½¿ç”¨EMè®­ç»ƒçš„é«˜æ–¯æ··åˆæ¨¡å‹æ˜¯K-meansçš„è½¯ç‰ˆæœ¬ï¼Œä½†è¿™ä¸¤ç§ç®—æ³•åœ¨ç›¸åŒçš„æ•°æ®é›†ä¸‹ä¸ä¸€å®šäº§ç”Ÿç›¸åŒçš„èšç±»ä¸­å¿ƒã€‚
- DBSCAN is capable to discover clusters of any shapes.  
  DBSCANèƒ½å¤Ÿå‘ç°ä»»ä½•å½¢çŠ¶çš„é›†ç¾¤
- Using Gaussian mixture model with Expectation-maximization
  optimization to cluster a data set, the result is non-deterministic
  and may get stuck in local optima.  
  åˆ©ç”¨é«˜æ–¯æ··åˆæ¨¡å‹å’ŒæœŸæœ›æœ€å¤§åŒ–ä¼˜åŒ–å¯¹æ•°æ®é›†è¿›è¡Œèšç±»ï¼Œç»“æœæ˜¯ä¸ç¡®å®šæ€§çš„ï¼Œå¯èƒ½ä¼šé™·å…¥å±€éƒ¨æœ€ä¼˜ã€‚

## 6. Supervised Learning

### 6.1 Supervised Learning

- One of the most prevalent forms of ML  
  MLæœ€æ™®éçš„å½¢å¼ä¹‹ä¸€
  - Teach a computer to do something, then let it use its knowledge to
    do it  
    æ•™ç”µè„‘å»åšæŸä»¶äº‹ï¼Œç„¶åè®©å®ƒç”¨è‡ªå·±çš„çŸ¥è¯†å»åš
- Other forms of ML  
  å…¶ä»–å½¢å¼çš„ML
  - Unsupervised learning  
    æ— ç›‘ç£å­¦ä¹ 
  - Reinforcement learning  
    å¼ºåŒ–å­¦ä¹ 

- Types of supervised learning  
  ç›‘ç£å­¦ä¹ çš„ç±»å‹
  - Regression  
    å›å½’
  - Classification  
    èšç±»
    - Binary
    - Multi-class

### 6.2 Training data

- Supervised learning needs annotated data for training:  
  ç›‘ç£å­¦ä¹ éœ€è¦åŸ¹è®­çš„æ³¨é‡Šæ•°æ®ï¼š  
  in the form of examples of (Input, Output) pairs  
  ä»¥ï¼ˆè¾“å…¥ã€è¾“å‡ºï¼‰å¯¹çš„ç¤ºä¾‹çš„å½¢å¼å‡ºç°
- After training completed  
  åŸ¹è®­å®Œæˆå
  - you present it with new Input that it hasnâ€™t seen before  
    ä½ ç”¨å®ƒä»¥å‰ä»æœªè§è¿‡çš„æ–°è¾“å…¥æ¥å‘ˆç°å®ƒ
  - It needs to predict the appropriate Output  
    å®ƒéœ€è¦é¢„æµ‹é€‚å½“çš„è¾“å‡º

### 6.3 Terminology in Supervised Learning

- Input = attribute(s) = feature(s) = independent variable
- Output = target = response = dependent variable
- function = hypothesis = predictor

### 6.4 Applications of supervised learning

- Handwriting recognition  
  æ‰‹å†™è¯†åˆ«
  - When you write an envelope, algorithms can automatically route
    envelopes through the post  
    å½“ä½ å†™ä¸€ä¸ªä¿¡å°æ—¶ï¼Œç®—æ³•å¯ä»¥è‡ªåŠ¨é€šè¿‡é‚®ä»¶å‘é€ä¿¡å°
- Computer vision & graphics  
  è®¡ç®—æœºè§†è§‰å’Œå›¾å½¢
  - When you go out during lockdown, object detection & visual tracking
    algorithms can automatically detect compliance with the rules  
    å½“ä½ åœ¨é”å®šæœŸé—´å¤–å‡ºæ—¶ï¼Œç›®æ ‡æ£€æµ‹å’Œè§†è§‰è·Ÿè¸ªç®—æ³•å¯ä»¥è‡ªåŠ¨æ£€æµ‹åˆ°æ˜¯å¦ç¬¦åˆè§„åˆ™
- Bioinformatics  
  ç”Ÿç‰©
  - Algorithms can predict protein function from sequence  
    ç®—æ³•å¯ä»¥ä»åºåˆ—ä¸­é¢„æµ‹è›‹ç™½è´¨çš„åŠŸèƒ½
- Human-computer interaction  
  äººæœºäº’åŠ¨
  - Intrusion detection algorithms can recognise speech, gestures,
    intention  
    å…¥ä¾µæ£€æµ‹ç®—æ³•å¯ä»¥è¯†åˆ«è¯­éŸ³ã€æ‰‹åŠ¿ã€æ„å›¾

## 7. Linear Regression

### 7.1 Regression

- Regression means learning a function that captures the â€œtrendâ€ between
  input and output  
  å›å½’æ„å‘³ç€å­¦ä¹ ä¸€ä¸ªæ•è·è¾“å…¥å’Œè¾“å‡ºä¹‹é—´çš„â€œè¶‹åŠ¿â€çš„å‡½æ•°
- We then use this function to predict target values for new inputs  
  ç„¶åï¼Œæˆ‘ä»¬ä½¿ç”¨è¿™ä¸ªå‡½æ•°æ¥é¢„æµ‹æ–°è¾“å…¥çš„ç›®æ ‡å€¼

### 7.2 Univariate linear regression

- Visually, there appears to be a trend  
  ä»è§†è§‰ä¸Šçœ‹ï¼Œä¼¼ä¹æœ‰ä¸€ç§è¶‹åŠ¿
- A reasonable **model** seems to be the **class of linear functions
  (lines)**  
  ä¸€ä¸ªåˆç†çš„æ¨¡å‹ä¼¼ä¹æ˜¯ä¸€ç±»çº¿æ€§å‡½æ•°ï¼ˆçº¿ï¼‰
- We have one input attribute (year) - hence the name **univariate**  
  æˆ‘ä»¬æœ‰ä¸€ä¸ªè¾“å…¥å±æ€§ï¼ˆå¹´ä»½ï¼‰ï¼Œå› æ­¤å®ƒè¢«å‘½åä¸ºå•å˜é‡

![LinearRegression_1.png](Images/LinearRegression_1.png)

- Any line is described by this equation by specifying values for ğ‘¤1,
  ğ‘¤0.

### 7.3 Loss functions (or cost functions)

- We need a criterion that, given the data, for any given line will tell
  us how bad is that line.  
  æˆ‘ä»¬éœ€è¦ä¸€ä¸ªæ ‡å‡†ï¼Œç»™å®šæ•°æ®ï¼Œå¯¹äºä»»ä½•ç»™å®šçš„çº¿éƒ½ä¼šå‘Šè¯‰æˆ‘ä»¬è¿™æ¡çº¿æœ‰å¤šç³Ÿç³•
- Such criterion is called a loss function. It is a function of the free
  parameters!  
  è¿™ç§å‡†åˆ™è¢«ç§°ä¸ºæŸå¤±å‡½æ•°ã€‚å®ƒæ˜¯ä¸€ä¸ªè‡ªç”±å‚æ•°çš„å‡½æ•°
- Loss function = cost function = loss = cost = error function

![Loss functions_1.png](Images/Loss%20functions_1.png)

- Square loss(L2 loss)
  - The loss expresses an error, so it must be always non-negative  
    æŸå¤±è¡¨ç¤ºä¸€ä¸ªé”™è¯¯ï¼Œæ‰€ä»¥å®ƒå¿…é¡»æ€»æ˜¯æ˜¯éè´Ÿçš„
  - Square loss is a sensible choice to measure mismatch for regression  
    å¹³æ–¹æŸå¤±æ˜¯è¡¡é‡å›å½’ä¸åŒ¹é…çš„åˆç†é€‰æ‹©
  - Mean Square Error (MSE)å¹³å‡å¹³æ–¹è¯¯å·®(MSE)  
    ![Loss functions_2.png](Images/Loss%20functions_2.png)

![Loss functions_3.png](Images/Loss%20functions_3.png)

### 7.4 what we want to do

- Given training data  
  ![Loss functions_4.png](Images/Loss%20functions_4.png)
- Fit the model  
  ![Loss functions_5.png](Images/Loss%20functions_5.png)
- By minimising the cost function  
  ![Loss functions_6.png](Images/Loss%20functions_6.png)

- Every combination of w0 and w1 has an associated cost  
  w0å’Œw1çš„æ¯ä¸ªç»„åˆéƒ½æœ‰ä¸€ä¸ªç›¸å…³çš„æˆæœ¬
- To find the â€˜best fitâ€™ we need to find values for w0 and w1 such that
  the cost is minimum.  
  ä¸ºäº†æ‰¾åˆ°â€œæœ€ä½³æ‹Ÿåˆâ€ï¼Œæˆ‘ä»¬éœ€è¦æ‰¾åˆ°w0å’Œw1çš„å€¼ï¼Œä»è€Œä½¿æˆæœ¬æœ€å°

### 7.5 Gradient Descent

- A general strategy to minimise cost functions  
  ä¸€ç§æœ€å°åŒ–æˆæœ¬å‡½æ•°çš„ä¸€èˆ¬ç­–ç•¥
- Goal: Minimise cost function ğ‘”(ğ‘¤), where ğ’˜ =(ğ‘¤0, ğ‘¤1, , â€¦)  
  ç›®æ ‡ï¼šæœ€å°åŒ–æˆæœ¬å‡½æ•°ï¼Œğ‘”(ğ‘¤)  
  ![Gradient Descent_1.png](Images/Gradient%20Descent_1.png)
- Î± is called â€œlearning rateâ€= â€œstep sizeâ€

- If the value of alpha is too high, Gradient Descent will never reach
  the minimum  
  å¦‚æœalphaçš„å€¼è¿‡é«˜ï¼Œæ¢¯åº¦ä¸‹é™å°†æ°¸è¿œä¸ä¼šè¾¾åˆ°æœ€å°å€¼

### 7.6 Gradient

- Partial derivative with respect to ğ‘¤0
  is![Gradient_1.png](Images/Gradient_1.png)It means the derivative
  function of ğ‘”(ğ‘¤0, ğ‘¤1) when ğ‘¤1 is treated as constant.
- Partial derivative with respect to ğ‘¤1
  is![Gradient_2.png](Images/Gradient_2.png)It means the derivative
  function of ğ‘”(ğ‘¤0, ğ‘¤1) when ğ‘¤0 is treated as constant.
- The vector of partial derivatives is called the gradient  
  åå¯¼æ•°çš„å‘é‡ç§°ä¸ºæ¢¯åº¦ ![Gradient_3.png](Images/Gradient_3.png)
- The negative of the gradient evaluated at a location (ğ‘¤0, ğ‘¤1) gives
  us the direction of the steepest descent from that location.  
  åœ¨ä¸€ä¸ªä½ç½®ï¼ˆğ‘¤0ï¼Œğ‘¤1ï¼‰ä¸Šè®¡ç®—çš„æ¢¯åº¦çš„è´Ÿå€¼ç»™å‡ºäº†ä»è¯¥ä½ç½®æœ€é™¡ä¸‹é™çš„æ–¹å‘

- Computing the gradient for our L2 loss  
  è®¡ç®—L2æŸå¤±çš„æ¢¯åº¦  
  ![Gradient_4.png](Images/Gradient_4.png)
- Algorithm for univariate linear regression using GD  
  åŸºäºGDçš„å•å˜é‡çº¿æ€§å›å½’ç®—æ³•  
  ![Gradient_5.png](Images/Gradient_5.png)

- Multivariate linear regression  
  å¤šå…ƒçº¿æ€§å›å½’  
  ![Gradient_6.png](Images/Gradient_6.png)

- Univariate nonlinear regression  
  å•å˜é‡éçº¿æ€§å›å½’  
  ![Gradient_7.png](Images/Gradient_7.png)

- Advantages of vector notation  
  çŸ¢é‡ç¬¦å·çš„ä¼˜ç‚¹
  - Vector notation in concise  
    å‘é‡ç¬¦å·ç®€æ˜
  - With the vectors ğ’˜ and ğ± populated appropriately (and differently
    in each case, as on the previous 2 slides), these models are still
    linear in the parameter vector.  
    ç”±äºå‘é‡ğ’˜å’Œğ±é€‚å½“å¡«å……ï¼ˆæ¯ç§æƒ…å†µä¸‹éƒ½ä¸åŒï¼Œå¦‚å‰ä¸¤ä¸ªå¹»ç¯ç‰‡ï¼‰ï¼Œè¿™äº›æ¨¡å‹åœ¨å‚æ•°å‘é‡ä¸­ä»ç„¶æ˜¯çº¿æ€§çš„ã€‚
  - The cost function is the L2 as before  
    æˆæœ¬å‡½æ•°å’Œå‰é¢ä¸€æ ·æ˜¯l2
  - So the gradient in both cases
    is:![Gradient_8.png](Images/Gradient_8.png)
  - Ready to be plugged into the general gradient descent algorithm  
    å‡†å¤‡å¥½è¢«æ’å…¥åˆ°ä¸€èˆ¬çš„æ¢¯åº¦ä¸‹é™ç®—æ³•ä¸­

- x is independent variables
- w is free parameters(weights)

- Note: The choice of learning rate alpha depends upon dataset and
  hypothesis function. Thus, without any further known details and given
  an arbitrary choice of alpha, it cannot be estimated whether gradient
  descent will converge or not.  
  æ³¨ï¼šå­¦ä¹ ç‡alphaçš„é€‰æ‹©å–å†³äºæ•°æ®é›†å’Œå‡è®¾å‡½æ•°ã€‚å› æ­¤ï¼Œå¦‚æœæ²¡æœ‰ä»»ä½•å·²çŸ¥ç»†èŠ‚å’Œç»™å®šä»»æ„é€‰æ‹©ï¼Œå°±ä¸èƒ½ä¼°è®¡æ¢¯åº¦ä¸‹é™æ˜¯å¦ä¼šæ”¶æ•›ã€‚

